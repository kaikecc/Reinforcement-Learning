{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "sys.path.append(os.path.join('..', '..', '..'))\n",
    "\n",
    "import toolkit as tk\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>well</th>\n",
       "      <th>id</th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>P-JUS-CKGL</th>\n",
       "      <th>T-JUS-CKGL</th>\n",
       "      <th>QGL</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:07</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10092110.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1609800.0</td>\n",
       "      <td>84.59782</td>\n",
       "      <td>1564147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:08</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10092000.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1618206.0</td>\n",
       "      <td>84.58997</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:09</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091890.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1626612.0</td>\n",
       "      <td>84.58213</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:10</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091780.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1635018.0</td>\n",
       "      <td>84.57429</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:11</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091670.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1643424.0</td>\n",
       "      <td>84.56644</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:56</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0453</td>\n",
       "      <td>1504822.0</td>\n",
       "      <td>83.44021</td>\n",
       "      <td>1567749.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:57</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0452</td>\n",
       "      <td>1510422.0</td>\n",
       "      <td>83.45413</td>\n",
       "      <td>1567749.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:58</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0451</td>\n",
       "      <td>1516023.0</td>\n",
       "      <td>83.46806</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:59</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0450</td>\n",
       "      <td>1521623.0</td>\n",
       "      <td>83.48199</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 07:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0449</td>\n",
       "      <td>1527223.0</td>\n",
       "      <td>83.49591</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17874 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label        well              id  P-PDG       P-TPT  \\\n",
       "timestamp                                                                   \n",
       "2017-02-01 02:02:07      0  WELL-00001  20170201020207    0.0  10092110.0   \n",
       "2017-02-01 02:02:08      0  WELL-00001  20170201020207    0.0  10092000.0   \n",
       "2017-02-01 02:02:09      0  WELL-00001  20170201020207    0.0  10091890.0   \n",
       "2017-02-01 02:02:10      0  WELL-00001  20170201020207    0.0  10091780.0   \n",
       "2017-02-01 02:02:11      0  WELL-00001  20170201020207    0.0  10091670.0   \n",
       "...                    ...         ...             ...    ...         ...   \n",
       "2017-02-01 06:59:56      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:57      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:58      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:59      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 07:00:00      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "\n",
       "                        T-TPT  P-MON-CKP  T-JUS-CKP  P-JUS-CKGL  T-JUS-CKGL  \\\n",
       "timestamp                                                                     \n",
       "2017-02-01 02:02:07  119.0944  1609800.0   84.59782   1564147.0         NaN   \n",
       "2017-02-01 02:02:08  119.0944  1618206.0   84.58997   1564148.0         NaN   \n",
       "2017-02-01 02:02:09  119.0944  1626612.0   84.58213   1564148.0         NaN   \n",
       "2017-02-01 02:02:10  119.0944  1635018.0   84.57429   1564148.0         NaN   \n",
       "2017-02-01 02:02:11  119.0944  1643424.0   84.56644   1564148.0         NaN   \n",
       "...                       ...        ...        ...         ...         ...   \n",
       "2017-02-01 06:59:56  119.0453  1504822.0   83.44021   1567749.0         NaN   \n",
       "2017-02-01 06:59:57  119.0452  1510422.0   83.45413   1567749.0         NaN   \n",
       "2017-02-01 06:59:58  119.0451  1516023.0   83.46806   1567750.0         NaN   \n",
       "2017-02-01 06:59:59  119.0450  1521623.0   83.48199   1567750.0         NaN   \n",
       "2017-02-01 07:00:00  119.0449  1527223.0   83.49591   1567750.0         NaN   \n",
       "\n",
       "                     QGL  class  \n",
       "timestamp                        \n",
       "2017-02-01 02:02:07  0.0      0  \n",
       "2017-02-01 02:02:08  0.0      0  \n",
       "2017-02-01 02:02:09  0.0      0  \n",
       "2017-02-01 02:02:10  0.0      0  \n",
       "2017-02-01 02:02:11  0.0      0  \n",
       "...                  ...    ...  \n",
       "2017-02-01 06:59:56  0.0      0  \n",
       "2017-02-01 06:59:57  0.0      0  \n",
       "2017-02-01 06:59:58  0.0      0  \n",
       "2017-02-01 06:59:59  0.0      0  \n",
       "2017-02-01 07:00:00  0.0      0  \n",
       "\n",
       "[17874 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_instances, simulated_instances, drawn_instances = tk.get_all_labels_and_files()\n",
    "tk.load_instance(real_instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Ãºnico dataframe com todos os poÃ§os reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14516197 entries, 0 to 14516196\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   label       int64  \n",
      " 1   well        object \n",
      " 2   id          object \n",
      " 3   P-PDG       float64\n",
      " 4   P-TPT       float64\n",
      " 5   T-TPT       float64\n",
      " 6   P-MON-CKP   float64\n",
      " 7   T-JUS-CKP   float64\n",
      " 8   P-JUS-CKGL  float64\n",
      " 9   T-JUS-CKGL  float64\n",
      " 10  QGL         float64\n",
      " 11  class       float64\n",
      "dtypes: float64(9), int64(1), object(2)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregar todos os DataFrames dos arquivos de instÃ¢ncias reais\n",
    "df_all_instances_real = [pd.DataFrame(tk.load_instance(instance)) for instance in real_instances]\n",
    "\n",
    "# Concatenar todos os DataFrames na lista para formar um Ãºnico DataFrame\n",
    "df = pd.concat(df_all_instances_real, ignore_index=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando o percentual de valores nÃ£o classificado por poÃ§o real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Presumindo que real_instances esteja definido corretamente em algum lugar acima\n",
    "\n",
    "\n",
    "def plot_events(df):\n",
    "    wells = df['well'].unique()  # Lista de poÃ§os\n",
    "    tipos_events = {well: 0 for well in wells}  # Inicializa para calcular o percentual de NaNs\n",
    "\n",
    "    nan_percentages = {well: 0 for well in wells}  # Inicializa para calcular o percentual de NaNs\n",
    "\n",
    "    for well in wells:\n",
    "        dataframes_list = []  # Reinicializa a lista para cada poÃ§o\n",
    "        for instance in real_instances:\n",
    "            instance_path_str = str(instance[1])\n",
    "            pattern = rf'{well}'\n",
    "            if re.search(pattern, instance_path_str):           \n",
    "                try:\n",
    "                    #df_aux = pd.read_csv(instance[1])  # LÃª o DataFrame\n",
    "                    #dataframes_list.append(df_aux)  # Adiciona Ã  lista\n",
    "\n",
    "                    dataframes_list.append(pd.DataFrame(tk.load_instance(instance)))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao ler o arquivo {instance[1]}: {e}\")\n",
    "\n",
    "        if dataframes_list:  # Verifica se a lista nÃ£o estÃ¡ vazia\n",
    "            df_well = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "            total_values = len(df_well['class'])  # Total de valores na coluna\n",
    "        \n",
    "            non_nans = df_well['class'].isna().sum()  # Conta nÃ£o-NaNs        \n",
    "            \n",
    "            percent_nan = (non_nans / total_values)   # Percentual de NaNs\n",
    "            \n",
    "            nan_percentages[well] = percent_nan  # Atualiza o dicionÃ¡rio com o percentual de NaNs\n",
    "\n",
    "            # Verifica se a coluna 'label' existe\n",
    "            if 'label' in df_well.columns:\n",
    "                \n",
    "                unique_label = df_well['label'].unique()   \n",
    "                tipos_events[well] = unique_label         \n",
    "                \n",
    "            else:\n",
    "                print(f\"A coluna 'label' nÃ£o foi encontrada para o poÃ§o {well}.\")\n",
    "                tipos_events[well] = 0  # Ou alguma outra indicaÃ§Ã£o de dado ausente\n",
    "\n",
    "\n",
    "    # Construindo o DataFrame de presenÃ§a/ausÃªncia\n",
    "    rows = []\n",
    "    for well, events in tipos_events.items():\n",
    "        row = [1 if event in events else 0 for event in range(8)]\n",
    "        rows.append(row)\n",
    "    df_presence = pd.DataFrame(rows, index=tipos_events.keys(), columns=range(8))\n",
    "\n",
    "    # Definindo cores personalizadas para o grÃ¡fico de calor\n",
    "    cmap = ListedColormap(['red', 'green'])\n",
    "\n",
    "    # VisualizaÃ§Ã£o com Seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(df_presence, cmap=cmap, linewidths=.5, annot=True, cbar=False, yticklabels=True)\n",
    "\n",
    "    plt.xlabel(\"Tipos de Eventos\")\n",
    "    plt.ylabel(\"PoÃ§os Reais\")\n",
    "    #plt.title(\"PresenÃ§a de Tipos de Eventos por PoÃ§o\")\n",
    "    plt.xticks(ticks=[x + 0.5 for x in range(8)], labels=range(8))  # Centraliza os rÃ³tulos do eixo x\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('heatmap_events.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordenando o dicionÃ¡rio nan_percentages por seus valores (percentual de NaNs) em ordem decrescente\n",
    "sorted_wells = sorted(nan_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Separando em duas listas: uma para os poÃ§os e outra para os percentuais, jÃ¡ convertendo para percentual\n",
    "wells = [item[0] for item in sorted_wells]\n",
    "percent_nans = [item[1] * 100 for item in sorted_wells]  # Convertendo para percentual\n",
    "\n",
    "# Definindo cores: 'skyblue' para percentuais > 0, 'lightgreen' para percentuais == 0\n",
    "colors = ['lightgreen' if percent == 0 else 'skyblue' for percent in percent_nans]\n",
    "\n",
    "# Criando o grÃ¡fico de barras\n",
    "plt.figure(figsize=(12, 6))  # Ajustando o tamanho da figura\n",
    "bars = plt.bar(wells, percent_nans, color=colors)  # Aplicando as cores\n",
    "\n",
    "plt.xlabel('PoÃ§o')  # RÃ³tulo do eixo x\n",
    "plt.ylabel('Percentual de Valores NaN (%)')  # RÃ³tulo do eixo y\n",
    "#plt.title('Percentual de Valores NaN por PoÃ§o (Ordenado)')  # TÃ­tulo do grÃ¡fico\n",
    "plt.grid(axis='y')  # Adicionando grid ao eixo y\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotacionando os rÃ³tulos do eixo x\n",
    "plt.tight_layout()  # Ajusta os parÃ¢metros do subplot\n",
    "\n",
    "# Adicionando rÃ³tulos de percentual acima das barras para clareza\n",
    "for bar, percent in zip(bars, percent_nans):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(percent, 2), ha='center', va='bottom')\n",
    "\n",
    "# Mostrando o grÃ¡fico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando a relaÃ§Ã£o de tipos de eventos por poÃ§o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Dados fornecidos\n",
    "\n",
    "\n",
    "# Construindo o DataFrame de presenÃ§a/ausÃªncia\n",
    "rows = []\n",
    "for well, events in tipos_events.items():\n",
    "    row = [1 if event in events else 0 for event in range(8)]\n",
    "    rows.append(row)\n",
    "df_presence = pd.DataFrame(rows, index=tipos_events.keys(), columns=range(8))\n",
    "\n",
    "# Definindo cores personalizadas para o grÃ¡fico de calor\n",
    "cmap = ListedColormap(['red', 'green'])\n",
    "\n",
    "# VisualizaÃ§Ã£o com Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_presence, cmap=cmap, linewidths=.5, annot=True, cbar=False, yticklabels=True)\n",
    "\n",
    "plt.xlabel(\"Tipos de Eventos\")\n",
    "plt.ylabel(\"PoÃ§os Reais\")\n",
    "#plt.title(\"PresenÃ§a de Tipos de Eventos por PoÃ§o\")\n",
    "plt.xticks(ticks=[x + 0.5 for x in range(8)], labels=range(8))  # Centraliza os rÃ³tulos do eixo x\n",
    "plt.tight_layout()\n",
    "#plt.savefig('heatmap_events.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um dataframe de treinamento e teste para o ambiente\n",
    "\n",
    "CritÃ©rio de seleÃ§Ã£o dos poÃ§os\n",
    "\n",
    "1. Com mais de trÃªs tipos de eventos: 'WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006'\n",
    "2. Dois poÃ§os para equilibrar a falta de eventos: 'WELL-00015', 'WELL-00016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9645620 entries, 0 to 14428408\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   P-PDG      float64\n",
      " 1   P-TPT      float64\n",
      " 2   T-TPT      float64\n",
      " 3   P-MON-CKP  float64\n",
      " 4   T-JUS-CKP  float64\n",
      " 5   class      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 515.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Lista de poÃ§os para treinamento do modelo\n",
    "wells_to_include = ['WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006', 'WELL-00015', 'WELL-00016']\n",
    "\n",
    "# Filtrando o DataFrame para incluir apenas os poÃ§os em wells_to_include\n",
    "df_env = df[df['well'].isin(wells_to_include)]\n",
    "\n",
    "# Selecionando colunas especÃ­ficas\n",
    "columns_to_select = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class']\n",
    "df_env = df_env[columns_to_select]\n",
    "\n",
    "# Apagar linhas que contenham NaNs\n",
    "df_env = df_env.dropna()\n",
    "\n",
    "# Mostrando informaÃ§Ãµes do DataFrame filtrado\n",
    "df_env.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-PDG        0\n",
      "P-TPT        0\n",
      "T-TPT        0\n",
      "P-MON-CKP    0\n",
      "T-JUS-CKP    0\n",
      "class        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count = df_env.isna().sum()\n",
    "\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrÃ©-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Suponha que 'df' seja o seu DataFrame do pandas com as colunas corretas\n",
    "features = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']]\n",
    "labels = df['class']\n",
    "\n",
    "# Criando o dataset a partir de tensores\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features.values, labels.values))\n",
    "\n",
    "# Definindo uma funÃ§Ã£o para agrupar caracterÃ­sticas numÃ©ricas sob a chave \"numeric\"\n",
    "def pack_numeric_features(features, label):\n",
    "    numeric_features = features\n",
    "    return {'numeric': numeric_features}, label\n",
    "\n",
    "# Aplicando a funÃ§Ã£o ao dataset\n",
    "packed_dataset = dataset.map(pack_numeric_features)\n",
    "\n",
    "# Agrupando os dados em lotes\n",
    "batch_size = 32  # VocÃª pode ajustar o tamanho do lote conforme necessÃ¡rio\n",
    "batched_dataset = packed_dataset.batch(batch_size)\n",
    "\n",
    "# Extraindo um exemplo de lote e os rÃ³tulos do batch\n",
    "example_batch, labels_batch = next(iter(batched_dataset))\n",
    "\n",
    "# Exibindo o exemplo do lote e os rÃ³tulos para verificaÃ§Ã£o\n",
    "print(\"Features batch shape:\", example_batch['numeric'].shape)\n",
    "print(\"Labels batch shape:\", labels_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
    "desc = df[NUMERIC_FEATURES].describe()\n",
    "desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MEAN = np.array(desc.T['mean'])\n",
    "STD = np.array(desc.T['std'])\n",
    "\n",
    "def normalize_numeric_data(data, mean, std):\n",
    "  # Center the data\n",
    "  return (data-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Veja o que vocÃª acabou de criar.\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch['numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados CSV\n",
    "\n",
    "Carregar dados CSV de um arquivo em um tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091529</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.855077</td>\n",
       "      <td>0.095567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091529</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.852894</td>\n",
       "      <td>0.095304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.848527</td>\n",
       "      <td>0.094779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091531</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.846344</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      P-PDG     P-TPT    T-TPT  P-MON-CKP  T-JUS-CKP  class\n",
       "0  0.195413 -0.091529  0.38587  -0.855077   0.095567      0\n",
       "1  0.195413 -0.091529  0.38587  -0.852894   0.095304      0\n",
       "2  0.195413 -0.091530  0.38587  -0.850711   0.095041      0\n",
       "3  0.195413 -0.091530  0.38587  -0.848527   0.094779      0\n",
       "4  0.195413 -0.091531  0.38587  -0.846344   0.094516      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suponha que df seja o seu DataFrame\n",
    "\n",
    "# Colunas para normalizar, exceto 'class'\n",
    "columns_to_normalize = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
    "\n",
    "# Aplicando Z-score Standardization\n",
    "for col in columns_to_normalize:\n",
    "    df_env[col] = (df_env[col] - df_env[col].mean()) / df_env[col].std()\n",
    "\n",
    "\n",
    "# converta a coluna class para valor int\n",
    "df_env['class'] = df_env['class'].astype(int)\n",
    "\n",
    "# Verifique as primeiras linhas para confirmar a normalizaÃ§Ã£o\n",
    "df_env.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Estado Inicial: TimeStep(\n",
      "{'step_type': array(0),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.06410941, -1.4085369 ,  0.8308479 , -0.46433973],\n",
      "      dtype=float32)})\n",
      "ApÃ³s a aÃ§Ã£o: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05659381, -1.4072676 ,  1.1816554 , -0.5270461 ],\n",
      "      dtype=float32)})\n",
      "ApÃ³s a aÃ§Ã£o: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.04113068,  0.00758022,  1.9139245 , -0.68807405],\n",
      "      dtype=float32)})\n",
      "ApÃ³s a aÃ§Ã£o: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05391548,  0.29965225, -0.6110193 , -0.09242856],\n",
      "      dtype=float32)})\n",
      "ApÃ³s a aÃ§Ã£o: 1 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05669723, -1.4152119 ,  1.2372649 , -0.44958538],\n",
      "      dtype=float32)})\n",
      "ApÃ³s a aÃ§Ã£o: 1 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.10003506,  0.01710772, -0.8623314 , -0.25641102],\n",
      "      dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join('..'))\n",
    "\n",
    "from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
    "\n",
    "\n",
    "df_test = df_env.sample(frac=0.2)\n",
    "\n",
    "\n",
    "env = env3W(df_test)\n",
    "\n",
    "# Testa o mÃ©todo reset para iniciar o ambiente\n",
    "time_step = env.reset()\n",
    "print(\"Estado Inicial:\", time_step)\n",
    "\n",
    "# Executa algumas aÃ§Ãµes para testar a resposta do ambiente\n",
    "for _ in range(5):\n",
    "    action = np.random.randint(0, 2)  # Escolhe uma aÃ§Ã£o aleatÃ³ria, 0 ou 1\n",
    "    time_step = env.step(action)\n",
    "    print(\"ApÃ³s a aÃ§Ã£o:\", action, \"Time Step:\", time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import utils\n",
    "sys.path.append(os.path.join('..'))\n",
    "\n",
    "from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
    "\n",
    "df_test = df_env.sample(frac=0.002)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "env = env3W(df_test)\n",
    "\n",
    "utils.validate_py_environment(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   4,   6, 107,   5,   3, 101, 105,   7,   1, 102, 106])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['class'].unique()\n",
    "\n",
    "#git config --global user.email \"kaike.castro@posgrad.ufsc.br\"\n",
    "#git config --global user.name \"kaikecc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiperparÃ¢mentros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraÃ§Ãµes iniciais\n",
    "num_iterations = 20000  # ou mais, dependendo da complexidade do ambiente\n",
    "\n",
    "initial_collect_steps = 1000  # NÃºmero de passos de coleta inicial\n",
    "collect_steps_per_iteration = 1  # Passos de coleta por iteraÃ§Ã£o de treinamento\n",
    "replay_buffer_max_length = 100000  # Tamanho mÃ¡ximo do replay buffer\n",
    "\n",
    "log_interval = 1000  # Log do progresso a cada 1000 iteraÃ§Ãµes de treinamento\n",
    "eval_interval = 5000  # AvaliaÃ§Ã£o do desempenho do agente a cada 5000 iteraÃ§Ãµes\n",
    "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
    "num_iterations = 10000  # Exemplo de nÃºmero de iteraÃ§Ãµes de treinamento\n",
    "learning_rate= 1e-3 # Taxa de aprendizado\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambiente\n",
    "\n",
    "No Aprendizado por ReforÃ§o (RL), um ambiente representa a tarefa ou problema a ser resolvido. Ambientes padrÃ£o podem ser criados em TF-Agents usando conjuntos tf_agents.environments.\n",
    "\n",
    "1. O ambiente Ã© um repositÃ³rio di github https://github.com/petrobras/3W\n",
    "2. O ambiente Ã© composto por dados de seis poÃ§os de petrÃ³leo, com cinco variÃ¡veis (observaÃ§Ãµes) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rÃ³tulo indentificador de falha [class]\n",
    "3. O ambiente Ã© um ambiente de simulaÃ§Ã£o, onde o agente pode escolher entre duas aÃ§Ãµes: 0 - NÃ£o Detectado ou 1 - Detectado para cada observaÃ§Ã£o\n",
    "4. A recompensa Ã© calculada com base na aÃ§Ã£o escolhida e no rÃ³tulo de falha [class]:\n",
    "\n",
    "Estados:\n",
    "- rÃ³tulo de falha: 0 - Estado Normal\n",
    "- rÃ³tulo de falha: 1 a 8 - EstÃ¡vel de Anomalia (Falha)\n",
    "- rÃ³tulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
    "\n",
    "AÃ§Ãµes/Recompensas:\n",
    "- Se o rÃ³tulo de falha for 0, a recompensa Ã© 1 se a aÃ§Ã£o for 0, caso contrÃ¡rio, a recompensa Ã© -100\n",
    "- Se o rÃ³tulo de falha estiver entre 1 e 8, a recompensa Ã© -100 se a aÃ§Ã£o for 0, caso contrÃ¡rio, a recompensa Ã© 100\n",
    "- Se o rÃ³tulo de falha estiver entre 101 e 108, a recompensa Ã© -10 se a aÃ§Ã£o for 0, caso contrÃ¡rio, a recompensa Ã© 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = env3W(df_test)  # Supondo que `dataframe` Ã© o seu DataFrame Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'step_type': array(0),\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'discount': array(1., dtype=float32),\n",
       " 'observation': array([ 0.19541295, -0.07263099,  0.02980154,  0.31418243, -0.41717005],\n",
       "      dtype=float32)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ambiente env3W:\n",
    "\n",
    "* ObservaÃ§Ã£o Ã© um array de cinco floats:\n",
    "\n",
    "    * TrÃªs sensores de pressÃ£o: 'P-PDG', 'P-TPT', 'P-MON-CKP'\n",
    "    * Dois sensores de temperatura: 'T-TPT', T-JUS-CKP'\n",
    "\n",
    "* A recompensa Ã© um escalar inteiro.\n",
    "\n",
    "* A aÃ§Ã£o Ã© um escalar inteiro com duas possibilidades:\n",
    "\n",
    "    * 0 â€” \"NÃ£o Detectado\"\n",
    "    * 1 â€” \"Detectado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(\n",
      "{'step_type': array(0),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.07263099,  0.02980154,  0.31418243, -0.41717005],\n",
      "      dtype=float32)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.04104911,  0.01807758,  1.9162282 , -0.6764714 ],\n",
      "      dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente dois ambientes sÃ£o instanciados: um para treinamento e outro para avaliaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)  # AvaliaÃ§Ã£o pode ser em um env similar ou diferente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente\n",
    "\n",
    "O algoritmo utilizado para resolver um problema de RL Ã© representado por um Agente. TF-Agents fornece implementaÃ§Ãµes padrÃ£o de uma variedade de Agentes, incluindo:\n",
    "\n",
    "* DQN (Usado para teste)\n",
    "* REINFORCE\n",
    "* DDPG\n",
    "* TD3\n",
    "* PPO\n",
    "* SAC\n",
    "\n",
    "O agente DQN pode ser utilizado em qualquer ambiente que possua um espaÃ§o de aÃ§Ã£o discreto.\n",
    "\n",
    "No coraÃ§Ã£o de um Agente DQN estÃ¡ uma QNetwork, um modelo de rede neural que pode aprender a prever QValues (retornos esperados) para todas as aÃ§Ãµes, dada uma observaÃ§Ã£o do ambiente.\n",
    "\n",
    "Usaremos tf_agents.networks. para criar uma QNetwork. A rede serÃ¡ composta por uma sequÃªncia de camadas tf.keras.layers.Dense, onde a camada final terÃ¡ 1 saÃ­da para cada aÃ§Ã£o possÃ­vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Defina uma funÃ§Ã£o auxiliar para criar camadas densas configuradas com o direito\n",
    "# ativaÃ§Ã£o e inicializador do kernel.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "# QNetwork consiste em uma sequÃªncia de camadas densas seguidas por uma camada densa\n",
    "# com unidades `num_actions` para gerar um q_value por aÃ§Ã£o disponÃ­vel como sua saÃ­da.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora use tf_agents.agents.dqn.dqn_agent para instanciar um DqnAgent. AlÃ©m de time_step_spec, action_spec e QNetwork, o construtor do agente tambÃ©m requer um otimizador (neste caso, AdamOptimizer), uma funÃ§Ã£o de perda e um contador de passos inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q_net = q_network.QNetwork(\\n    train_env.observation_spec(),\\n    train_env.action_spec(),\\n    fc_layer_params=fc_layer_params)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "'''q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolÃ­tica\n",
    "\n",
    "Uma polÃ­tica define a forma como um agente atua em um ambiente. Normalmente, o objetivo da aprendizagem por reforÃ§o Ã© treinar o modelo subjacente atÃ© que a polÃ­tica produza o resultado desejado.\n",
    "\n",
    "* O resultado desejado Ã© a identificar uma falha com base nas observaÃ§Ãµes\n",
    "* A polÃ­tica retorna uma aÃ§Ã£o (NÃ£o Detectado ou Detectado) para cada observaÃ§Ã£o time_step.\n",
    "\n",
    "Os agentes contÃªm duas polÃ­ticas:\n",
    "\n",
    "* agent.policy â€” A polÃ­tica principal usada para avaliaÃ§Ã£o e implantaÃ§Ã£o.\n",
    "* agent.collect_policy â€” Uma segunda polÃ­tica usada para coleta de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As polÃ­ticas podem ser criadas independentemente dos agentes. Por exemplo, use tf_agents.policies.random_tf_policy para criar uma polÃ­tica que selecionarÃ¡ aleatoriamente uma aÃ§Ã£o para cada time_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter uma aÃ§Ã£o de uma polÃ­tica, chame o mÃ©todo policy.action(time_step). O time_step contÃ©m a observaÃ§Ã£o do ambiente. Este mÃ©todo retorna um PolicyStep, que Ã© uma tupla nomeada com trÃªs componentes:\n",
    "\n",
    "* aÃ§Ã£o - a aÃ§Ã£o a ser executada (neste caso, 0 ou 1)\n",
    "* estado â€“ usado para polÃ­ticas com estado (isto Ã©, baseadas em ANN)\n",
    "* info â€” dados auxiliares, como log de probabilidades de aÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "# Exemplo de criaÃ§Ã£o de uma polÃ­tica aleatÃ³ria\n",
    "# Supondo que env seja seu ambiente TF-Agents\n",
    "random_policy = random_tf_policy.RandomTFPolicy(action_spec=env.action_spec(),\n",
    "                                                 time_step_spec=env.time_step_spec())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÃ©tricas e AvaliaÃ§Ã£o\n",
    "\n",
    "A mÃ©trica mais comum usada para avaliar uma polÃ­tica Ã© o retorno mÃ©dio. O retorno Ã© a soma das recompensas obtidas durante a execuÃ§Ã£o de uma polÃ­tica em um ambiente durante um episÃ³dio. VÃ¡rios episÃ³dios sÃ£o executados, criando um retorno mÃ©dio.\n",
    "\n",
    "A funÃ§Ã£o a seguir calcula o retorno mÃ©dio de uma polÃ­tica, dada a polÃ­tica, o ambiente e um nÃºmero de episÃ³dios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execuÃ§Ã£o desse cÃ¡lculo em random_policy mostra um desempenho de linha de base no ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-779431.8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "Para acompanhar os dados coletados do ambiente, usaremos o Reverb, um sistema de replay eficiente, extensÃ­vel e fÃ¡cil de usar da Deepmind. Ele armazena dados de experiÃªncia quando coletamos trajetÃ³rias e sÃ£o consumidos durante o treinamento.\n",
    "\n",
    "Este buffer de reproduÃ§Ã£o Ã© construÃ­do usando especificaÃ§Ãµes que descrevem os tensores que devem ser armazenados, que podem ser obtidos do agente usando agent.collect_data_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a maioria dos agentes, collect_data_spec Ã© uma tupla nomeada chamada TrajetÃ³ria, contendo especificaÃ§Ãµes para observaÃ§Ãµes, aÃ§Ãµes, recompensas e outros itens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(5,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
       " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(1)),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# ConfiguraÃ§Ãµes para o Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Configura o driver para coleta de experiÃªncias\n",
    "collect_driver = DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# FunÃ§Ã£o para realizar a coleta de experiÃªncias\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''collect_driver.run = common.function(collect_driver.run)\n",
    "\n",
    "# InicializaÃ§Ã£o do ambiente e do estado\n",
    "time_step = train_env.reset()\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "# ColeÃ§Ã£o inicial de experiÃªncias\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColeÃ§Ã£o de dados\n",
    "\n",
    "Agora execute a polÃ­tica aleatÃ³ria no ambiente por algumas etapas, registrando os dados no buffer de reproduÃ§Ã£o.\n",
    "\n",
    "Aqui estamos usando 'PyDriver' para executar o ciclo de coleta de experiÃªncia. VocÃª pode aprender mais sobre o driver TF Agents em nosso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rb_observer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#@test {\"skip\": true}\u001b[39;00m\n\u001b[0;32m      2\u001b[0m py_driver\u001b[38;5;241m.\u001b[39mPyDriver(\n\u001b[0;32m      3\u001b[0m     env,\n\u001b[0;32m      4\u001b[0m     py_tf_eager_policy\u001b[38;5;241m.\u001b[39mPyTFEagerPolicy(\n\u001b[0;32m      5\u001b[0m       random_policy, use_tf_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m----> 6\u001b[0m     [\u001b[43mrb_observer\u001b[49m],\n\u001b[0;32m      7\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39minitial_collect_steps)\u001b[38;5;241m.\u001b[39mrun(train_env\u001b[38;5;241m.\u001b[39mreset())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rb_observer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#@test {\"skip\": true}\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Certifique-se de que collect_step estÃ¡ definido para coletar experiÃªncias corretamente\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "# Prepara o dataset a partir do replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Loop de treinamento\n",
    "for i in range(num_iterations):\n",
    "    # Coleta de novas experiÃªncias\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step()\n",
    "\n",
    "    # Amostra um batch de experiÃªncias para treinamento\n",
    "    experiences, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experiences)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print(f'Step = {i}, Loss = {train_loss.loss:.4f}')\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        # Implemente a avaliaÃ§Ã£o do agente aqui\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "import tensorflow as tf\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "import tensorflow as tf\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "\n",
    "\n",
    "env = env3W(df_test)  # Supondo que `dataframe` Ã© o seu DataFrame Pandas\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)  # AvaliaÃ§Ã£o pode ser em um env similar ou diferente\n",
    "\n",
    "\n",
    "\n",
    "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# Configurando o decaimento exponencial da taxa de aprendizado\n",
    "initial_learning_rate = 1e-4  # Taxa de aprendizado inicial\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "# Agora, usamos lr_schedule diretamente no otimizador\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_schedule(train_step_counter))\n",
    "\n",
    "\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ConfiguraÃ§Ãµes iniciais\n",
    "num_iterations = 20000  # ou mais, dependendo da complexidade do ambiente\n",
    "initial_collect_steps = 1000  # NÃºmero de passos de coleta inicial\n",
    "collect_steps_per_iteration = 1  # Passos de coleta por iteraÃ§Ã£o de treinamento\n",
    "replay_buffer_max_length = 100000  # Tamanho mÃ¡ximo do replay buffer\n",
    "\n",
    "# ConfiguraÃ§Ãµes para o Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "epsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0,  # Epsilon inicial\n",
    "    decay_steps=25000,\n",
    "    end_learning_rate=0.01)  # Epsilon final\n",
    "\n",
    "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "    agent.policy,\n",
    "    epsilon=epsilon_fn(train_step_counter))\n",
    "\n",
    "# Configura o driver para coleta de experiÃªncias\n",
    "collect_driver = DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=epsilon_greedy_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# FunÃ§Ã£o para realizar a coleta de experiÃªncias\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "\n",
    "\n",
    "# InicializaÃ§Ã£o do ambiente e do estado\n",
    "time_step = train_env.reset()\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "\n",
    "# ColeÃ§Ã£o inicial de experiÃªncias\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_interval = 1000  # Log do progresso a cada 1000 iteraÃ§Ãµes de treinamento\n",
    "eval_interval = 5000  # AvaliaÃ§Ã£o do desempenho do agente a cada 5000 iteraÃ§Ãµes\n",
    "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
    "num_iterations = 10000  # Exemplo de nÃºmero de iteraÃ§Ãµes de treinamento\n",
    "\n",
    "\n",
    "# Certifique-se de que collect_step estÃ¡ definido para coletar experiÃªncias corretamente\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "# Prepara o dataset a partir do replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Loop de treinamento\n",
    "for i in range(num_iterations):\n",
    "    # Coleta de novas experiÃªncias\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step()\n",
    "\n",
    "    # Amostra um batch de experiÃªncias para treinamento\n",
    "    experiences, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experiences)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print(f'Step = {i}, Loss = {train_loss.loss:.4f}')\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        # Implemente a avaliaÃ§Ã£o do agente aqui\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
