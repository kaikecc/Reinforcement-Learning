{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "sys.path.append(os.path.join('..', '..', '..'))\n",
    "\n",
    "import toolkit as tk\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>well</th>\n",
       "      <th>id</th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>P-JUS-CKGL</th>\n",
       "      <th>T-JUS-CKGL</th>\n",
       "      <th>QGL</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:07</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10092110.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1609800.0</td>\n",
       "      <td>84.59782</td>\n",
       "      <td>1564147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:08</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10092000.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1618206.0</td>\n",
       "      <td>84.58997</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:09</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091890.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1626612.0</td>\n",
       "      <td>84.58213</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:10</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091780.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1635018.0</td>\n",
       "      <td>84.57429</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 02:02:11</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10091670.0</td>\n",
       "      <td>119.0944</td>\n",
       "      <td>1643424.0</td>\n",
       "      <td>84.56644</td>\n",
       "      <td>1564148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:56</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0453</td>\n",
       "      <td>1504822.0</td>\n",
       "      <td>83.44021</td>\n",
       "      <td>1567749.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:57</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0452</td>\n",
       "      <td>1510422.0</td>\n",
       "      <td>83.45413</td>\n",
       "      <td>1567749.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:58</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0451</td>\n",
       "      <td>1516023.0</td>\n",
       "      <td>83.46806</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 06:59:59</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0450</td>\n",
       "      <td>1521623.0</td>\n",
       "      <td>83.48199</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 07:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>WELL-00001</td>\n",
       "      <td>20170201020207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10014690.0</td>\n",
       "      <td>119.0449</td>\n",
       "      <td>1527223.0</td>\n",
       "      <td>83.49591</td>\n",
       "      <td>1567750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17874 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label        well              id  P-PDG       P-TPT  \\\n",
       "timestamp                                                                   \n",
       "2017-02-01 02:02:07      0  WELL-00001  20170201020207    0.0  10092110.0   \n",
       "2017-02-01 02:02:08      0  WELL-00001  20170201020207    0.0  10092000.0   \n",
       "2017-02-01 02:02:09      0  WELL-00001  20170201020207    0.0  10091890.0   \n",
       "2017-02-01 02:02:10      0  WELL-00001  20170201020207    0.0  10091780.0   \n",
       "2017-02-01 02:02:11      0  WELL-00001  20170201020207    0.0  10091670.0   \n",
       "...                    ...         ...             ...    ...         ...   \n",
       "2017-02-01 06:59:56      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:57      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:58      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 06:59:59      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "2017-02-01 07:00:00      0  WELL-00001  20170201020207    0.0  10014690.0   \n",
       "\n",
       "                        T-TPT  P-MON-CKP  T-JUS-CKP  P-JUS-CKGL  T-JUS-CKGL  \\\n",
       "timestamp                                                                     \n",
       "2017-02-01 02:02:07  119.0944  1609800.0   84.59782   1564147.0         NaN   \n",
       "2017-02-01 02:02:08  119.0944  1618206.0   84.58997   1564148.0         NaN   \n",
       "2017-02-01 02:02:09  119.0944  1626612.0   84.58213   1564148.0         NaN   \n",
       "2017-02-01 02:02:10  119.0944  1635018.0   84.57429   1564148.0         NaN   \n",
       "2017-02-01 02:02:11  119.0944  1643424.0   84.56644   1564148.0         NaN   \n",
       "...                       ...        ...        ...         ...         ...   \n",
       "2017-02-01 06:59:56  119.0453  1504822.0   83.44021   1567749.0         NaN   \n",
       "2017-02-01 06:59:57  119.0452  1510422.0   83.45413   1567749.0         NaN   \n",
       "2017-02-01 06:59:58  119.0451  1516023.0   83.46806   1567750.0         NaN   \n",
       "2017-02-01 06:59:59  119.0450  1521623.0   83.48199   1567750.0         NaN   \n",
       "2017-02-01 07:00:00  119.0449  1527223.0   83.49591   1567750.0         NaN   \n",
       "\n",
       "                     QGL  class  \n",
       "timestamp                        \n",
       "2017-02-01 02:02:07  0.0      0  \n",
       "2017-02-01 02:02:08  0.0      0  \n",
       "2017-02-01 02:02:09  0.0      0  \n",
       "2017-02-01 02:02:10  0.0      0  \n",
       "2017-02-01 02:02:11  0.0      0  \n",
       "...                  ...    ...  \n",
       "2017-02-01 06:59:56  0.0      0  \n",
       "2017-02-01 06:59:57  0.0      0  \n",
       "2017-02-01 06:59:58  0.0      0  \n",
       "2017-02-01 06:59:59  0.0      0  \n",
       "2017-02-01 07:00:00  0.0      0  \n",
       "\n",
       "[17874 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_instances, simulated_instances, drawn_instances = tk.get_all_labels_and_files()\n",
    "tk.load_instance(real_instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um único dataframe com todos os poços reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14516197 entries, 0 to 14516196\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   label       int64  \n",
      " 1   well        object \n",
      " 2   id          object \n",
      " 3   P-PDG       float64\n",
      " 4   P-TPT       float64\n",
      " 5   T-TPT       float64\n",
      " 6   P-MON-CKP   float64\n",
      " 7   T-JUS-CKP   float64\n",
      " 8   P-JUS-CKGL  float64\n",
      " 9   T-JUS-CKGL  float64\n",
      " 10  QGL         float64\n",
      " 11  class       float64\n",
      "dtypes: float64(9), int64(1), object(2)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregar todos os DataFrames dos arquivos de instâncias reais\n",
    "df_all_instances_real = [pd.DataFrame(tk.load_instance(instance)) for instance in real_instances]\n",
    "\n",
    "# Concatenar todos os DataFrames na lista para formar um único DataFrame\n",
    "df = pd.concat(df_all_instances_real, ignore_index=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando o percentual de valores não classificado por poço real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Presumindo que real_instances esteja definido corretamente em algum lugar acima\n",
    "\n",
    "\n",
    "def plot_events(df):\n",
    "    wells = df['well'].unique()  # Lista de poços\n",
    "    tipos_events = {well: 0 for well in wells}  # Inicializa para calcular o percentual de NaNs\n",
    "\n",
    "    nan_percentages = {well: 0 for well in wells}  # Inicializa para calcular o percentual de NaNs\n",
    "\n",
    "    for well in wells:\n",
    "        dataframes_list = []  # Reinicializa a lista para cada poço\n",
    "        for instance in real_instances:\n",
    "            instance_path_str = str(instance[1])\n",
    "            pattern = rf'{well}'\n",
    "            if re.search(pattern, instance_path_str):           \n",
    "                try:\n",
    "                    #df_aux = pd.read_csv(instance[1])  # Lê o DataFrame\n",
    "                    #dataframes_list.append(df_aux)  # Adiciona à lista\n",
    "\n",
    "                    dataframes_list.append(pd.DataFrame(tk.load_instance(instance)))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao ler o arquivo {instance[1]}: {e}\")\n",
    "\n",
    "        if dataframes_list:  # Verifica se a lista não está vazia\n",
    "            df_well = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "            total_values = len(df_well['class'])  # Total de valores na coluna\n",
    "        \n",
    "            non_nans = df_well['class'].isna().sum()  # Conta não-NaNs        \n",
    "            \n",
    "            percent_nan = (non_nans / total_values)   # Percentual de NaNs\n",
    "            \n",
    "            nan_percentages[well] = percent_nan  # Atualiza o dicionário com o percentual de NaNs\n",
    "\n",
    "            # Verifica se a coluna 'label' existe\n",
    "            if 'label' in df_well.columns:\n",
    "                \n",
    "                unique_label = df_well['label'].unique()   \n",
    "                tipos_events[well] = unique_label         \n",
    "                \n",
    "            else:\n",
    "                print(f\"A coluna 'label' não foi encontrada para o poço {well}.\")\n",
    "                tipos_events[well] = 0  # Ou alguma outra indicação de dado ausente\n",
    "\n",
    "\n",
    "    # Construindo o DataFrame de presença/ausência\n",
    "    rows = []\n",
    "    for well, events in tipos_events.items():\n",
    "        row = [1 if event in events else 0 for event in range(8)]\n",
    "        rows.append(row)\n",
    "    df_presence = pd.DataFrame(rows, index=tipos_events.keys(), columns=range(8))\n",
    "\n",
    "    # Definindo cores personalizadas para o gráfico de calor\n",
    "    cmap = ListedColormap(['red', 'green'])\n",
    "\n",
    "    # Visualização com Seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(df_presence, cmap=cmap, linewidths=.5, annot=True, cbar=False, yticklabels=True)\n",
    "\n",
    "    plt.xlabel(\"Tipos de Eventos\")\n",
    "    plt.ylabel(\"Poços Reais\")\n",
    "    #plt.title(\"Presença de Tipos de Eventos por Poço\")\n",
    "    plt.xticks(ticks=[x + 0.5 for x in range(8)], labels=range(8))  # Centraliza os rótulos do eixo x\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('heatmap_events.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordenando o dicionário nan_percentages por seus valores (percentual de NaNs) em ordem decrescente\n",
    "sorted_wells = sorted(nan_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Separando em duas listas: uma para os poços e outra para os percentuais, já convertendo para percentual\n",
    "wells = [item[0] for item in sorted_wells]\n",
    "percent_nans = [item[1] * 100 for item in sorted_wells]  # Convertendo para percentual\n",
    "\n",
    "# Definindo cores: 'skyblue' para percentuais > 0, 'lightgreen' para percentuais == 0\n",
    "colors = ['lightgreen' if percent == 0 else 'skyblue' for percent in percent_nans]\n",
    "\n",
    "# Criando o gráfico de barras\n",
    "plt.figure(figsize=(12, 6))  # Ajustando o tamanho da figura\n",
    "bars = plt.bar(wells, percent_nans, color=colors)  # Aplicando as cores\n",
    "\n",
    "plt.xlabel('Poço')  # Rótulo do eixo x\n",
    "plt.ylabel('Percentual de Valores NaN (%)')  # Rótulo do eixo y\n",
    "#plt.title('Percentual de Valores NaN por Poço (Ordenado)')  # Título do gráfico\n",
    "plt.grid(axis='y')  # Adicionando grid ao eixo y\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotacionando os rótulos do eixo x\n",
    "plt.tight_layout()  # Ajusta os parâmetros do subplot\n",
    "\n",
    "# Adicionando rótulos de percentual acima das barras para clareza\n",
    "for bar, percent in zip(bars, percent_nans):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(percent, 2), ha='center', va='bottom')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando a relação de tipos de eventos por poço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Dados fornecidos\n",
    "\n",
    "\n",
    "# Construindo o DataFrame de presença/ausência\n",
    "rows = []\n",
    "for well, events in tipos_events.items():\n",
    "    row = [1 if event in events else 0 for event in range(8)]\n",
    "    rows.append(row)\n",
    "df_presence = pd.DataFrame(rows, index=tipos_events.keys(), columns=range(8))\n",
    "\n",
    "# Definindo cores personalizadas para o gráfico de calor\n",
    "cmap = ListedColormap(['red', 'green'])\n",
    "\n",
    "# Visualização com Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(df_presence, cmap=cmap, linewidths=.5, annot=True, cbar=False, yticklabels=True)\n",
    "\n",
    "plt.xlabel(\"Tipos de Eventos\")\n",
    "plt.ylabel(\"Poços Reais\")\n",
    "#plt.title(\"Presença de Tipos de Eventos por Poço\")\n",
    "plt.xticks(ticks=[x + 0.5 for x in range(8)], labels=range(8))  # Centraliza os rótulos do eixo x\n",
    "plt.tight_layout()\n",
    "#plt.savefig('heatmap_events.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um dataframe de treinamento e teste para o ambiente\n",
    "\n",
    "Critério de seleção dos poços\n",
    "\n",
    "1. Com mais de três tipos de eventos: 'WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006'\n",
    "2. Dois poços para equilibrar a falta de eventos: 'WELL-00015', 'WELL-00016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9645620 entries, 0 to 14428408\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   P-PDG      float64\n",
      " 1   P-TPT      float64\n",
      " 2   T-TPT      float64\n",
      " 3   P-MON-CKP  float64\n",
      " 4   T-JUS-CKP  float64\n",
      " 5   class      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 515.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Lista de poços para treinamento do modelo\n",
    "wells_to_include = ['WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006', 'WELL-00015', 'WELL-00016']\n",
    "\n",
    "# Filtrando o DataFrame para incluir apenas os poços em wells_to_include\n",
    "df_env = df[df['well'].isin(wells_to_include)]\n",
    "\n",
    "# Selecionando colunas específicas\n",
    "columns_to_select = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class']\n",
    "df_env = df_env[columns_to_select]\n",
    "\n",
    "# Apagar linhas que contenham NaNs\n",
    "df_env = df_env.dropna()\n",
    "\n",
    "# Mostrando informações do DataFrame filtrado\n",
    "df_env.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-PDG        0\n",
      "P-TPT        0\n",
      "T-TPT        0\n",
      "P-MON-CKP    0\n",
      "T-JUS-CKP    0\n",
      "class        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count = df_env.isna().sum()\n",
    "\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Suponha que 'df' seja o seu DataFrame do pandas com as colunas corretas\n",
    "features = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']]\n",
    "labels = df['class']\n",
    "\n",
    "# Criando o dataset a partir de tensores\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features.values, labels.values))\n",
    "\n",
    "# Definindo uma função para agrupar características numéricas sob a chave \"numeric\"\n",
    "def pack_numeric_features(features, label):\n",
    "    numeric_features = features\n",
    "    return {'numeric': numeric_features}, label\n",
    "\n",
    "# Aplicando a função ao dataset\n",
    "packed_dataset = dataset.map(pack_numeric_features)\n",
    "\n",
    "# Agrupando os dados em lotes\n",
    "batch_size = 32  # Você pode ajustar o tamanho do lote conforme necessário\n",
    "batched_dataset = packed_dataset.batch(batch_size)\n",
    "\n",
    "# Extraindo um exemplo de lote e os rótulos do batch\n",
    "example_batch, labels_batch = next(iter(batched_dataset))\n",
    "\n",
    "# Exibindo o exemplo do lote e os rótulos para verificação\n",
    "print(\"Features batch shape:\", example_batch['numeric'].shape)\n",
    "print(\"Labels batch shape:\", labels_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
    "desc = df[NUMERIC_FEATURES].describe()\n",
    "desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MEAN = np.array(desc.T['mean'])\n",
    "STD = np.array(desc.T['std'])\n",
    "\n",
    "def normalize_numeric_data(data, mean, std):\n",
    "  # Center the data\n",
    "  return (data-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Veja o que você acabou de criar.\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch['numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados CSV\n",
    "\n",
    "Carregar dados CSV de um arquivo em um tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091529</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.855077</td>\n",
       "      <td>0.095567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091529</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.852894</td>\n",
       "      <td>0.095304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091530</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.848527</td>\n",
       "      <td>0.094779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195413</td>\n",
       "      <td>-0.091531</td>\n",
       "      <td>0.38587</td>\n",
       "      <td>-0.846344</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      P-PDG     P-TPT    T-TPT  P-MON-CKP  T-JUS-CKP  class\n",
       "0  0.195413 -0.091529  0.38587  -0.855077   0.095567      0\n",
       "1  0.195413 -0.091529  0.38587  -0.852894   0.095304      0\n",
       "2  0.195413 -0.091530  0.38587  -0.850711   0.095041      0\n",
       "3  0.195413 -0.091530  0.38587  -0.848527   0.094779      0\n",
       "4  0.195413 -0.091531  0.38587  -0.846344   0.094516      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suponha que df seja o seu DataFrame\n",
    "\n",
    "# Colunas para normalizar, exceto 'class'\n",
    "columns_to_normalize = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
    "\n",
    "# Aplicando Z-score Standardization\n",
    "for col in columns_to_normalize:\n",
    "    df_env[col] = (df_env[col] - df_env[col].mean()) / df_env[col].std()\n",
    "\n",
    "\n",
    "# converta a coluna class para valor int\n",
    "df_env['class'] = df_env['class'].astype(int)\n",
    "\n",
    "# Verifique as primeiras linhas para confirmar a normalização\n",
    "df_env.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Estado Inicial: TimeStep(\n",
      "{'step_type': array(0),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.06410941, -1.4085369 ,  0.8308479 , -0.46433973],\n",
      "      dtype=float32)})\n",
      "Após a ação: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05659381, -1.4072676 ,  1.1816554 , -0.5270461 ],\n",
      "      dtype=float32)})\n",
      "Após a ação: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.04113068,  0.00758022,  1.9139245 , -0.68807405],\n",
      "      dtype=float32)})\n",
      "Após a ação: 0 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05391548,  0.29965225, -0.6110193 , -0.09242856],\n",
      "      dtype=float32)})\n",
      "Após a ação: 1 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.05669723, -1.4152119 ,  1.2372649 , -0.44958538],\n",
      "      dtype=float32)})\n",
      "Após a ação: 1 Time Step: TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.10003506,  0.01710772, -0.8623314 , -0.25641102],\n",
      "      dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join('..'))\n",
    "\n",
    "from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
    "\n",
    "\n",
    "df_test = df_env.sample(frac=0.2)\n",
    "\n",
    "\n",
    "env = env3W(df_test)\n",
    "\n",
    "# Testa o método reset para iniciar o ambiente\n",
    "time_step = env.reset()\n",
    "print(\"Estado Inicial:\", time_step)\n",
    "\n",
    "# Executa algumas ações para testar a resposta do ambiente\n",
    "for _ in range(5):\n",
    "    action = np.random.randint(0, 2)  # Escolhe uma ação aleatória, 0 ou 1\n",
    "    time_step = env.step(action)\n",
    "    print(\"Após a ação:\", action, \"Time Step:\", time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import utils\n",
    "sys.path.append(os.path.join('..'))\n",
    "\n",
    "from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
    "\n",
    "df_test = df_env.sample(frac=0.002)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "env = env3W(df_test)\n",
    "\n",
    "utils.validate_py_environment(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   4,   6, 107,   5,   3, 101, 105,   7,   1, 102, 106])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['class'].unique()\n",
    "\n",
    "#git config --global user.email \"kaike.castro@posgrad.ufsc.br\"\n",
    "#git config --global user.name \"kaikecc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparâmentros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações iniciais\n",
    "num_iterations = 20000  # ou mais, dependendo da complexidade do ambiente\n",
    "\n",
    "initial_collect_steps = 1000  # Número de passos de coleta inicial\n",
    "collect_steps_per_iteration = 1  # Passos de coleta por iteração de treinamento\n",
    "replay_buffer_max_length = 100000  # Tamanho máximo do replay buffer\n",
    "\n",
    "log_interval = 1000  # Log do progresso a cada 1000 iterações de treinamento\n",
    "eval_interval = 5000  # Avaliação do desempenho do agente a cada 5000 iterações\n",
    "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
    "num_iterations = 10000  # Exemplo de número de iterações de treinamento\n",
    "learning_rate= 1e-3 # Taxa de aprendizado\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambiente\n",
    "\n",
    "No Aprendizado por Reforço (RL), um ambiente representa a tarefa ou problema a ser resolvido. Ambientes padrão podem ser criados em TF-Agents usando conjuntos tf_agents.environments.\n",
    "\n",
    "1. O ambiente é um repositório di github https://github.com/petrobras/3W\n",
    "2. O ambiente é composto por dados de seis poços de petróleo, com cinco variáveis (observações) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rótulo indentificador de falha [class]\n",
    "3. O ambiente é um ambiente de simulação, onde o agente pode escolher entre duas ações: 0 - Não Detectado ou 1 - Detectado para cada observação\n",
    "4. A recompensa é calculada com base na ação escolhida e no rótulo de falha [class]:\n",
    "\n",
    "Estados:\n",
    "- rótulo de falha: 0 - Estado Normal\n",
    "- rótulo de falha: 1 a 8 - Estável de Anomalia (Falha)\n",
    "- rótulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
    "\n",
    "Ações/Recompensas:\n",
    "- Se o rótulo de falha for 0, a recompensa é 1 se a ação for 0, caso contrário, a recompensa é -100\n",
    "- Se o rótulo de falha estiver entre 1 e 8, a recompensa é -100 se a ação for 0, caso contrário, a recompensa é 100\n",
    "- Se o rótulo de falha estiver entre 101 e 108, a recompensa é -10 se a ação for 0, caso contrário, a recompensa é 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = env3W(df_test)  # Supondo que `dataframe` é o seu DataFrame Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'step_type': array(0),\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'discount': array(1., dtype=float32),\n",
       " 'observation': array([ 0.19541295, -0.07263099,  0.02980154,  0.31418243, -0.41717005],\n",
       "      dtype=float32)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ambiente env3W:\n",
    "\n",
    "* Observação é um array de cinco floats:\n",
    "\n",
    "    * Três sensores de pressão: 'P-PDG', 'P-TPT', 'P-MON-CKP'\n",
    "    * Dois sensores de temperatura: 'T-TPT', T-JUS-CKP'\n",
    "\n",
    "* A recompensa é um escalar inteiro.\n",
    "\n",
    "* A ação é um escalar inteiro com duas possibilidades:\n",
    "\n",
    "    * 0 — \"Não Detectado\"\n",
    "    * 1 — \"Detectado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(\n",
      "{'step_type': array(0),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.07263099,  0.02980154,  0.31418243, -0.41717005],\n",
      "      dtype=float32)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'step_type': array(1),\n",
      " 'reward': array(-100., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.19541295, -0.04104911,  0.01807758,  1.9162282 , -0.6764714 ],\n",
      "      dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente dois ambientes são instanciados: um para treinamento e outro para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)  # Avaliação pode ser em um env similar ou diferente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente\n",
    "\n",
    "O algoritmo utilizado para resolver um problema de RL é representado por um Agente. TF-Agents fornece implementações padrão de uma variedade de Agentes, incluindo:\n",
    "\n",
    "* DQN (Usado para teste)\n",
    "* REINFORCE\n",
    "* DDPG\n",
    "* TD3\n",
    "* PPO\n",
    "* SAC\n",
    "\n",
    "O agente DQN pode ser utilizado em qualquer ambiente que possua um espaço de ação discreto.\n",
    "\n",
    "No coração de um Agente DQN está uma QNetwork, um modelo de rede neural que pode aprender a prever QValues (retornos esperados) para todas as ações, dada uma observação do ambiente.\n",
    "\n",
    "Usaremos tf_agents.networks. para criar uma QNetwork. A rede será composta por uma sequência de camadas tf.keras.layers.Dense, onde a camada final terá 1 saída para cada ação possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Defina uma função auxiliar para criar camadas densas configuradas com o direito\n",
    "# ativação e inicializador do kernel.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "# QNetwork consiste em uma sequência de camadas densas seguidas por uma camada densa\n",
    "# com unidades `num_actions` para gerar um q_value por ação disponível como sua saída.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora use tf_agents.agents.dqn.dqn_agent para instanciar um DqnAgent. Além de time_step_spec, action_spec e QNetwork, o construtor do agente também requer um otimizador (neste caso, AdamOptimizer), uma função de perda e um contador de passos inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q_net = q_network.QNetwork(\\n    train_env.observation_spec(),\\n    train_env.action_spec(),\\n    fc_layer_params=fc_layer_params)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "'''q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Política\n",
    "\n",
    "Uma política define a forma como um agente atua em um ambiente. Normalmente, o objetivo da aprendizagem por reforço é treinar o modelo subjacente até que a política produza o resultado desejado.\n",
    "\n",
    "* O resultado desejado é a identificar uma falha com base nas observações\n",
    "* A política retorna uma ação (Não Detectado ou Detectado) para cada observação time_step.\n",
    "\n",
    "Os agentes contêm duas políticas:\n",
    "\n",
    "* agent.policy — A política principal usada para avaliação e implantação.\n",
    "* agent.collect_policy — Uma segunda política usada para coleta de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As políticas podem ser criadas independentemente dos agentes. Por exemplo, use tf_agents.policies.random_tf_policy para criar uma política que selecionará aleatoriamente uma ação para cada time_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter uma ação de uma política, chame o método policy.action(time_step). O time_step contém a observação do ambiente. Este método retorna um PolicyStep, que é uma tupla nomeada com três componentes:\n",
    "\n",
    "* ação - a ação a ser executada (neste caso, 0 ou 1)\n",
    "* estado – usado para políticas com estado (isto é, baseadas em ANN)\n",
    "* info — dados auxiliares, como log de probabilidades de ações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "# Exemplo de criação de uma política aleatória\n",
    "# Supondo que env seja seu ambiente TF-Agents\n",
    "random_policy = random_tf_policy.RandomTFPolicy(action_spec=env.action_spec(),\n",
    "                                                 time_step_spec=env.time_step_spec())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas e Avaliação\n",
    "\n",
    "A métrica mais comum usada para avaliar uma política é o retorno médio. O retorno é a soma das recompensas obtidas durante a execução de uma política em um ambiente durante um episódio. Vários episódios são executados, criando um retorno médio.\n",
    "\n",
    "A função a seguir calcula o retorno médio de uma política, dada a política, o ambiente e um número de episódios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução desse cálculo em random_policy mostra um desempenho de linha de base no ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-779431.8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "Para acompanhar os dados coletados do ambiente, usaremos o Reverb, um sistema de replay eficiente, extensível e fácil de usar da Deepmind. Ele armazena dados de experiência quando coletamos trajetórias e são consumidos durante o treinamento.\n",
    "\n",
    "Este buffer de reprodução é construído usando especificações que descrevem os tensores que devem ser armazenados, que podem ser obtidos do agente usando agent.collect_data_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a maioria dos agentes, collect_data_spec é uma tupla nomeada chamada Trajetória, contendo especificações para observações, ações, recompensas e outros itens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(5,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
       " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(1)),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Configurações para o Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Configura o driver para coleta de experiências\n",
    "collect_driver = DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Função para realizar a coleta de experiências\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''collect_driver.run = common.function(collect_driver.run)\n",
    "\n",
    "# Inicialização do ambiente e do estado\n",
    "time_step = train_env.reset()\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "# Coleção inicial de experiências\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleção de dados\n",
    "\n",
    "Agora execute a política aleatória no ambiente por algumas etapas, registrando os dados no buffer de reprodução.\n",
    "\n",
    "Aqui estamos usando 'PyDriver' para executar o ciclo de coleta de experiência. Você pode aprender mais sobre o driver TF Agents em nosso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rb_observer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#@test {\"skip\": true}\u001b[39;00m\n\u001b[0;32m      2\u001b[0m py_driver\u001b[38;5;241m.\u001b[39mPyDriver(\n\u001b[0;32m      3\u001b[0m     env,\n\u001b[0;32m      4\u001b[0m     py_tf_eager_policy\u001b[38;5;241m.\u001b[39mPyTFEagerPolicy(\n\u001b[0;32m      5\u001b[0m       random_policy, use_tf_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m----> 6\u001b[0m     [\u001b[43mrb_observer\u001b[49m],\n\u001b[0;32m      7\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39minitial_collect_steps)\u001b[38;5;241m.\u001b[39mrun(train_env\u001b[38;5;241m.\u001b[39mreset())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rb_observer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#@test {\"skip\": true}\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Certifique-se de que collect_step está definido para coletar experiências corretamente\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "# Prepara o dataset a partir do replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Loop de treinamento\n",
    "for i in range(num_iterations):\n",
    "    # Coleta de novas experiências\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step()\n",
    "\n",
    "    # Amostra um batch de experiências para treinamento\n",
    "    experiences, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experiences)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print(f'Step = {i}, Loss = {train_loss.loss:.4f}')\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        # Implemente a avaliação do agente aqui\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "import tensorflow as tf\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "import tensorflow as tf\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "\n",
    "\n",
    "env = env3W(df_test)  # Supondo que `dataframe` é o seu DataFrame Pandas\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)  # Avaliação pode ser em um env similar ou diferente\n",
    "\n",
    "\n",
    "\n",
    "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# Configurando o decaimento exponencial da taxa de aprendizado\n",
    "initial_learning_rate = 1e-4  # Taxa de aprendizado inicial\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "# Agora, usamos lr_schedule diretamente no otimizador\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_schedule(train_step_counter))\n",
    "\n",
    "\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configurações iniciais\n",
    "num_iterations = 20000  # ou mais, dependendo da complexidade do ambiente\n",
    "initial_collect_steps = 1000  # Número de passos de coleta inicial\n",
    "collect_steps_per_iteration = 1  # Passos de coleta por iteração de treinamento\n",
    "replay_buffer_max_length = 100000  # Tamanho máximo do replay buffer\n",
    "\n",
    "# Configurações para o Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "epsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0,  # Epsilon inicial\n",
    "    decay_steps=25000,\n",
    "    end_learning_rate=0.01)  # Epsilon final\n",
    "\n",
    "epsilon_greedy_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "    agent.policy,\n",
    "    epsilon=epsilon_fn(train_step_counter))\n",
    "\n",
    "# Configura o driver para coleta de experiências\n",
    "collect_driver = DynamicStepDriver(\n",
    "    env=train_env,\n",
    "    policy=epsilon_greedy_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Função para realizar a coleta de experiências\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "\n",
    "\n",
    "# Inicialização do ambiente e do estado\n",
    "time_step = train_env.reset()\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "\n",
    "# Coleção inicial de experiências\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_interval = 1000  # Log do progresso a cada 1000 iterações de treinamento\n",
    "eval_interval = 5000  # Avaliação do desempenho do agente a cada 5000 iterações\n",
    "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
    "num_iterations = 10000  # Exemplo de número de iterações de treinamento\n",
    "\n",
    "\n",
    "# Certifique-se de que collect_step está definido para coletar experiências corretamente\n",
    "def collect_step():\n",
    "    time_step = train_env.current_time_step()\n",
    "    action_step = agent.collect_policy.action(time_step)\n",
    "    next_time_step = train_env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "# Prepara o dataset a partir do replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Loop de treinamento\n",
    "for i in range(num_iterations):\n",
    "    # Coleta de novas experiências\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step()\n",
    "\n",
    "    # Amostra um batch de experiências para treinamento\n",
    "    experiences, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experiences)\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print(f'Step = {i}, Loss = {train_loss.loss:.4f}')\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        # Implemente a avaliação do agente aqui\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
