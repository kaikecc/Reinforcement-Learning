{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent import Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            try:\n",
    "                class_code = int(class_path.stem)\n",
    "            except ValueError:\n",
    "                # Se não for possível converter para int, pule este diretório\n",
    "                continue\n",
    "\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instance_with_numpy(data_path, events_names, columns, well_names):\n",
    "    data_path = Path(data_path)\n",
    "  \n",
    "    real_instances = list(class_and_file_generator(data_path, real=True, simulated=False, drawn=False))\n",
    "\n",
    "    arrays_list = []  # List to store processed NumPy arrays\n",
    "\n",
    "    for instance in real_instances:\n",
    "        class_code, instance_path = instance    \n",
    "        well, _ = instance_path.stem.split('_')  \n",
    "             \n",
    "        if class_code in events_names.keys() and well in well_names:\n",
    "\n",
    "            # Read the entire CSV file into a NumPy array\n",
    "            with open(instance_path, 'r') as file:\n",
    "                header = file.readline().strip().split(',')\n",
    "                indices = [header.index(col) for col in columns]\n",
    "                                                \n",
    "                if 'timestamp' in columns:\n",
    "                    arr = np.genfromtxt(file, delimiter=',', usecols=indices[1:], dtype=np.float32)\n",
    "                    arr[np.isinf(arr)] = np.nan\n",
    "                    timestamp_idx = header.index('timestamp')\n",
    "                    # Primeiro, vamos carregar todos os timestamps como um array numpy\n",
    "                    file.seek(0)\n",
    "                    file.readline()  # Skip the header\n",
    "                    timestamps = np.genfromtxt(file, delimiter=',', skip_header=0, usecols=timestamp_idx, dtype=str)\n",
    "                    if isinstance(timestamps, str):\n",
    "                        timestamps = np.array([timestamps])\n",
    "\n",
    "                    # Converta os timestamps para o formato desejado antes de aplicar o filtro\n",
    "                    fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "                    rounded_timestamps = np.array([datetime.strptime(ts, fmt).strftime(\"%Y-%m-%d %H:%M:%S\") for ts in timestamps])\n",
    "\n",
    "                    # Agora, aplicamos o mesmo filtro usado em 'arr' para 'rounded_timestamps'\n",
    "                    # Precisamos determinar quais linhas em 'arr' NÃO serão removidas por conter NaN\n",
    "                    not_nan_rows = ~np.isnan(arr).any(axis=1)\n",
    "\n",
    "                    # Filtramos 'arr' e 'rounded_timestamps' usando este índice\n",
    "                    arr_filtered = arr[not_nan_rows]\n",
    "                    rounded_timestamps_filtered = rounded_timestamps[not_nan_rows]\n",
    "\n",
    "                    # Agora, ambos 'arr_filtered' e 'rounded_timestamps_filtered' estão alinhados\n",
    "                    # e podemos concatená-los sem enfrentar o problema de dimensões incompatíveis\n",
    "                    final_data = np.hstack([rounded_timestamps_filtered.reshape(-1, 1), arr_filtered])\n",
    "\n",
    "                    arrays_list.append(final_data)\n",
    "                else:\n",
    "                    arr = np.genfromtxt(file, delimiter=',', usecols=indices, dtype=np.float32)\n",
    "                    # Tratando NaN e valores infinitos antes da conversão\n",
    "                    arr[np.isinf(arr)] = np.nan\n",
    "                    arr = arr[~np.isnan(arr).any(axis=1)]\n",
    "                    arr[:, :-1] = arr[:, :-1].astype(np.float32)  # Convert selected columns to float32\n",
    "                    arr[:, -1] = arr[:, -1].astype(np.int16)  # Convert 'class' column to int16\n",
    "                    \n",
    "                    arrays_list.append(arr)               \n",
    "\n",
    "    # Concatenate all processed NumPy arrays\n",
    "    final_array = np.concatenate(arrays_list) if arrays_list else np.array([])\n",
    "\n",
    "    return final_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env3W_dqn(env):\n",
    "\n",
    "    # Instanciar o modelo DQN com a política MLP\n",
    "    model = DQN(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        learning_rate=1e-4,\n",
    "        buffer_size=10000,\n",
    "        learning_starts=10000,\n",
    "        batch_size=32,\n",
    "        tau=1.0,\n",
    "        gamma=0.99,\n",
    "        train_freq=4,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.01,\n",
    "        max_grad_norm=10,\n",
    "        tensorboard_log=None,\n",
    "        verbose=1,\n",
    "        device='auto'\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    model.learn(total_timesteps=int(1.2e5))\n",
    "\n",
    "    # Salvar o modelo\n",
    "    model.save('dqn_env3W')\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env3W_dqn_eval(model, env, n_eval_episodes=1):\n",
    "    # Inicializa listas para armazenar os dados de cada episódio\n",
    "    correct_predictions_list = []\n",
    "    total_predictions_list = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False]\n",
    "\n",
    "        # Reseta contadores para o episódio atual\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        while not all(done):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, rewards, done, info = env.step(action)\n",
    "\n",
    "            for reward in rewards:\n",
    "                if reward > 0:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "\n",
    "        # Atualiza a mensagem de log para incluir a recompensa e a porcentagem de acerto\n",
    "        if total_predictions > 0:  # Evita divisão por zero\n",
    "            percentage_correct = correct_predictions / total_predictions\n",
    "            #logging.info(f\"[{episode}] Recompensa: {correct_predictions}, Porcentagem de acerto: {percentage_correct:.2f}.\")\n",
    "            #print(f\"[{episode}] Recompensa: {correct_predictions}, Porcentagem de acerto: {percentage_correct:.2f}.\")\n",
    "        else:\n",
    "            logging.info(f\"Sem predições.\")\n",
    "            #print(f\"[{episode}] Sem predições.\")\n",
    "\n",
    "        # Armazena os dados do episódio atual nas listas\n",
    "        correct_predictions_list.append(correct_predictions)\n",
    "        total_predictions_list.append(total_predictions)\n",
    "\n",
    "    # Calcula a precisão geral\n",
    "    accuracy = sum(correct_predictions_list) / sum(total_predictions_list) if sum(total_predictions_list) > 0 else 0\n",
    "\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0]\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'/home/Reinforcement-Learning/logs/{current_time}_{event_name}-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True)\n",
    "\n",
    "    path_dataset = '/home/dataset'    \n",
    "\n",
    "    columns = [       \n",
    "        'P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        #'P-JUS-CKGL',\n",
    "        #'T-JUS-CKGL',\n",
    "        #'QGL',\n",
    "        'class'\n",
    "    ]\n",
    "   \n",
    "    well_names = [f'WELL-{i:05d}' for i in range(1, 19)]\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = load_instance_with_numpy(data_path = path_dataset, events_names = events_names, columns = columns, well_names = well_names)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "        \n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    # Inicializando listas para guardar índices de treino e teste\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Processamento genérico para cada classe\n",
    "    for event in np.unique(dataset[:, -1]):\n",
    "        # Selecionando índices para a classe atual        \n",
    "        class_indices = np.where(dataset[:, -1] == event)[0]\n",
    "        \n",
    "        # Logando o número de amostras por classe\n",
    "        print(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        logging.info(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        \n",
    "        # Dividindo os índices da classe atual em treino e teste\n",
    "        class_train_indices, class_test_indices = train_test_split(class_indices, train_size=train_percentage, random_state=42)\n",
    "        \n",
    "        # Logando o número de amostras de treino e teste\n",
    "        logging.info(f'Número de amostras de treino da classe {event}: {len(class_train_indices)}')\n",
    "        logging.info(f'Número de amostras de teste da classe {event}: {len(class_test_indices)}')\n",
    "        \n",
    "        # Adicionando aos índices gerais de treino e teste\n",
    "        train_indices.extend(class_train_indices)\n",
    "        test_indices.extend(class_test_indices)\n",
    "\n",
    "    # Convertendo listas para arrays numpy para futura manipulação\n",
    "    train_indices = np.array(train_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "\n",
    "    # Embaralhando os índices (opcional, dependendo da necessidade)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    # Criando conjuntos de dados de treino e teste\n",
    "    dataset_train = dataset[train_indices]\n",
    "    dataset_test = dataset[test_indices]\n",
    "\n",
    "    # Dividindo em features (X) e target (y)\n",
    "    X_train, y_train = dataset_train[:, :-1], dataset_train[:, -1]\n",
    "    X_test, y_test = dataset_test[:, :-1], dataset_test[:, -1]\n",
    "\n",
    "    # Escalonando as features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "    # Se necessário, você pode combinar as features escalonadas e o target para formar os datasets finais\n",
    "    dataset_train_scaled = np.column_stack((X_train_scaled, y_train))\n",
    "    dataset_test_scaled = np.column_stack((X_test_scaled, y_test))\n",
    "    logging.info(f'Fim divisão do dataset em treino e teste')\n",
    "    \n",
    "    logging.info(f'Iniciando treinamento do modelo DQN')\n",
    "    start_time = time.time()\n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    shuffled_indices = np.random.permutation(len(dataset_train_scaled))\n",
    "    dataset_shuffled = dataset_train_scaled[shuffled_indices]\n",
    "\n",
    "    # Dividir o dataset embaralhado para os ambientes\n",
    "    n_envs = 5\n",
    "    split_datasets = np.array_split(dataset_shuffled, n_envs)\n",
    "\n",
    "    # Função modificada para criar ambientes com partes específicas do dataset\n",
    "    def create_env(dataset_part):\n",
    "        return Env3WGym(dataset_part)\n",
    "    # Crie o ambiente (modifique conforme necessário para seu caso de uso)\n",
    "    #env = make_vec_env(lambda: Env3WGym(dataset_train_scaled), n_envs=5)\n",
    "    envs = make_vec_env(lambda: create_env(split_datasets.pop(0)), n_envs=n_envs)\n",
    "    env3W_dqn(envs)  # Chamar a função para treinar o modelo\n",
    "    print(f\"Tempo de Treinamento DQN: {time.time() - start_time}\")\n",
    "    logging.info(f\"Tempo de Treinamento DQN: {time.time() - start_time}\")\n",
    "    logging.info(f'Fim treinamento do modelo DQN')\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do modelo de teste DQN')\n",
    "    dqn_model = DQN.load('dqn_env3W')\n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    shuffled_indices = np.random.permutation(len(dataset_test_scaled))\n",
    "    dataset_shuffled = dataset_test_scaled[shuffled_indices]\n",
    "\n",
    "    # Dividir o dataset embaralhado para os ambientes\n",
    "    n_envs = 5\n",
    "    split_datasets = np.array_split(dataset_shuffled, n_envs)\n",
    "\n",
    "    # Função modificada para criar ambientes com partes específicas do dataset\n",
    "    def create_env(dataset_part):\n",
    "        return Env3WGym(dataset_part)\n",
    "    \n",
    "    envs = make_vec_env(lambda: create_env(split_datasets.pop(0)), n_envs=n_envs)\n",
    "    #env = make_vec_env(lambda: Env3WGym(dataset_test_scaled), n_envs=5)\n",
    "    # Executar a função de avaliação\n",
    "    accuracy = env3W_dqn_eval(dqn_model, envs)\n",
    "    print(f'Acurácia: {accuracy}')\n",
    "    logging.info(f'Acurácia: {accuracy}')\n",
    "    logging.info(f'Fim avaliação do modelo de teste DQN')\n",
    "\n",
    "    logging.info(f'Iniciando o teste do modelo de teste DQN')\n",
    "    columns = [\n",
    "        'timestamp',\n",
    "        'P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        #'P-JUS-CKGL',\n",
    "        #'T-JUS-CKGL',\n",
    "        #'QGL',\n",
    "        'class'       \n",
    "    ]\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset de teste')\n",
    "    dataset_test = load_instance_with_numpy(data_path = '/home/dataset_test', events_names = events_names, columns = columns, well_names = well_names)\n",
    "    # Escalone as features dataset_test\n",
    "    logging.info(f'Fim carregamento do dataset de teste')\n",
    "\n",
    "    logging.info(f'Iniciando escalonamento do dataset de teste')\n",
    "    X_test = dataset_test[:, 1:-1]\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    dataset_test = np.column_stack((dataset_test[:, 0], X_test_scaled, dataset_test[:, -1]))\n",
    "    logging.info(f'Fim escalonamento do dataset de teste')\n",
    "\n",
    "    logging.info(f'Iniciando predição de uma instância para teste DQN')\n",
    "    array_action_pred = []\n",
    "    for i in range(0, len(dataset_test)):\n",
    "        obs = dataset_test[i, 1:-1].astype(np.float32)\n",
    "        action, _states = dqn_model.predict(obs, deterministic=True)  \n",
    "        array_action_pred.append(action)\n",
    "\n",
    "    \n",
    "    expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "    logging.info(f'Fim predição da instância de teste DQN')    \n",
    "   \n",
    "    \n",
    "    df = pd.DataFrame(expanded_array, columns = ['timestamp', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'action'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']] = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']].astype('float32')\n",
    "    df['class'] = df['class'].astype(float).astype('int16')\n",
    "    df['action'] = df['action'].astype(float).astype('int16')\n",
    "\n",
    "\n",
    "    explora = exploration(df)\n",
    "    explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'], _title = '{event_name} - DQN')\n",
    "    logging.info(f'Concluído a execução do aprendizado por reforço')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0]\n",
    "    \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'..\\\\..\\\\logs\\\\{current_time}_{event_name}-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True)\n",
    "\n",
    "    path_dataset = '..\\\\..\\\\..\\\\dataset'     \n",
    "\n",
    "    columns = [ \n",
    "        'timestamp',      \n",
    "        'P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        #'P-JUS-CKGL',\n",
    "        #'T-JUS-CKGL',\n",
    "        #'QGL',\n",
    "        'class'\n",
    "    ]\n",
    "   \n",
    "    well_names = [f'WELL-{i:05d}' for i in range(1, 19)]\n",
    "\n",
    "    instances = LoadInstances(path_dataset)\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = instances.load_instance_with_numpy(events_names, columns)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "        \n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    # Inicializando listas para guardar índices de treino e teste\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Processamento genérico para cada classe\n",
    "    for event in np.unique(dataset[:, -1]):\n",
    "        # Selecionando índices para a classe atual        \n",
    "        class_indices = np.where(dataset[:, -1] == event)[0]\n",
    "        \n",
    "        # Logando o número de amostras por classe\n",
    "        print(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        logging.info(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        \n",
    "        # Dividindo os índices da classe atual em treino e teste\n",
    "        class_train_indices, class_test_indices = train_test_split(class_indices, train_size=train_percentage, random_state=42)\n",
    "        \n",
    "        # Logando o número de amostras de treino e teste\n",
    "        logging.info(f'Número de amostras de treino da classe {event}: {len(class_train_indices)}')\n",
    "        logging.info(f'Número de amostras de teste da classe {event}: {len(class_test_indices)}')\n",
    "        \n",
    "        # Adicionando aos índices gerais de treino e teste\n",
    "        train_indices.extend(class_train_indices)\n",
    "        test_indices.extend(class_test_indices)\n",
    "\n",
    "    # Convertendo listas para arrays numpy para futura manipulação\n",
    "    train_indices = np.array(train_indices)    \n",
    "    test_temp_indices = np.array(test_indices)       \n",
    "\n",
    "    test_indices, validation_indices = train_test_split(test_temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Embaralhando os índices (opcional, dependendo da necessidade)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "\n",
    "    # Criando conjuntos de dados de treino e teste\n",
    "    dataset_train = dataset[train_indices]\n",
    "    dataset_test = dataset[test_indices]\n",
    "    dataset_validation = dataset[validation_indices]\n",
    "    \n",
    "\n",
    "    # Dividindo em features (X) e target (y)\n",
    "    X_train, y_train = dataset_train[:, :-1], dataset_train[:, -1]\n",
    "    X_test, y_test = dataset_test[:, :-1], dataset_test[:, -1]\n",
    "    X_validation, y_validation = dataset_validation[:, :-1], dataset_validation[:, -1]\n",
    "\n",
    "    # Delete a primeira coluna (timestamp) das features\n",
    "    X_train = np.delete(X_train, 0, axis=1)\n",
    "    X_test = np.delete(X_test, 0, axis=1)\n",
    "\n",
    "    # Escalonando as features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_validation_scaled = scaler.transform(X_validation[:, 1:])   \n",
    "   \n",
    "\n",
    "    # Se necessário, você pode combinar as features escalonadas e o target para formar os datasets finais\n",
    "    dataset_train_scaled = np.column_stack((X_train_scaled, y_train))\n",
    "    dataset_test_scaled = np.column_stack((X_test_scaled, y_test))\n",
    "    dataset_validation_scaled = np.column_stack((X_validation_scaled, y_validation))\n",
    "    logging.info(f'Fim divisão do dataset em treino e teste')\n",
    "    \n",
    "    logging.info(f'Iniciando treinamento do modelo DQN')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    agente = Agent('..\\\\models\\\\dqn_env3W')\n",
    "\n",
    "    agente.env3W_dqn(dataset_train_scaled, n_envs = 5)  \n",
    "    print(f\"Tempo de Treinamento DQN: {time.time() - start_time}\")\n",
    "    logging.info(f\"Tempo de Treinamento DQN: {time.time() - start_time}\")\n",
    "\n",
    "    logging.info(f'Fim treinamento do modelo DQN')\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do modelo de teste DQN')    \n",
    "    \n",
    "    accuracy, dqn_model = agente.env3W_dqn_eval(dataset_test_scaled, n_envs = 5)\n",
    "    print(f'Acurácia: {accuracy}')\n",
    "    logging.info(f'Acurácia: {accuracy}')\n",
    "    logging.info(f'Fim avaliação do modelo de teste DQN')\n",
    "\n",
    "    # Obtendo os índices que ordenariam a primeira coluna\n",
    "    sort_indices = np.argsort(dataset_validation_scaled[:, 0])\n",
    "\n",
    "    # Usando esses índices para reordenar todo o array\n",
    "    dataset_validation_sorted = dataset_validation_scaled[sort_indices]\n",
    "\n",
    "    # Inicializando previous_timestamp como None para a primeira comparação\n",
    "    previous_datetime = None\n",
    "\n",
    "    for row in dataset_validation_sorted:\n",
    "        current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        if previous_datetime and (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "            # Imprime a hora do timestamp atual se a diferença for maior que 1 hora\n",
    "            print(current_datetime.hour)\n",
    "        \n",
    "        previous_datetime = current_datetime\n",
    "        \n",
    "    # Inicializando a lista para armazenar os sub-datasets\n",
    "    datasets = []\n",
    "    current_dataset = []\n",
    "\n",
    "    # Inicializando previous_datetime como None para a primeira comparação\n",
    "    previous_datetime = None\n",
    "\n",
    "    for row in dataset_validation_sorted:\n",
    "        current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Verifica se é a primeira iteração ou se a diferença é maior que 1 hora\n",
    "        if previous_datetime is None or (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "            # Se não for a primeira iteração e a condição for verdadeira, inicia um novo dataset\n",
    "            if current_dataset:\n",
    "                datasets.append(np.array(current_dataset))\n",
    "                current_dataset = []\n",
    "        \n",
    "        # Adiciona o registro atual ao dataset corrente\n",
    "        current_dataset.append(row)\n",
    "        previous_datetime = current_datetime\n",
    "\n",
    "    # Não esqueça de adicionar o último dataset após terminar o loop\n",
    "    if current_dataset:\n",
    "        datasets.append(np.array(current_dataset))\n",
    "    \n",
    "    count = -1\n",
    "\n",
    "    for dataset_test in datasets:\n",
    "        count += 1\n",
    "        logging.info(f'Iniciando predição de uma instância para teste DQN')\n",
    "        array_action_pred = []\n",
    "        for i in range(0, len(dataset_test)):\n",
    "            obs = dataset_test[i, 1:-1].astype(np.float32)\n",
    "            action, _states = dqn_model.predict(obs, deterministic=True)  \n",
    "            array_action_pred.append(action)\n",
    "\n",
    "        \n",
    "        expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "        logging.info(f'Fim predição da instância de teste DQN')    \n",
    "    \n",
    "        \n",
    "        df = pd.DataFrame(expanded_array, columns = ['timestamp', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'action'])\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']] = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']].astype('float32')\n",
    "        df['class'] = df['class'].astype(float).astype('int16')\n",
    "        df['action'] = df['action'].astype(float).astype('int16')\n",
    "\n",
    "\n",
    "        explora = exploration(df)\n",
    "        explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'], _title = '[{count}] - {event_name} - DQN')\n",
    "        logging.info(f'Concluído a execução do aprendizado por reforço')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras da classe 0.0: 8430978\n",
      "Número de amostras da classe 4.0: 2460270\n",
      "23\n",
      "16\n",
      "1\n",
      "15\n",
      "16\n",
      "0\n",
      "6\n",
      "20\n",
      "11\n",
      "2\n",
      "22\n",
      "2\n",
      "0\n",
      "20\n",
      "12\n",
      "18\n",
      "19\n",
      "6\n",
      "10\n",
      "18\n",
      "21\n",
      "5\n",
      "21\n",
      "19\n",
      "0\n",
      "22\n",
      "18\n",
      "21\n",
      "5\n",
      "6\n",
      "3\n",
      "9\n",
      "18\n",
      "16\n",
      "17\n",
      "8\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "events_names = {\n",
    "    0: 'Normal',\n",
    "    # 1: 'Abrupt Increase of BSW',\n",
    "    # 2: 'Spurious Closure of DHSV',\n",
    "    # 3: 'Severe Slugging',\n",
    "    4: 'Flow Instability',\n",
    "    # 5: 'Rapid Productivity Loss',\n",
    "    # 6: 'Quick Restriction in PCK',\n",
    "    # 7: 'Scaling in PCK',\n",
    "    # 8: 'Hydrate in Production Line'\n",
    "}\n",
    "\n",
    "event_name = [value for key, value in events_names.items() if key != 0]\n",
    "\n",
    "path_dataset = '..\\\\..\\\\..\\\\dataset'    \n",
    "\n",
    "columns = [ \n",
    "    'timestamp',      \n",
    "    'P-PDG',\n",
    "    'P-TPT',\n",
    "    'T-TPT',\n",
    "    'P-MON-CKP',\n",
    "    'T-JUS-CKP',\n",
    "    #'P-JUS-CKGL',\n",
    "    #'T-JUS-CKGL',\n",
    "    #'QGL',\n",
    "    'class'\n",
    "]\n",
    "\n",
    "well_names = [f'WELL-{i:05d}' for i in range(1, 19)]\n",
    "\n",
    "instances = LoadInstances(path_dataset)\n",
    "\n",
    "\n",
    "dataset = instances.load_instance_with_numpy(events_names, columns)    \n",
    "    \n",
    "# Definindo a porcentagem para divisão entre treino e teste\n",
    "train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "# Inicializando listas para guardar índices de treino e teste\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# Processamento genérico para cada classe\n",
    "for event in np.unique(dataset[:, -1]):\n",
    "    # Selecionando índices para a classe atual        \n",
    "    class_indices = np.where(dataset[:, -1] == event)[0]\n",
    "    \n",
    "    # Logando o número de amostras por classe\n",
    "    print(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "    \n",
    "    \n",
    "    # Dividindo os índices da classe atual em treino e teste\n",
    "    class_train_indices, class_test_indices = train_test_split(class_indices, train_size=train_percentage, random_state=42)\n",
    "    \n",
    "    # Logando o número de amostras de treino e teste\n",
    "    logging.info(f'Número de amostras de treino da classe {event}: {len(class_train_indices)}')\n",
    "    logging.info(f'Número de amostras de teste da classe {event}: {len(class_test_indices)}')\n",
    "    \n",
    "    # Adicionando aos índices gerais de treino e teste\n",
    "    train_indices.extend(class_train_indices)\n",
    "    test_indices.extend(class_test_indices)\n",
    "\n",
    "# Convertendo listas para arrays numpy para futura manipulação\n",
    "train_indices = np.array(train_indices)    \n",
    "test_temp_indices = np.array(test_indices)       \n",
    "\n",
    "test_indices, validation_indices = train_test_split(test_temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# Embaralhando os índices (opcional, dependendo da necessidade)\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "\n",
    "# Criando conjuntos de dados de treino e teste\n",
    "dataset_train = dataset[train_indices]\n",
    "dataset_test = dataset[test_indices]\n",
    "dataset_validation = dataset[validation_indices]\n",
    "\n",
    "\n",
    "# Dividindo em features (X) e target (y)\n",
    "X_train, y_train = dataset_train[:, :-1], dataset_train[:, -1]\n",
    "X_test, y_test = dataset_test[:, :-1], dataset_test[:, -1]\n",
    "X_validation, y_validation = dataset_validation[:, :-1], dataset_validation[:, -1]\n",
    "\n",
    "# Delete a primeira coluna (timestamp) das features\n",
    "X_train = np.delete(X_train, 0, axis=1)\n",
    "X_test = np.delete(X_test, 0, axis=1)\n",
    "\n",
    "# Escalonando as features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_validation_scaled = np.column_stack((X_validation[:, 0], scaler.transform(X_validation[:, 1:])))   \n",
    "\n",
    "\n",
    "# Se necessário, você pode combinar as features escalonadas e o target para formar os datasets finais\n",
    "dataset_train_scaled = np.column_stack((X_train_scaled, y_train))\n",
    "dataset_test_scaled = np.column_stack((X_test_scaled, y_test))\n",
    "dataset_validation_scaled = np.column_stack((X_validation_scaled, y_validation))\n",
    "\n",
    "# Obtendo os índices que ordenariam a primeira coluna\n",
    "sort_indices = np.argsort(dataset_validation_scaled[:, 0])\n",
    "\n",
    "# Usando esses índices para reordenar todo o array\n",
    "dataset_validation_sorted = dataset_validation_scaled[sort_indices]\n",
    "\n",
    "# Inicializando previous_timestamp como None para a primeira comparação\n",
    "previous_datetime = None\n",
    "\n",
    "for row in dataset_validation_sorted:\n",
    "    current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if previous_datetime and (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "        # Imprime a hora do timestamp atual se a diferença for maior que 1 hora\n",
    "        print(current_datetime.hour)\n",
    "    \n",
    "    previous_datetime = current_datetime\n",
    "    \n",
    "# Inicializando a lista para armazenar os sub-datasets\n",
    "datasets = []\n",
    "current_dataset = []\n",
    "\n",
    "# Inicializando previous_datetime como None para a primeira comparação\n",
    "previous_datetime = None\n",
    "\n",
    "for row in dataset_validation_sorted:\n",
    "    current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Verifica se é a primeira iteração ou se a diferença é maior que 1 hora\n",
    "    if previous_datetime is None or (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "        # Se não for a primeira iteração e a condição for verdadeira, inicia um novo dataset\n",
    "        if current_dataset:\n",
    "            datasets.append(np.array(current_dataset))\n",
    "            current_dataset = []\n",
    "    \n",
    "    # Adiciona o registro atual ao dataset corrente\n",
    "    current_dataset.append(row)\n",
    "    previous_datetime = current_datetime\n",
    "\n",
    "# Não esqueça de adicionar o último dataset após terminar o loop\n",
    "if current_dataset:\n",
    "    datasets.append(np.array(current_dataset))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
