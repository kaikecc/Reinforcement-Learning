{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sklearn.utils import resample\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent import Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar undersampling no dataset\n",
    "def apply_undersampling(X, y):\n",
    "    logging.info(\"Iniciando o processo de undersampling.\")\n",
    "    \n",
    "    # Concatenar os arrays de features e target para facilitar o resampling\n",
    "    dataset = np.column_stack((X, y))\n",
    "    \n",
    "    # Separar os datasets por classe\n",
    "    datasets_by_class = {label: dataset[dataset[:, -1] == label] for label in np.unique(dataset[:, -1])}\n",
    "    \n",
    "    # Encontrar o tamanho da menor classe\n",
    "    min_class_size = min(len(datasets_by_class[label]) for label in datasets_by_class)\n",
    "    logging.info(f\"Tamanho da menor classe: {min_class_size}\")\n",
    "    \n",
    "    # Aplicar undersampling em cada classe para igualar ao tamanho da menor classe\n",
    "    undersampled_datasets = []\n",
    "    for label in datasets_by_class:\n",
    "        undersampled_data = resample(datasets_by_class[label], replace=False, n_samples=min_class_size, random_state=42)\n",
    "        undersampled_datasets.append(undersampled_data)\n",
    "        logging.info(f\"Classe {label} foi undersampled para {min_class_size} instâncias.\")\n",
    "    \n",
    "       \n",
    "    # Combinar todos os datasets undersampled em um único conjunto\n",
    "    undersampled_dataset = np.vstack(undersampled_datasets)\n",
    "    \n",
    "    # Embaralhar o dataset final para garantir uma distribuição aleatória\n",
    "    np.random.shuffle(undersampled_dataset)\n",
    "    logging.info(\"Dataset final undersampled e embaralhado.\")\n",
    "    \n",
    "    # Separar novamente em X e y\n",
    "    X_undersampled, y_undersampled = undersampled_dataset[:, :-1], undersampled_dataset[:, -1]\n",
    "    \n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras da classe 0: 8467297\n",
      "Número de amostras da classe 6.0: 12951\n",
      "Número de amostras da classe 106.0: 6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | -147     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 12864    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15005    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 250      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 397      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 7217     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 30010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.476    |\n",
      "|    n_updates        | 1000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 494      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 6151     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 647      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 5825     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 708      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 5824     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 60020    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 2500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 781      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 5667     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 75025    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 3251     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 826      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 5581     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 90030    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.181    |\n",
      "|    n_updates        | 4001     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 840      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 5470     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3e+03    |\n",
      "|    ep_rew_mean      | 869      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 5415     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Tempo de Treinamento DQN: 23.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia de 99.13% no conjunto de dados de teste usando DQN\n",
      "Acurácia da 0ª instância: 100.000%\n",
      "Acurácia da 1ª instância: 100.000%\n",
      "Acurácia da 2ª instância: 36.505%\n",
      "Acurácia da 3ª instância: 12.305%\n",
      "Acurácia da 4ª instância: 52.724%\n",
      "Acurácia da 5ª instância: 0.000%\n",
      "Acurácia da 6ª instância: 100.000%\n",
      "Acurácia da 7ª instância: 100.000%\n",
      "Acurácia da 8ª instância: 100.000%\n",
      "Acurácia da 9ª instância: 100.000%\n",
      "Acurácia da 10ª instância: 100.000%\n",
      "Acurácia da 11ª instância: 100.000%\n",
      "Acurácia da 12ª instância: 100.000%\n",
      "Acurácia da 13ª instância: 100.000%\n",
      "Acurácia da 14ª instância: 100.000%\n",
      "Acurácia da 15ª instância: 100.000%\n",
      "Acurácia da 16ª instância: 100.000%\n",
      "Acurácia da 17ª instância: 100.000%\n",
      "Acurácia da 18ª instância: 100.000%\n",
      "Acurácia da 19ª instância: 100.000%\n",
      "Acurácia da 20ª instância: 0.000%\n",
      "Acurácia da 21ª instância: 100.000%\n",
      "Acurácia da 22ª instância: 93.998%\n",
      "Acurácia da 23ª instância: 100.000%\n",
      "Acurácia da 24ª instância: 100.000%\n",
      "Acurácia da 25ª instância: 28.676%\n",
      "Acurácia: 97.089% no conjunto de dados de validação usando DQN\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        # 4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0][0]\n",
    "    \n",
    "    \n",
    "    directory = f'..\\\\..\\\\logs\\\\{event_name}'\n",
    "    path_dataset = '..\\\\..\\\\..\\\\dataset'   \n",
    "    path_model = f'..\\\\models\\\\{event_name}_DQN_Env3W'  \n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'{directory}\\\\{current_time}_{event_name}-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True, encoding='utf-8')\n",
    "\n",
    "    instances = LoadInstances(path_dataset)\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = instances.load_instance_with_numpy(events_names)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "        \n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    # Inicializando listas para guardar índices de treino e teste\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Processamento genérico para cada classe\n",
    "    for event in np.unique(dataset[:, -1]):\n",
    "        # Selecionando índices para a classe atual        \n",
    "        class_indices = np.where(dataset[:, -1] == event)[0]\n",
    "        \n",
    "        # Logando o número de amostras por classe\n",
    "        print(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        logging.info(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        \n",
    "        #O parâmetro random_state=42 garante que essa divisão seja feita de maneira reproducível, ou seja, a função produzirá o mesmo resultado cada vez que for executada com o mesmo estado aleatório. \n",
    "        # Dividindo os índices da classe atual em treino e teste\n",
    "        class_train_indices, class_test_indices = train_test_split(class_indices, train_size=train_percentage) # , random_state=42\n",
    "        \n",
    "        # Logando o número de amostras de treino e teste\n",
    "        logging.info(f'Número de amostras de treino da classe {event}: {len(class_train_indices)}')\n",
    "        logging.info(f'Número de amostras de teste da classe {event}: {len(class_test_indices)}')\n",
    "        \n",
    "        # Adicionando aos índices gerais de treino e teste\n",
    "        train_indices.extend(class_train_indices)\n",
    "        test_indices.extend(class_test_indices)\n",
    "\n",
    "    # Convertendo listas para arrays numpy para futura manipulação\n",
    "    train_indices = np.array(train_indices)    \n",
    "    test_temp_indices = np.array(test_indices)       \n",
    "\n",
    "    test_indices, validation_indices = train_test_split(test_temp_indices, test_size=0.5) # , random_state=42\n",
    "\n",
    "    # Embaralhando os índices (opcional, dependendo da necessidade)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "\n",
    "    # Criando conjuntos de dados de treino e teste\n",
    "    dataset_train = dataset[train_indices]\n",
    "    dataset_test = dataset[test_indices]\n",
    "    dataset_validation = dataset[validation_indices]\n",
    "\n",
    "    logging.info(f'Número de registros de treino: {len(dataset_train)}')\n",
    "    logging.info(f'Número de registros de teste: {len(dataset_test)}')\n",
    "    logging.info(f'Número de registros de validação: {len(dataset_validation)}')\n",
    "    \n",
    "\n",
    "    # Dividindo em features (X) e target (y)\n",
    "    X_train, y_train = dataset_train[:, :-1], dataset_train[:, -1]\n",
    "    X_test, y_test = dataset_test[:, :-1], dataset_test[:, -1]\n",
    "    X_validation, y_validation = dataset_validation[:, :-1], dataset_validation[:, -1]\n",
    "\n",
    "    # Delete a primeira coluna (timestamp) das features\n",
    "    X_train = np.delete(X_train, 0, axis=1)\n",
    "    X_test = np.delete(X_test, 0, axis=1)\n",
    "\n",
    "    # Escalonando as features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_validation_scaled = np.column_stack((X_validation[:, 0], scaler.transform(X_validation[:, 1:])))    \n",
    "   \n",
    "    X_train_undersampled, y_train_undersampled = apply_undersampling(X_train_scaled, y_train)\n",
    "    X_test_undersampled, y_test_undersampled = apply_undersampling(X_test_scaled, y_test) # Se desejar aplicar no teste também\n",
    "\n",
    "\n",
    "    # Se necessário, você pode combinar as features escalonadas e o target para formar os datasets finais\n",
    "    #dataset_train_scaled = np.column_stack((X_train_scaled, y_train))\n",
    "    #dataset_test_scaled = np.column_stack((X_test_scaled, y_test))\n",
    "    dataset_train_scaled = np.column_stack((X_train_undersampled, y_train_undersampled))\n",
    "    dataset_test_scaled = np.column_stack((X_test_undersampled, y_test_undersampled))\n",
    "    dataset_validation_scaled = np.column_stack((X_validation_scaled, y_validation))\n",
    "\n",
    "    logging.info(f'Número de registros de treino undersampling: {len(dataset_train_scaled)}')\n",
    "    logging.info(f'Número de registros de teste undersampling: {len(dataset_test_scaled)}')\n",
    "    logging.info(f'Número de registros de validação: {len(dataset_validation_scaled)}')       \n",
    "    logging.info(f'Fim divisão do dataset em treino e teste')\n",
    "    \n",
    "\n",
    "    logging.info(f'Iniciando treinamento do algoritmo DQN')    \n",
    "    start_time = time.time()\n",
    "    agente = Agent(path_model)\n",
    "    agente.env3W_dqn(dataset_train_scaled, n_envs = 5)  \n",
    "    print(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f'Fim treinamento do algoritmo DQN')\n",
    "\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do algoritmo DQN conjunto de teste')\n",
    "    accuracy, dqn_model = agente.env3W_dqn_eval(dataset_test_scaled, n_envs = 1)\n",
    "    print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando DQN')\n",
    "    logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando DQN')\n",
    "    logging.info(f'Fim avaliação  do algoritmo DQN conjunto de teste')\n",
    "\n",
    "    if accuracy > 0.8:\n",
    "        logging.info(f'Iniciando a separação dos grupos de dados para validação individual')\n",
    "        # Obtendo os índices que ordenariam a primeira coluna\n",
    "        sort_indices = np.argsort(dataset_validation_scaled[:, 0])\n",
    "\n",
    "        # Usando esses índices para reordenar todo o array\n",
    "        dataset_validation_sorted = dataset_validation_scaled[sort_indices]\n",
    "        \n",
    "        # Inicializando a lista para armazenar os sub-datasets\n",
    "        datasets = []\n",
    "        current_dataset = []\n",
    "\n",
    "        # Inicializando previous_datetime como None para a primeira comparação\n",
    "        previous_datetime = None\n",
    "\n",
    "        for row in dataset_validation_sorted:\n",
    "            current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Verifica se é a primeira iteração ou se a diferença é maior que 1 hora\n",
    "            if previous_datetime is None or (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "                # Se não for a primeira iteração e a condição for verdadeira, inicia um novo dataset\n",
    "                if current_dataset:\n",
    "                    datasets.append(np.array(current_dataset))\n",
    "                    current_dataset = []\n",
    "            \n",
    "            # Adiciona o registro atual ao dataset corrente\n",
    "            current_dataset.append(row)\n",
    "            previous_datetime = current_datetime\n",
    "\n",
    "        # Não esqueça de adicionar o último dataset após terminar o loop\n",
    "        if current_dataset:\n",
    "            datasets.append(np.array(current_dataset))\n",
    "        \n",
    "\n",
    "        logging.info(f'Fim da separação dos grupos de dados para validação com {len(datasets)} grupos de instâncias')\n",
    "        \n",
    "        count = -1\n",
    "        acc_total = []\n",
    "        array_prec_total = []\n",
    "        for dataset_test in datasets:\n",
    "            acc = 0\n",
    "            count += 1\n",
    "            logging.info(f'Iniciando predição da {count}ª instância para teste usando DQN')\n",
    "            array_action_pred = []\n",
    "            for i in range(0, len(dataset_test)):\n",
    "                obs = dataset_test[i, 1:-1].astype(np.float32)\n",
    "                action, _states = dqn_model.predict(obs, deterministic=True)  \n",
    "                array_action_pred.append(action)\n",
    "\n",
    "                true_action = dataset_test[i, -1]\n",
    "                if true_action == 0:\n",
    "                    acc +=  1 if action == 0 else 0\n",
    "                elif true_action in range(1, 10):\n",
    "                    acc +=  1 if action == 1 else 0\n",
    "                elif true_action in range(101, 110):  # Corrigido para refletir o intervalo correto\n",
    "                    acc +=  1 if action == 1 else 0  \n",
    "                \n",
    "                    \n",
    "            acc_total.append(acc)\n",
    "            array_prec_total.append(len(array_action_pred))        \n",
    "            final_acc = int(acc)/len(array_action_pred) * 100\n",
    "            logging.info(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            print(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "            logging.info(f'Fim predição da instância de teste DQN')    \n",
    "        \n",
    "            \n",
    "            df = pd.DataFrame(expanded_array, columns = ['timestamp', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'action'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']] = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']].astype('float32')\n",
    "            df['class'] = df['class'].astype(float).astype('int16')\n",
    "            df['action'] = df['action'].astype(float).astype('int16')\n",
    "\n",
    "\n",
    "            explora = exploration(df)\n",
    "            explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'], _title = f'[{count}] - {event_name} - DQN')\n",
    "        \n",
    "        logging.info(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando DQN')\n",
    "        print(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando DQN')\n",
    "\n",
    "    else:\n",
    "        logging.info(f'Acurácia insuficiente para validação individual')\n",
    "        print(f'Acurácia insuficiente para validação individual')\n",
    "    logging.info(f'Concluído a execução do aprendizado por reforço')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
