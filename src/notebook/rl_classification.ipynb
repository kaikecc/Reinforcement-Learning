{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sklearn.utils import resample\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent import Agent\n",
    "from classes._Supervised import Supervised\n",
    "from classes._ValidationModel import ValidationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        # 0: 'Normal',\n",
    "        1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        # 4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0][0]\n",
    "    \n",
    "    models = ['DQN'] # 'DQN' or 'PPO', 'RNA' \n",
    "    \n",
    "    for model_type in models:\n",
    "        for event_name in [value for key, value in events_names.items() if key != 0]:\n",
    "\n",
    "            directory = f'..\\\\..\\\\logs\\\\{event_name}'\n",
    "            path_dataset = '..\\\\..\\\\..\\\\dataset'   \n",
    "            path_model = f'..\\\\models\\\\{event_name}\\\\{model_type}'  \n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            \n",
    "            if not os.path.exists(path_model):\n",
    "                os.makedirs(path_model)                \n",
    "            \n",
    "                    \n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            log_filename = f'{directory}\\\\{current_time}_{event_name}_{model_type}-log.txt'\n",
    "            # Configuração do Logging\n",
    "            logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True, encoding='utf-8')\n",
    "\n",
    "            instances = LoadInstances(path_dataset)\n",
    "            \n",
    "            logging.info(f'Iniciando carregamento do dataset')\n",
    "            dataset = instances.load_instance_with_numpy(events_names)    \n",
    "            logging.info(f'Fim carregamento do dataset')\n",
    "            \n",
    "            logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "                \n",
    "            # Definindo a porcentagem para divisão entre treino e teste\n",
    "            train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "            dataset_train_scaled, dataset_test_scaled, dataset_validation_scaled = instances.data_preparation(dataset, train_percentage)\n",
    "\n",
    "            if model_type == 'DQN':\n",
    "                logging.info(f'Iniciando treinamento do algoritmo DQN')    \n",
    "                start_time = time.time()\n",
    "                agente = Agent(path_model)\n",
    "                agente.env3W_dqn(dataset_train_scaled, n_envs = 5)  \n",
    "                print(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f'Fim treinamento do algoritmo DQN')\n",
    "\n",
    "\n",
    "                logging.info(f'Iniciando avaliação do algoritmo DQN conjunto de teste')\n",
    "                accuracy, model_agent = agente.env3W_dqn_eval(dataset_test_scaled, n_envs = 1)\n",
    "                print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando DQN')\n",
    "                logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando DQN')\n",
    "                logging.info(f'Fim avaliação  do algoritmo DQN conjunto de teste')\n",
    "                \n",
    "            elif model_type == 'PPO':      \n",
    "            \n",
    "                start_time = time.time()\n",
    "                agente = Agent(path_model)\n",
    "                agente.env3W_ppo(dataset_train_scaled, n_envs = 5)  \n",
    "                print(f\"Tempo de Treinamento {model}: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f\"Tempo de Treinamento {model}: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f'Fim treinamento do algoritmo {model}')\n",
    "\n",
    "                logging.info(f'Iniciando avaliação do algoritmo {model} conjunto de teste')\n",
    "                accuracy, model_agent = agente.env3W_ppo_eval(dataset_test_scaled, n_envs = 1)\n",
    "                print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando {model}')\n",
    "                logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando {model}')\n",
    "                logging.info(f'Fim avaliação  do algoritmo PPO conjunto de teste')\n",
    "            \n",
    "            elif model_type == 'RNA':\n",
    "                logging.info(f'Iniciando treinamento do modelo RNA')  \n",
    "                \n",
    "                supervised = Supervised(path_model, dataset_train_scaled, dataset_test_scaled)\n",
    "                start_time = time.time()\n",
    "                model_agent = supervised.keras_train()  \n",
    "                print(f\"Tempo de Treinamento RNA: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f\"Tempo de Treinamento RNA: {round(time.time() - start_time, 2)}s\")\n",
    "                logging.info(f'Fim treinamento do modelo RNA')\n",
    "\n",
    "                logging.info(f'Iniciando avaliação do modelo RNA conjunto de teste')\n",
    "                accuracy = supervised.keras_evaluate()\n",
    "                print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando RNA')\n",
    "                logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando RNA')\n",
    "                logging.info(f'Fim avaliação  do modelo RNA conjunto de teste')\n",
    "\n",
    "            logging.info(f'Iniciando a validação do modelo {model_type}') \n",
    "            validation = ValidationModel(model_type, event_name)\n",
    "\n",
    "            validation.validation_model(accuracy, dataset_validation_scaled, model_agent)\n",
    "\n",
    "            logging.info(f'Concluído a execução do algoritmo {model_type} para o evento {event_name}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando novo modelo de ambiente para detecção de Aumento Abrupto de BSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Tempo de Treinamento DQN: 23.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia de 61.37% no conjunto de dados de teste usando DQN\n",
      "Using cpu device\n",
      "Logging to ..\\models\\Abrupt Increase of BSW\\DQN\\tensorboard_logs\\PPO_2\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 16248 |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 0     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2498       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047476 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | 5.74e-05   |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.774      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 3.19       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1959      |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 30720     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0283886 |\n",
      "|    clip_fraction        | 0.464     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.607    |\n",
      "|    explained_variance   | 8e-05     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 1.12      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0534   |\n",
      "|    value_loss           | 2.53      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1762       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05537925 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.44      |\n",
      "|    explained_variance   | -8.82e-05  |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.724      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0449    |\n",
      "|    value_loss           | 1.91       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1675       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15161872 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.224     |\n",
      "|    explained_variance   | 0.00161    |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.695      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    value_loss           | 1.45       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1615      |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 38        |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7465174 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0162   |\n",
      "|    explained_variance   | 0.00055   |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 0.548     |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0346   |\n",
      "|    value_loss           | 1.2       |\n",
      "---------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1575           |\n",
      "|    iterations           | 7              |\n",
      "|    time_elapsed         | 45             |\n",
      "|    total_timesteps      | 71680          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000110889945 |\n",
      "|    clip_fraction        | 0.000273       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.00127       |\n",
      "|    explained_variance   | 0.0049         |\n",
      "|    learning_rate        | 0.001          |\n",
      "|    loss                 | 0.0431         |\n",
      "|    n_updates            | 60             |\n",
      "|    policy_gradient_loss | -0.000293      |\n",
      "|    value_loss           | 0.127          |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1545          |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016648458 |\n",
      "|    clip_fraction        | 0.00375       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0275       |\n",
      "|    explained_variance   | 0.778         |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.214         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 0.00222       |\n",
      "|    value_loss           | 0.639         |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1527       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61614853 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.032     |\n",
      "|    explained_variance   | 0.0616     |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.436      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    value_loss           | 0.979      |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1511          |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031364305 |\n",
      "|    clip_fraction        | 8.79e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000147     |\n",
      "|    explained_variance   | 0.785         |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 0.00202       |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000148     |\n",
      "|    value_loss           | 0.0042        |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1503      |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 74        |\n",
      "|    total_timesteps      | 112640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.71e-05 |\n",
      "|    explained_variance   | 0.883     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 0.000747  |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -3.1e-09  |\n",
      "|    value_loss           | 0.00208   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1496      |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 82        |\n",
      "|    total_timesteps      | 122880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.89e-05 |\n",
      "|    explained_variance   | 0.84      |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 0.000557  |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -2.73e-08 |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "Tempo de Treinamento PPO: 119.14s\n",
      "Acurácia de 10.20% no conjunto de dados de teste usando PPO\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._Env3WGym_copy import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent_copy import Agent\n",
    "import numpy as np\n",
    "\n",
    "path_dataset = '..\\\\..\\\\..\\\\dataset' \n",
    "\n",
    "events_names = {\n",
    "        # 0: 'Normal',\n",
    "        1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        # 4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "event_name = [value for key, value in events_names.items() if key != 0][0]  \n",
    "\n",
    "instances = LoadInstances(path_dataset)\n",
    "\n",
    "_, array_list = instances.load_instance_with_numpy(events_names)\n",
    "\n",
    "# Step 1: Concatenate the numerical part of all arrays, excluding the timestamp\n",
    "# Assuming each sub-array has the same number of numerical features\n",
    "numerical_data = [sub_array[:, 1:-1].astype(float) for sub_array in array_list]  # Convert to float\n",
    "numerical_data_concatenated = np.concatenate(numerical_data, axis=0)\n",
    "\n",
    "# Step 2: Normalize the numerical data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numerical_data_concatenated)\n",
    "array_list_scaled = [scaler.transform(sub_array[:, 1:-1].astype(float)) for sub_array in array_list]\n",
    "\n",
    "import time\n",
    "event_name = [value for key, value in events_names.items() if key != 0][0]  \n",
    "\n",
    "\n",
    "path_model = f'..\\\\models\\\\{event_name}\\\\DQN'   \n",
    "start_time = time.time()\n",
    "agente = Agent(path_model)\n",
    "agente.env3W_dqn(array_list_scaled[1:], n_envs = 5)  \n",
    "print(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "\n",
    "\n",
    "accuracy, model_agent = agente.env3W_dqn_eval(array_list_scaled[0], n_envs = 1)\n",
    "print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando DQN')\n",
    "\n",
    "agente.env3W_ppo(array_list_scaled[1:], n_envs = 5)  \n",
    "print(f\"Tempo de Treinamento PPO: {round(time.time() - start_time, 2)}s\")\n",
    "\n",
    "\n",
    "accuracy, model_agent = agente.env3W_ppo_eval(array_list_scaled[0], n_envs = 1)\n",
    "print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando PPO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
