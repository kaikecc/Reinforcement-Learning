{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sklearn.utils import resample\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent import Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_dqn(accuracy, dataset_validation_scaled, dqn_model, event_name):\n",
    "       \n",
    "    if accuracy > 0.8:\n",
    "       \n",
    "        logging.info(f'Iniciando a separação dos grupos de dados para validação individual')\n",
    "        # Obtendo os índices que ordenariam a primeira coluna\n",
    "        sort_indices = np.argsort(dataset_validation_scaled[:, 0])\n",
    "\n",
    "        # Usando esses índices para reordenar todo o array\n",
    "        dataset_validation_sorted = dataset_validation_scaled[sort_indices]\n",
    "        \n",
    "        # Inicializando a lista para armazenar os sub-datasets\n",
    "        datasets = []\n",
    "        current_dataset = []\n",
    "\n",
    "        # Inicializando previous_datetime como None para a primeira comparação\n",
    "        previous_datetime = None\n",
    "\n",
    "        for row in dataset_validation_sorted:\n",
    "            current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Verifica se é a primeira iteração ou se a diferença é maior que 1 hora\n",
    "            if previous_datetime is None or (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "                # Se não for a primeira iteração e a condição for verdadeira, inicia um novo dataset\n",
    "                if current_dataset:\n",
    "                    datasets.append(np.array(current_dataset))\n",
    "                    current_dataset = []\n",
    "            \n",
    "            # Adiciona o registro atual ao dataset corrente\n",
    "            current_dataset.append(row)\n",
    "            previous_datetime = current_datetime\n",
    "\n",
    "        # Não esqueça de adicionar o último dataset após terminar o loop\n",
    "        if current_dataset:\n",
    "            datasets.append(np.array(current_dataset))        \n",
    "\n",
    "        logging.info(f'Fim da separação dos grupos de dados para validação com {len(datasets)} grupos de instâncias')\n",
    "        \n",
    "        count = -1\n",
    "        acc_total = []\n",
    "        array_prec_total = []\n",
    "        for dataset_test in datasets:\n",
    "            acc = 0\n",
    "            count += 1\n",
    "            logging.info(f'Iniciando predição da {count}ª instância de validação usando DQN')\n",
    "            array_action_pred = []\n",
    "            for i in range(0, len(dataset_test)):\n",
    "                obs = dataset_test[i, 1:-1].astype(np.float32)\n",
    "                action, _states = dqn_model.predict(obs, deterministic=True)  \n",
    "                array_action_pred.append(action)\n",
    "\n",
    "                true_action = dataset_test[i, -1]\n",
    "                if true_action == 0:\n",
    "                    acc +=  1 if action == 0 else 0\n",
    "                elif true_action in range(1, 10):\n",
    "                    acc +=  1 if action == 1 else 0\n",
    "                elif true_action in range(101, 110):  # Corrigido para refletir o intervalo correto\n",
    "                    acc +=  1 if action == 1 else 0  \n",
    "                \n",
    "                    \n",
    "            acc_total.append(acc)\n",
    "            array_prec_total.append(len(array_action_pred))        \n",
    "            final_acc = int(acc)/len(array_action_pred) * 100\n",
    "            \n",
    "            expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "              \n",
    "        \n",
    "            \n",
    "            df = pd.DataFrame(expanded_array, columns = ['timestamp', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'action'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']] = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']].astype('float32')\n",
    "            df['class'] = df['class'].astype(float).astype('int16')\n",
    "            df['action'] = df['action'].astype(float).astype('int16')\n",
    "\n",
    "            # Faça um filtro no dataframe df para exibir apenas os registros cujo o valor da coluna class é igual a 0\n",
    "\n",
    "            numerator_normal = len(df[(df['class'] == 0) & (df['action'] == 0)])\n",
    "            denominator_normal = len(df.loc[df['class'] == 0])\n",
    "            acc_normal = numerator_normal / denominator_normal if denominator_normal > 0 else 0\n",
    "\n",
    "            numerator_falha = len(df[(df['class'] != 0) & (df['action'] == 1)])\n",
    "            denominator_falha = len(df.loc[df['class'] != 0])\n",
    "            acc_falha = numerator_falha / denominator_falha if denominator_falha > 0 else 0\n",
    "\n",
    "            logging.info(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            print(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            logging.info(f'Acurácia de Não-Falha na {count}ª instância: {acc_normal * 100:.3f}%')\n",
    "            logging.info(f'Acurácia das Falha na {count}ª instância: {acc_falha * 100:.3f}%')\n",
    "            #logging.info(f'Acurácia de Detecção de Falhas na {count}ª instância : {acc_ref * 100:.3f}%')\n",
    "            logging.info(f'Fim predição da instância de teste DQN')  \n",
    "\n",
    "            additional_labels = [\n",
    "                f'Acurácia (Teste): {accuracy * 100:.1f}%', \n",
    "                f'Acurácia (Validação): {final_acc:.1f}%',  \n",
    "                f'Acurácia de Não-Falha: {acc_normal * 100:.1f}%',  \n",
    "                f'Acurácia de Falha: {acc_falha * 100:.1f}%' \n",
    "            ]\n",
    "\n",
    "            explora = exploration(df)\n",
    "            explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'], _title = f'[{count}] - {event_name} - DQN', additional_labels =  additional_labels)\n",
    "        \n",
    "        logging.info(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando DQN')\n",
    "        print(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando DQN')\n",
    "\n",
    "    else:\n",
    "        logging.info(f'Acurácia insuficiente para validação individual')\n",
    "        print(f'Acurácia insuficiente para validação individual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras da classe 0: 8430978\n",
      "Número de amostras da classe 4: 2460270\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0][0]\n",
    "    \n",
    "    \n",
    "    directory = f'..\\\\..\\\\logs\\\\{event_name}'\n",
    "    path_dataset = '..\\\\..\\\\..\\\\dataset'   \n",
    "    path_model = f'..\\\\models\\\\{event_name}'  \n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'{directory}\\\\{current_time}_{event_name}-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True, encoding='utf-8')\n",
    "\n",
    "    instances = LoadInstances(path_dataset)\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = instances.load_instance_with_numpy(events_names)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "        \n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    dataset_train_scaled, dataset_test_scaled, dataset_validation_scaled = instances.data_preparation(dataset, train_percentage)\n",
    "\n",
    "    '''logging.info(f'Iniciando treinamento do algoritmo DQN')    \n",
    "    start_time = time.time()\n",
    "    agente = Agent(path_model)\n",
    "    agente.env3W_dqn(dataset_train_scaled, n_envs = 5)  \n",
    "    print(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f'Fim treinamento do algoritmo DQN')\n",
    "\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do algoritmo DQN conjunto de teste')\n",
    "    accuracy, dqn_model = agente.env3W_dqn_eval(dataset_test_scaled, n_envs = 1)\n",
    "    print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando DQN')\n",
    "    logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando DQN')\n",
    "    logging.info(f'Fim avaliação  do algoritmo DQN conjunto de teste')'''\n",
    "    \n",
    "    start_time = time.time()\n",
    "    agente = Agent(path_model)\n",
    "    agente.env3W_ppo(dataset_train_scaled, n_envs = 5)  \n",
    "    print(f\"Tempo de Treinamento PPO: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f\"Tempo de Treinamento PPO: {round(time.time() - start_time, 2)}s\")\n",
    "    logging.info(f'Fim treinamento do algoritmo PPO')\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do algoritmo PPO conjunto de teste')\n",
    "    accuracy, ppo_model = agente.env3W_ppo_eval(dataset_test_scaled, n_envs = 1)\n",
    "    print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando PPO')\n",
    "    logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando PPO')\n",
    "    logging.info(f'Fim avaliação  do algoritmo PPO conjunto de teste')\n",
    "\n",
    "    logging.info(f'Iniciando a validação do modelo PPO') \n",
    "    validation_dqn(accuracy, dataset_validation_scaled, ppo_model, event_name)\n",
    "\n",
    "    logging.info(f'Concluído a execução do aprendizado por reforço')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ..\\models\\Flow Instability\\tensorboard_logs\\PPO_2\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 6613  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 1     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1778        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018178323 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -0.0103     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0405     |\n",
      "|    value_loss           | 5.89        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1538        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018632267 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | -0.00967    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1371        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027613545 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.00662     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1344       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03298659 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.433     |\n",
      "|    explained_variance   | 0.0295     |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 1.62       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    value_loss           | 3.82       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1332        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010694163 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.0457      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1328        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011729376 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.043       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1324        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005896254 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.0427      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.962       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1322        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004417827 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.057       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1320         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054645343 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.0582       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00357     |\n",
      "|    value_loss           | 2.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1318         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024731748 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.0524       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.11         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 2.39         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1317         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025937476 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.0568       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 2.45         |\n",
      "------------------------------------------\n",
      "Tempo de Treinamento PPO: 104.11s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "agente = Agent(path_model)\n",
    "agente.env3W_ppo(dataset_train_scaled, n_envs = 5)  \n",
    "print(f\"Tempo de Treinamento PPO: {round(time.time() - start_time, 2)}s\")\n",
    "logging.info(f\"Tempo de Treinamento PPO: {round(time.time() - start_time, 2)}s\")\n",
    "logging.info(f'Fim treinamento do algoritmo PPO')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia de 94.61% no conjunto de dados de teste usando PPO\n"
     ]
    }
   ],
   "source": [
    "logging.info(f'Iniciando avaliação do algoritmo PPO conjunto de teste')\n",
    "accuracy, ppo_model = agente.env3W_ppo_eval(dataset_test_scaled, n_envs = 1)\n",
    "print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando PPO')\n",
    "logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando PPO')\n",
    "logging.info(f'Fim avaliação  do algoritmo PPO conjunto de teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da 0ª instância: 100.000%\n",
      "Acurácia da 1ª instância: 28.708%\n",
      "Acurácia da 2ª instância: 80.999%\n",
      "Acurácia da 3ª instância: 100.000%\n",
      "Acurácia da 4ª instância: 100.000%\n",
      "Acurácia da 5ª instância: 100.000%\n",
      "Acurácia da 6ª instância: 100.000%\n",
      "Acurácia da 7ª instância: 100.000%\n",
      "Acurácia da 8ª instância: 67.682%\n",
      "Acurácia da 9ª instância: 100.000%\n",
      "Acurácia da 10ª instância: 100.000%\n",
      "Acurácia da 11ª instância: 100.000%\n",
      "Acurácia da 12ª instância: 100.000%\n",
      "Acurácia da 13ª instância: 100.000%\n",
      "Acurácia da 14ª instância: 100.000%\n",
      "Acurácia da 15ª instância: 54.755%\n",
      "Acurácia da 16ª instância: 0.000%\n",
      "Acurácia da 17ª instância: 0.000%\n",
      "Acurácia da 18ª instância: 100.000%\n",
      "Acurácia da 19ª instância: 50.154%\n",
      "Acurácia da 20ª instância: 100.000%\n",
      "Acurácia da 21ª instância: 100.000%\n",
      "Acurácia da 22ª instância: 100.000%\n",
      "Acurácia da 23ª instância: 100.000%\n",
      "Acurácia da 24ª instância: 100.000%\n",
      "Acurácia da 25ª instância: 98.802%\n",
      "Acurácia da 26ª instância: 75.682%\n",
      "Acurácia da 27ª instância: 0.000%\n",
      "Acurácia da 28ª instância: 100.000%\n",
      "Acurácia da 29ª instância: 100.000%\n",
      "Acurácia da 30ª instância: 100.000%\n",
      "Acurácia da 31ª instância: 100.000%\n",
      "Acurácia da 32ª instância: 100.000%\n",
      "Acurácia da 33ª instância: 100.000%\n",
      "Acurácia da 34ª instância: 100.000%\n",
      "Acurácia da 35ª instância: 100.000%\n",
      "Acurácia da 36ª instância: 100.000%\n",
      "Acurácia da 37ª instância: 100.000%\n",
      "Acurácia: 94.600% no conjunto de dados de validação usando DQN\n"
     ]
    }
   ],
   "source": [
    "logging.info(f'Iniciando a validação do modelo PPO') \n",
    "validation_dqn(accuracy, dataset_validation_scaled, ppo_model, event_name)\n",
    "\n",
    "logging.info(f'Concluído a execução do aprendizado por reforço')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
