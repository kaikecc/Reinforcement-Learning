{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sklearn.utils import resample\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "from classes._LoadInstances import LoadInstances\n",
    "from classes._Agent import Agent\n",
    "from classes._Supervised import Supervised\n",
    "from classes._ValidationModel import ValidationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras da classe 0: 8467297\n",
      "Número de amostras da classe 6.0: 12951\n",
      "Número de amostras da classe 106.0: 6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Tempo de Treinamento DQN: 23.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia de 99.54% no conjunto de dados de teste usando DQN\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        # 4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    event_name = [value for key, value in events_names.items() if key != 0][0]\n",
    "    \n",
    "    model = 'DQN' # 'DQN' or 'PPO', 'RNN' \n",
    "    \n",
    "    directory = f'..\\\\..\\\\logs\\\\{event_name}'\n",
    "    path_dataset = '..\\\\..\\\\..\\\\dataset'   \n",
    "    path_model = f'..\\\\models\\\\{event_name}\\\\{model}'  \n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "        \n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'{directory}\\\\{current_time}_{event_name}_{model}-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True, encoding='utf-8')\n",
    "\n",
    "    instances = LoadInstances(path_dataset)\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = instances.load_instance_with_numpy(events_names)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "        \n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    dataset_train_scaled, dataset_test_scaled, dataset_validation_scaled = instances.data_preparation(dataset, train_percentage)\n",
    "\n",
    "    if model == 'DQN':\n",
    "        logging.info(f'Iniciando treinamento do algoritmo DQN')    \n",
    "        start_time = time.time()\n",
    "        agente = Agent(path_model)\n",
    "        agente.env3W_dqn(dataset_train_scaled, n_envs = 5)  \n",
    "        print(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f\"Tempo de Treinamento DQN: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f'Fim treinamento do algoritmo DQN')\n",
    "\n",
    "\n",
    "        logging.info(f'Iniciando avaliação do algoritmo DQN conjunto de teste')\n",
    "        accuracy, dqn_model = agente.env3W_dqn_eval(dataset_test_scaled, n_envs = 1)\n",
    "        print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando DQN')\n",
    "        logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando DQN')\n",
    "        logging.info(f'Fim avaliação  do algoritmo DQN conjunto de teste')\n",
    "        \n",
    "    elif model == 'PPO':      \n",
    "    \n",
    "        start_time = time.time()\n",
    "        agente = Agent(path_model)\n",
    "        agente.env3W_ppo(dataset_train_scaled, n_envs = 5)  \n",
    "        print(f\"Tempo de Treinamento {model}: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f\"Tempo de Treinamento {model}: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f'Fim treinamento do algoritmo {model}')\n",
    "\n",
    "        logging.info(f'Iniciando avaliação do algoritmo {model} conjunto de teste')\n",
    "        accuracy, ppo_model = agente.env3W_ppo_eval(dataset_test_scaled, n_envs = 1)\n",
    "        print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando {model}')\n",
    "        logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando {model}')\n",
    "        logging.info(f'Fim avaliação  do algoritmo PPO conjunto de teste')\n",
    "    \n",
    "    elif model == 'RNN':\n",
    "        logging.info(f'Iniciando treinamento do modelo RNN')  \n",
    "        \n",
    "        supervised = Supervised(dataset_train_scaled, dataset_test_scaled)\n",
    "        start_time = time.time()\n",
    "        model = supervised.keras_train()  \n",
    "        print(f\"Tempo de Treinamento RNN: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f\"Tempo de Treinamento RNN: {round(time.time() - start_time, 2)}s\")\n",
    "        logging.info(f'Fim treinamento do modelo RNN')\n",
    "\n",
    "        logging.info(f'Iniciando avaliação do modelo RNN conjunto de teste')\n",
    "        accuracy = supervised.keras_evaluate(model)\n",
    "        print(f'Acurácia de {accuracy * 100:.2f}% no conjunto de dados de teste usando RNN')\n",
    "        logging.info(f'Acurácia de {accuracy:.5f} no conjunto de dados de teste usando RNN')\n",
    "        logging.info(f'Fim avaliação  do modelo RNN conjunto de teste')\n",
    "\n",
    "    logging.info(f'Iniciando a validação do modelo {model}') \n",
    "    #validation_dqn(accuracy, dataset_validation_scaled, ppo_model, event_name, model)\n",
    "\n",
    "    #logging.info(f'Concluído a execução do aprendizado por reforço')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = ValidationModel(model, event_name)\n",
    "\n",
    "validation.validation_model(accuracy, dataset_validation_scaled, dqn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_dqn(accuracy, dataset_validation_scaled, model, event_name, model_name):\n",
    "       \n",
    "    if accuracy > 0.8:\n",
    "       \n",
    "        logging.info(f'Iniciando a separação dos grupos de dados para validação individual')\n",
    "        # Obtendo os índices que ordenariam a primeira coluna\n",
    "        sort_indices = np.argsort(dataset_validation_scaled[:, 0])\n",
    "\n",
    "        # Usando esses índices para reordenar todo o array\n",
    "        dataset_validation_sorted = dataset_validation_scaled[sort_indices]\n",
    "        \n",
    "        # Inicializando a lista para armazenar os sub-datasets\n",
    "        datasets = []\n",
    "        current_dataset = []\n",
    "\n",
    "        # Inicializando previous_datetime como None para a primeira comparação\n",
    "        previous_datetime = None\n",
    "\n",
    "        for row in dataset_validation_sorted:\n",
    "            current_datetime = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Verifica se é a primeira iteração ou se a diferença é maior que 1 hora\n",
    "            if previous_datetime is None or (current_datetime - previous_datetime).total_seconds() / 3600 > 1:\n",
    "                # Se não for a primeira iteração e a condição for verdadeira, inicia um novo dataset\n",
    "                if current_dataset:\n",
    "                    datasets.append(np.array(current_dataset))\n",
    "                    current_dataset = []\n",
    "            \n",
    "            # Adiciona o registro atual ao dataset corrente\n",
    "            current_dataset.append(row)\n",
    "            previous_datetime = current_datetime\n",
    "\n",
    "        # Não esqueça de adicionar o último dataset após terminar o loop\n",
    "        if current_dataset:\n",
    "            datasets.append(np.array(current_dataset))        \n",
    "\n",
    "        logging.info(f'Fim da separação dos grupos de dados para validação com {len(datasets)} grupos de instâncias')\n",
    "        \n",
    "        count = -1\n",
    "        acc_total = []\n",
    "        array_prec_total = []\n",
    "        for dataset_test in datasets:\n",
    "            acc = 0\n",
    "            count += 1\n",
    "            logging.info(f'Iniciando predição da {count}ª instância de validação usando {model_name}')\n",
    "            array_action_pred = []\n",
    "            for i in range(0, len(dataset_test)):\n",
    "                \n",
    "                if model_name == 'RNN':\n",
    "                    obs = np.expand_dims(dataset_test[i, 1:-1].astype(np.float32), axis=0)\n",
    "                    action_probs = model.predict(obs, verbose=0)\n",
    "                    action = np.argmax(action_probs, axis=1)  # Converte probabilidades em classe prevista\n",
    "                else:\n",
    "                    obs = dataset_test[i, 1:-1].astype(np.float32)\n",
    "                    action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "                \n",
    "                   \n",
    "                array_action_pred.append(action)\n",
    "\n",
    "                true_action = dataset_test[i, -1]\n",
    "                if true_action == 0:\n",
    "                    acc +=  1 if action == 0 else 0\n",
    "                elif true_action in range(1, 10):\n",
    "                    acc +=  1 if action == 1 else 0\n",
    "                elif true_action in range(101, 110):  # Corrigido para refletir o intervalo correto\n",
    "                    acc +=  1 if action == 1 else 0  \n",
    "                \n",
    "                    \n",
    "            acc_total.append(acc)\n",
    "            array_prec_total.append(len(array_action_pred))        \n",
    "            final_acc = int(acc)/len(array_action_pred) * 100\n",
    "            \n",
    "            expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "              \n",
    "        \n",
    "            \n",
    "            df = pd.DataFrame(expanded_array, columns = ['timestamp', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'action'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']] = df[['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']].astype('float32')\n",
    "            df['class'] = df['class'].astype(float).astype('int16')\n",
    "            df['action'] = df['action'].astype(float).astype('int16')\n",
    "\n",
    "            # Faça um filtro no dataframe df para exibir apenas os registros cujo o valor da coluna class é igual a 0\n",
    "\n",
    "            numerator_normal = len(df[(df['class'] == 0) & (df['action'] == 0)])\n",
    "            denominator_normal = len(df.loc[df['class'] == 0])\n",
    "            acc_normal = numerator_normal / denominator_normal if denominator_normal > 0 else 0\n",
    "\n",
    "            numerator_falha = len(df[(df['class'] != 0) & (df['action'] == 1)])\n",
    "            denominator_falha = len(df.loc[df['class'] != 0])\n",
    "            acc_falha = numerator_falha / denominator_falha if denominator_falha > 0 else 0\n",
    "\n",
    "            logging.info(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            print(f'Acurácia da {count}ª instância: {final_acc:.3f}%')\n",
    "            logging.info(f'Acurácia de Não-Falha na {count}ª instância: {acc_normal * 100:.3f}%')\n",
    "            logging.info(f'Acurácia das Falha na {count}ª instância: {acc_falha * 100:.3f}%')\n",
    "            #logging.info(f'Acurácia de Detecção de Falhas na {count}ª instância : {acc_ref * 100:.3f}%')\n",
    "            logging.info(f'Fim predição da instância de teste {model_name}')  \n",
    "\n",
    "            additional_labels = [\n",
    "                f'Acurácia (Teste): {accuracy * 100:.1f}%', \n",
    "                f'Acurácia (Validação): {final_acc:.1f}%',  \n",
    "                f'Acurácia de Não-Falha: {acc_normal * 100:.1f}%',  \n",
    "                f'Acurácia de Falha: {acc_falha * 100:.1f}%' \n",
    "            ]\n",
    "\n",
    "            explora = exploration(df)\n",
    "            explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'], _title = f'[{count}] - {event_name} - {model_name}', additional_labels =  additional_labels, model = model_name)\n",
    "        \n",
    "        logging.info(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando {model_name}')\n",
    "        print(f'Acurácia: {sum(acc_total)/sum(array_prec_total) * 100:.3f}% no conjunto de dados de validação usando {model_name}')\n",
    "\n",
    "    else:\n",
    "        logging.info(f'Acurácia insuficiente para validação individual')\n",
    "        print(f'Acurácia insuficiente para validação individual')\n",
    "\n",
    "validation_dqn(accuracy, dataset_validation_scaled, dqn_model, event_name, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
