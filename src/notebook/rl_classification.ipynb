{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 08:41:24.382465: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 08:41:24.676304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 08:41:24.676340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 08:41:24.724189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 08:41:24.822108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 08:41:25.780548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from torch import nn  # Import the neural network module from PyTorch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from classes._exploration import exploration\n",
    "from classes._Env3WGym import Env3WGym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            try:\n",
    "                class_code = int(class_path.stem)\n",
    "            except ValueError:\n",
    "                # Se não for possível converter para int, pule este diretório\n",
    "                continue\n",
    "\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instance_with_numpy(data_path, events_names, columns):\n",
    "    data_path = Path(data_path)\n",
    "  \n",
    "\n",
    "    well_names = ['WELL-00001','WELL-00002', 'WELL-00004', 'WELL-00005', 'WELL-00007']\n",
    "\n",
    "    real_instances = list(class_and_file_generator(data_path, real=True, simulated=False, drawn=False))\n",
    "\n",
    "    arrays_list = []  # List to store processed NumPy arrays\n",
    "\n",
    "    for instance in real_instances:\n",
    "        class_code, instance_path = instance    \n",
    "        well, _ = instance_path.stem.split('_')  \n",
    "             \n",
    "        if class_code in events_names and well in well_names:\n",
    "\n",
    "            # Read the entire CSV file into a NumPy array\n",
    "            with open(instance_path, 'r') as file:\n",
    "                header = file.readline().strip().split(',')\n",
    "                indices = [header.index(col) for col in columns]\n",
    "                arr = np.genfromtxt(file, delimiter=',', usecols=indices, dtype=np.float32)\n",
    "                                                \n",
    "                arr = arr[~np.isnan(arr[:, [0, 1, 2, 3, 4, 5]]).any(axis=1)]\n",
    "                arr[:, :-1] = arr[:, :-1].astype(np.float32)  # Convert selected columns to float32\n",
    "                arr[:, -1] = arr[:, -1].astype(np.int16)  # Convert 'class' column to int16\n",
    "\n",
    "                # Adds the processed NumPy array to the list\n",
    "                arrays_list.append(arr)\n",
    "\n",
    "    # Concatenate all processed NumPy arrays\n",
    "    final_array = np.concatenate(arrays_list)\n",
    "\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env3W_dqn(env):\n",
    "\n",
    "    # Instanciar o modelo DQN com a política MLP\n",
    "    model = DQN(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        learning_rate=1e-4,\n",
    "        buffer_size=10000,\n",
    "        learning_starts=10000,\n",
    "        batch_size=32,\n",
    "        tau=1.0,\n",
    "        gamma=0.99,\n",
    "        train_freq=4,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.01,\n",
    "        max_grad_norm=10,\n",
    "        tensorboard_log=None,\n",
    "        verbose=1,\n",
    "        device='auto'\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    model.learn(total_timesteps=int(1.2e5))\n",
    "\n",
    "    # Salvar o modelo\n",
    "    model.save('dqn_env3W')\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env3W_dqn_eval(model, env, n_eval_episodes=1):\n",
    "    # Inicializa listas para armazenar os dados de cada episódio\n",
    "    correct_predictions_list = []\n",
    "    total_predictions_list = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False]\n",
    "\n",
    "        # Reseta contadores para o episódio atual\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        while not all(done):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, rewards, done, info = env.step(action)\n",
    "\n",
    "            for reward in rewards:\n",
    "                if reward > 0:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "\n",
    "        # Atualiza a mensagem de log para incluir a recompensa e a porcentagem de acerto\n",
    "        if total_predictions > 0:  # Evita divisão por zero\n",
    "            percentage_correct = correct_predictions / total_predictions\n",
    "            logging.info(f\"[{episode}] Recompensa: {correct_predictions}, Porcentagem de acerto: {percentage_correct:.2f}.\")\n",
    "            print(f\"[{episode}] Recompensa: {correct_predictions}, Porcentagem de acerto: {percentage_correct:.2f}.\")\n",
    "        else:\n",
    "            logging.info(f\"[{episode}] Sem predições.\")\n",
    "            print(f\"[{episode}] Sem predições.\")\n",
    "\n",
    "        # Armazena os dados do episódio atual nas listas\n",
    "        correct_predictions_list.append(correct_predictions)\n",
    "        total_predictions_list.append(total_predictions)\n",
    "\n",
    "    # Calcula a precisão geral\n",
    "    accuracy = sum(correct_predictions_list) / sum(total_predictions_list) if sum(total_predictions_list) > 0 else 0\n",
    "\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras da classe 0: 6372575\n",
      "Número de amostras da classe 4: 1711754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "DQN Training Time: 16.11586880683899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_660/3080990056.py\", line 103, in <module>\n",
      "    logging.info(\"DQN Training Time:\", time.time() - start_time)\n",
      "Message: 'DQN Training Time:'\n",
      "Arguments: (16.11596417427063,)\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Recompensa: 7188322, Porcentagem de acerto: 0.89.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (7078, 7), indices imply (7078, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 140\u001b[0m\n\u001b[1;32m    136\u001b[0m expanded_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((dataset_test, array_action_pred))\n\u001b[1;32m    138\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFim do teste do modelo de teste DQN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m explora \u001b[38;5;241m=\u001b[39m exploration(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP-PDG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP-TPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT-TPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP-MON-CKP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT-JUS-CKP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred_action\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    141\u001b[0m explora\u001b[38;5;241m.\u001b[39mplot_sensor(sensor_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-PDG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-TPT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT-TPT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-MON-CKP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT-JUS-CKP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m], _title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlot DQN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:816\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    805\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    806\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    814\u001b[0m         )\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (7078, 7), indices imply (7078, 8)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_filename = f'/home/Reinforcement-Learning/logs/{current_time}_RL-log.txt'\n",
    "    # Configuração do Logging\n",
    "    logging.basicConfig(filename=log_filename, filemode='w', level=logging.INFO, format='[%(levelname)s]\\t%(asctime)s - %(message)s', datefmt='%d/%m/%Y %I:%M:%S %p', force=True)\n",
    "\n",
    "    path_dataset = '/home/dataset'\n",
    "    events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "    columns = [       \n",
    "        'P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        #'P-JUS-CKGL',\n",
    "        #'T-JUS-CKGL',\n",
    "        #'QGL',\n",
    "        'class'\n",
    "    ]\n",
    "    \n",
    "    logging.info(f'Iniciando carregamento do dataset')\n",
    "    dataset = load_instance_with_numpy(data_path = path_dataset, events_names = events_names, columns = columns)    \n",
    "    logging.info(f'Fim carregamento do dataset')\n",
    "    \n",
    "    logging.info(f'Iniciando divisão do dataset em treino e teste')\n",
    "    \n",
    "    # Filtrar índices que correspondem aos eventos de interesse\n",
    "    condition = np.isin(dataset[:, -1], list(events_names.keys()))\n",
    "    indices = np.where(condition)[0]\n",
    "\n",
    "    # Definindo a porcentagem para divisão entre treino e teste\n",
    "    train_percentage = 0.8  # 80% para treino\n",
    "\n",
    "    # Inicializando listas para guardar índices de treino e teste\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Processamento genérico para cada classe\n",
    "    for event in events_names.keys():\n",
    "        # Selecionando índices para a classe atual\n",
    "        class_indices = indices[dataset[indices, -1] == event]\n",
    "        \n",
    "        # Logando o número de amostras por classe\n",
    "        print(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        logging.info(f'Número de amostras da classe {event}: {len(class_indices)}')\n",
    "        \n",
    "        # Dividindo os índices da classe atual em treino e teste\n",
    "        class_train_indices, class_test_indices = train_test_split(class_indices, train_size=train_percentage, random_state=42)\n",
    "        \n",
    "        # Logando o número de amostras de treino e teste\n",
    "        logging.info(f'Número de amostras de treino da classe {event}: {len(class_train_indices)}')\n",
    "        logging.info(f'Número de amostras de teste da classe {event}: {len(class_test_indices)}')\n",
    "        \n",
    "        # Adicionando aos índices gerais de treino e teste\n",
    "        train_indices.extend(class_train_indices)\n",
    "        test_indices.extend(class_test_indices)\n",
    "\n",
    "    # Convertendo listas para arrays numpy para futura manipulação\n",
    "    train_indices = np.array(train_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "\n",
    "    # Embaralhando os índices (opcional, dependendo da necessidade)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    # Criando conjuntos de dados de treino e teste\n",
    "    dataset_train = dataset[train_indices]\n",
    "    dataset_test = dataset[test_indices]\n",
    "\n",
    "    # Dividindo em features (X) e target (y)\n",
    "    X_train, y_train = dataset_train[:, :-1], dataset_train[:, -1]\n",
    "    X_test, y_test = dataset_test[:, :-1], dataset_test[:, -1]\n",
    "\n",
    "    # Escalonando as features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logging.info(f'Fim divisão do dataset em treino e teste')\n",
    "\n",
    "    # Se necessário, você pode combinar as features escalonadas e o target para formar os datasets finais\n",
    "    dataset_train_scaled = np.column_stack((X_train_scaled, y_train))\n",
    "    dataset_test_scaled = np.column_stack((X_test_scaled, y_test))\n",
    "    \n",
    "    logging.info(f'Iniciando treinamento do modelo DQN')\n",
    "    start_time = time.time()\n",
    "    # Crie o ambiente (modifique conforme necessário para seu caso de uso)\n",
    "    env = make_vec_env(lambda: Env3WGym(dataset_train_scaled), n_envs=5)\n",
    "    env3W_dqn(env)  # Chamar a função para treinar o modelo\n",
    "    print(\"DQN Training Time:\", time.time() - start_time)\n",
    "    logging.info(\"DQN Training Time:\", time.time() - start_time)\n",
    "    logging.info(f'Fim treinamento do modelo DQN')\n",
    "\n",
    "    logging.info(f'Iniciando avaliação do modelo de teste DQN')\n",
    "    dqn_model = DQN.load('dqn_env3W')\n",
    "    env = make_vec_env(lambda: Env3WGym(dataset_test_scaled), n_envs=5)\n",
    "    # Executar a função de avaliação\n",
    "    accuracy = env3W_dqn_eval(dqn_model, env)\n",
    "    logging.info(f'Acurácia: {accuracy}')\n",
    "    logging.info(f'Fim avaliação do modelo de teste DQN')\n",
    "\n",
    "    logging.info(f'Iniciando o teste do modelo de teste DQN')\n",
    "    columns = [\n",
    "        #'timestamp',\n",
    "        'P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        #'P-JUS-CKGL',\n",
    "        #'T-JUS-CKGL',\n",
    "        #'QGL',\n",
    "        'class'\n",
    "        \n",
    "    ]\n",
    "    dataset_test = load_instance_with_numpy(data_path = '/home/dataset_test', events_names = events_names, columns = columns)\n",
    "    array_action_pred = []\n",
    "    for i in range(0, len(dataset_test)):\n",
    "        obs = dataset_test[i, :-1]\n",
    "        action, _states = dqn_model.predict(obs, deterministic=True)  \n",
    "        array_action_pred.append(action)\n",
    "\n",
    "    \n",
    "    expanded_array = np.column_stack((dataset_test, array_action_pred))\n",
    "\n",
    "    logging.info(f'Fim do teste do modelo de teste DQN')\n",
    "\n",
    "    explora = exploration(pd.DataFrame(expanded_array, columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class', 'timestamp', 'pred_action']))\n",
    "    explora.plot_sensor(sensor_columns = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class'], _title = 'Plot DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_instance_with_numpy(data_path, events_names, columns):\n",
    "    data_path = Path(data_path)\n",
    "    arrays_list = []  # List to store processed NumPy arrays\n",
    "\n",
    "    # Exemplo: Tratando diretamente o arquivo carregado\n",
    "    real_instances = list(class_and_file_generator(data_path, real=True, simulated=False, drawn=False))\n",
    "\n",
    "    well_names = ['WELL-00001','WELL-00002', 'WELL-00004', 'WELL-00005', 'WELL-00007']\n",
    "\n",
    "    for instance in real_instances:\n",
    "\n",
    "        class_code, instance_path = instance    \n",
    "        well, _ = instance_path.stem.split('_')  \n",
    "        if class_code in events_names and well in well_names: \n",
    "            with open(instance_path, 'r') as file:\n",
    "                header = file.readline().strip().split(',')\n",
    "                indices = [header.index(col) for col in columns if col in header and col != 'timestamp']\n",
    "                timestamp_idx = header.index('timestamp')\n",
    "\n",
    "                numeric_data = np.genfromtxt(file, delimiter=',', skip_header=0, usecols=indices, dtype=np.float32)\n",
    "                if numeric_data.ndim == 1:\n",
    "                    numeric_data = np.expand_dims(numeric_data, axis=0)\n",
    "                numeric_data = numeric_data[~np.isnan(numeric_data).any(axis=1)]\n",
    "\n",
    "                file.seek(0)\n",
    "                file.readline()  # Pula o cabeçalho\n",
    "                timestamps = np.genfromtxt(file, delimiter=',', skip_header=0, usecols=timestamp_idx, dtype=str)\n",
    "                if isinstance(timestamps, str):                \n",
    "                    timestamps = np.array([timestamps])\n",
    "\n",
    "            # Processando e arredondando os timestamps\n",
    "            fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "            rounded_timestamps = [datetime.strptime(ts, fmt).replace(second=0, microsecond=0).strftime(\"%Y-%m-%d %H:%M:%S\") for ts in timestamps]\n",
    "            \n",
    "            # Combinação dos timestamps arredondados e dados numéricos\n",
    "            final_data = np.hstack([np.array(rounded_timestamps).reshape(-1, 1), numeric_data])\n",
    "            arrays_list.append(final_data)\n",
    "\n",
    "    \n",
    "    # Combinação dos timestamps e dados numéricos para DataFrame\n",
    "    final_array = np.vstack(arrays_list) if arrays_list else np.array([])\n",
    "\n",
    "    return final_array\n",
    "\n",
    "# Definindo as colunas\n",
    "columns = [\n",
    "    'timestamp',\n",
    "    'P-PDG',\n",
    "    'P-TPT',\n",
    "    'T-TPT',\n",
    "    'P-MON-CKP',\n",
    "    'T-JUS-CKP',\n",
    "    'class'\n",
    "]\n",
    "\n",
    "events_names = {\n",
    "        0: 'Normal',\n",
    "        # 1: 'Abrupt Increase of BSW',\n",
    "        # 2: 'Spurious Closure of DHSV',\n",
    "        # 3: 'Severe Slugging',\n",
    "        4: 'Flow Instability',\n",
    "        # 5: 'Rapid Productivity Loss',\n",
    "        # 6: 'Quick Restriction in PCK',\n",
    "        # 7: 'Scaling in PCK',\n",
    "        # 8: 'Hydrate in Production Line'\n",
    "    }\n",
    "\n",
    "# Carregando o dataset\n",
    "dataset_test = load_instance_with_numpy(data_path = '/home/dataset_test', events_names = events_names, columns = columns)\n",
    "arr_modificado = np.delete(dataset_test, 0, axis=1)\n",
    "arr_modificado\n",
    "# Criando o DataFrame\n",
    "#df = pd.DataFrame(dataset_test, columns=columns)\n",
    "#df.head()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
