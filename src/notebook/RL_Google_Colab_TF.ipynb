{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaikecc/Reinforcement-Learning/blob/main/src/notebook/RL_Google_Colab_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBFlNsimwUN5"
      },
      "source": [
        "# 1. Importação e Configuração do 3W Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bE09MzpdwUN6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "from pathlib import Path\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from collections import defaultdict\n",
        "from natsort import natsorted\n",
        "from pathlib import Path\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIqiWDgnz9oW",
        "outputId": "554f75eb-ff80-40a1-cb1f-00d84b1e0c9e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(filename='RL_log.txt',\n",
        "                    filemode='w',\n",
        "                    format='[%(levelname)s]\\t%(asctime)s - %(message)s',\n",
        "                    datefmt='%d/%m/%Y %I:%M:%S %p',\n",
        "                    level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L-x2zez_Ns2"
      },
      "source": [
        "# Teste GPU TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwCMGQIv79PM",
        "outputId": "1d6e3c24-2e6b-438b-fbef-628a573c4110"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTkPtwElH7wa",
        "outputId": "47add1af-7065-4ad3-da23-da265cea02ae"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQb8SQWdL67C",
        "outputId": "b79584ea-89f1-4a3e-f67a-b570faee897a"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjsIiU3E8V_G",
        "outputId": "4219f251-6d4f-49fe-f0e4-77cd0846e08e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fcq63mnrwUN8"
      },
      "outputs": [],
      "source": [
        "data_path = Path('C:\\\\Users\\\\kaike\\Documents\\\\UFSC\\\\3W\\\\dataset')\n",
        "#data_path = Path('/home/dataset')\n",
        "\n",
        "events_names = {0: 'Normal',\n",
        "                1: 'Abrupt Increase of BSW',\n",
        "                2: 'Spurious Closure of DHSV',\n",
        "                3: 'Severe Slugging',\n",
        "                4: 'Flow Instability',\n",
        "                5: 'Rapid Productivity Loss',\n",
        "                6: 'Quick Restriction in PCK',\n",
        "                7: 'Scaling in PCK',\n",
        "                8: 'Hydrate in Production Line'\n",
        "               }\n",
        "columns = ['P-PDG',\n",
        "           'P-TPT',\n",
        "           'T-TPT',\n",
        "           'P-MON-CKP',\n",
        "           'T-JUS-CKP',\n",
        "           #'P-JUS-CKGL',\n",
        "           #'T-JUS-CKGL',\n",
        "           #'QGL',\n",
        "           'class']\n",
        "rare_threshold = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fm9HObEYwUN8"
      },
      "outputs": [],
      "source": [
        "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
        "    for class_path in data_path.iterdir():\n",
        "        if class_path.is_dir():\n",
        "            try:\n",
        "                class_code = int(class_path.stem)\n",
        "            except ValueError:\n",
        "                # Se não for possível converter para int, pule este diretório\n",
        "                continue\n",
        "\n",
        "            for instance_path in class_path.iterdir():\n",
        "                if (instance_path.suffix == '.csv'):\n",
        "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
        "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
        "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
        "                       (not instance_path.stem.startswith('DRAWN'))):\n",
        "                        yield class_code, instance_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zlji4u1_wUN-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of real instances: 1019\n",
            "Number of simulated instances: 939\n",
            "Number of drawn instances: 20\n"
          ]
        }
      ],
      "source": [
        "real_instances = list(class_and_file_generator(data_path, real=True, simulated=False, drawn=False))\n",
        "simulated_instances = list(class_and_file_generator(data_path, real=False, simulated=True, drawn=False))\n",
        "drawn_instances = list(class_and_file_generator(data_path, real=False, simulated=False, drawn=True))\n",
        "\n",
        "print(f'Number of real instances: {len(real_instances)}')\n",
        "print(f'Number of simulated instances: {len(simulated_instances)}')\n",
        "print(f'Number of drawn instances: {len(drawn_instances)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "--yMNWGHwUN_",
        "outputId": "146961ac-e8f5-42cf-c685-4f8bc4fe3cc3"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "error reading file C:\\Users\\kaike\\Documents\\UFSC\\3W\\dataset\\0\\WELL-00001_20170201020207.csv: ('Shapes must match', (9,), (6,))",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mload_instance\u001b[1;34m(instances)\u001b[0m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(instance_path, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid columns in the file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(instance_path), \u001b[38;5;28mstr\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m class_code\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7129\u001b[0m, in \u001b[0;36mIndex._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCMultiIndex):\n\u001b[0;32m   7128\u001b[0m     \u001b[38;5;66;03m# don't pass MultiIndex\u001b[39;00m\n\u001b[1;32m-> 7129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:128\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes must match\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    129\u001b[0m result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n",
            "\u001b[1;31mValueError\u001b[0m: ('Shapes must match', (9,), (6,))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror reading file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(instance_path, e))\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_instances\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
            "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mload_instance\u001b[1;34m(instances)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror reading file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(instance_path, e))\n",
            "\u001b[1;31mException\u001b[0m: error reading file C:\\Users\\kaike\\Documents\\UFSC\\3W\\dataset\\0\\WELL-00001_20170201020207.csv: ('Shapes must match', (9,), (6,))"
          ]
        }
      ],
      "source": [
        "def load_instance(instances):\n",
        "    class_code, instance_path = instances\n",
        "    try:\n",
        "        well, instance_id = instance_path.stem.split('_')\n",
        "        df = pd.read_csv(instance_path, index_col='timestamp', parse_dates=['timestamp'])\n",
        "        assert (df.columns == columns).all(), \"invalid columns in the file {}: {}\".format(str(instance_path), str(df.columns.tolist()))\n",
        "        df['label'] = class_code\n",
        "        df['well'] = well\n",
        "        df['id'] = instance_id\n",
        "        df = df[['label', 'well', 'id'] + columns]\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        raise Exception('error reading file {}: {}'.format(instance_path, e))\n",
        "\n",
        "\n",
        "df = load_instance(real_instances[0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "wH6I_KHwwUN_",
        "outputId": "b3cdc2c9-0d0c-49dc-8d8b-54ce58d919c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Supondo que real_instances, simulated_instances, drawn_instances e events_names já estejam definidos\n",
        "instances_class = [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'REAL'} for c, p in real_instances] + \\\n",
        "                  [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'SIMULATED'} for c, p in simulated_instances] + \\\n",
        "                  [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'DRAWN'} for c, p in drawn_instances]\n",
        "df_class = pd.DataFrame(instances_class)\n",
        "\n",
        "# Correção aqui: pivot(index, columns, values) com argumentos nomeados\n",
        "df_class_count = df_class.groupby(['TYPE OF EVENT', 'SOURCE']).size().reset_index(name='count')\n",
        "df_class_count = df_class_count.pivot(index='TYPE OF EVENT', columns='SOURCE', values='count').fillna(0).astype(int)\n",
        "\n",
        "# Transposição não é necessária após a correção, mas ajustaremos a orientação conforme sua intenção original\n",
        "#df_class_count = df_class_count.T\n",
        "\n",
        "# Ordenação natural dos índices, se necessário\n",
        "df_class_count = df_class_count.loc[natsorted(df_class_count.index)]\n",
        "\n",
        "# Reordenar colunas se 'REAL', 'SIMULATED', 'DRAWN' estiverem presentes, adicionando a coluna 'TOTAL'\n",
        "df_class_count['TOTAL'] = df_class_count.sum(axis=1)\n",
        "df_class_count.loc['TOTAL'] = df_class_count.sum(axis=0)\n",
        "\n",
        "# Exibindo o DataFrame final\n",
        "df_class_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55UkwUSmwUOA"
      },
      "source": [
        "# Criação de um único dataframe com todos os poços reais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6btO7FZ2wUOA",
        "outputId": "697fd538-6b99-490a-a9b2-3c82757c7f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14516197 entries, 0 to 14516196\n",
            "Data columns (total 12 columns):\n",
            " #   Column      Dtype  \n",
            "---  ------      -----  \n",
            " 0   label       int64  \n",
            " 1   well        object \n",
            " 2   id          object \n",
            " 3   P-PDG       float64\n",
            " 4   P-TPT       float64\n",
            " 5   T-TPT       float64\n",
            " 6   P-MON-CKP   float64\n",
            " 7   T-JUS-CKP   float64\n",
            " 8   P-JUS-CKGL  float64\n",
            " 9   T-JUS-CKGL  float64\n",
            " 10  QGL         float64\n",
            " 11  class       float64\n",
            "dtypes: float64(9), int64(1), object(2)\n",
            "memory usage: 1.3+ GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Carregar todos os DataFrames dos arquivos de instâncias reais\n",
        "df_all_instances_real = [pd.DataFrame(load_instance(instance)) for instance in real_instances]\n",
        "\n",
        "# Concatenar todos os DataFrames na lista para formar um único DataFrame\n",
        "df = pd.concat(df_all_instances_real, ignore_index=True)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('df_raw.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqshw7WrwUOA"
      },
      "source": [
        "# Criação de um dataframe para o desenvolvimento de um Ambiente Python (TF)\n",
        "\n",
        "Critério de seleção dos poços\n",
        "\n",
        "1. Com mais de três tipos de eventos: 'WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006'\n",
        "2. Dois poços para equilibrar a falta de eventos: 'WELL-00015', 'WELL-00016'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q5JPWAswUOA",
        "outputId": "8e63e0d5-5e35-4229-ef6e-ac94aa0bc28c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 9645620 entries, 0 to 14484284\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Dtype  \n",
            "---  ------     -----  \n",
            " 0   label      int64  \n",
            " 1   P-PDG      float64\n",
            " 2   P-TPT      float64\n",
            " 3   T-TPT      float64\n",
            " 4   P-MON-CKP  float64\n",
            " 5   T-JUS-CKP  float64\n",
            " 6   class      float64\n",
            "dtypes: float64(6), int64(1)\n",
            "memory usage: 588.7 MB\n",
            "label        0\n",
            "P-PDG        0\n",
            "P-TPT        0\n",
            "T-TPT        0\n",
            "P-MON-CKP    0\n",
            "T-JUS-CKP    0\n",
            "class        0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Lista de poços para treinamento do modelo\n",
        "wells_to_include = ['WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006', 'WELL-00015', 'WELL-00016']\n",
        "\n",
        "# Selecionando colunas específicas\n",
        "columns_to_select = ['label', 'P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class']\n",
        "\n",
        "\n",
        "# Filtrando o DataFrame para incluir apenas os poços desejados e colunas específicas, e removendo linhas com NaNs\n",
        "df_env = df[df['well'].isin(wells_to_include)][columns_to_select].dropna()\n",
        "\n",
        "# Mostrando informações do DataFrame filtrado\n",
        "df_env.info()\n",
        "\n",
        "print(df_env.isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTA2HFRnwUOB"
      },
      "source": [
        "# Pré-processamento dos dados do dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c1TwtL1YwUOB",
        "outputId": "00c942b8-8a04-4650-8b8a-9ec39bf581c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>P-PDG</th>\n",
              "      <th>P-TPT</th>\n",
              "      <th>T-TPT</th>\n",
              "      <th>P-MON-CKP</th>\n",
              "      <th>T-JUS-CKP</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>0.195413</td>\n",
              "      <td>-0.063940</td>\n",
              "      <td>0.178590</td>\n",
              "      <td>0.367639</td>\n",
              "      <td>3.059375</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>0.195413</td>\n",
              "      <td>-0.063941</td>\n",
              "      <td>0.178590</td>\n",
              "      <td>0.370626</td>\n",
              "      <td>3.059375</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.195413</td>\n",
              "      <td>-0.063943</td>\n",
              "      <td>0.178576</td>\n",
              "      <td>0.373613</td>\n",
              "      <td>3.059375</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.195413</td>\n",
              "      <td>-0.063944</td>\n",
              "      <td>0.178562</td>\n",
              "      <td>0.376600</td>\n",
              "      <td>3.059375</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.195413</td>\n",
              "      <td>-0.063946</td>\n",
              "      <td>0.178547</td>\n",
              "      <td>0.379587</td>\n",
              "      <td>3.059375</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label     P-PDG     P-TPT     T-TPT  P-MON-CKP  T-JUS-CKP  class\n",
              "0      4  0.195413 -0.063940  0.178590   0.367639   3.059375      4\n",
              "1      4  0.195413 -0.063941  0.178590   0.370626   3.059375      4\n",
              "2      4  0.195413 -0.063943  0.178576   0.373613   3.059375      4\n",
              "3      4  0.195413 -0.063944  0.178562   0.376600   3.059375      4\n",
              "4      4  0.195413 -0.063946  0.178547   0.379587   3.059375      4"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Colunas para normalizar, exceto 'class'\n",
        "columns_to_normalize = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
        "\n",
        "# Aplicando Z-score Standardization\n",
        "df_env[columns_to_normalize] = (df_env[columns_to_normalize] - df_env[columns_to_normalize].mean()) / df_env[columns_to_normalize].std()\n",
        "\n",
        "# Converte a coluna 'class' para valor int\n",
        "df_env['class'] = df_env['class'].astype(int)\n",
        "\n",
        "\n",
        "# Verifique as primeiras linhas para confirmar a normalização\n",
        "df_env.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 9645620 entries, 0 to 14484284\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Dtype  \n",
            "---  ------     -----  \n",
            " 0   label      int64  \n",
            " 1   P-PDG      float64\n",
            " 2   P-TPT      float64\n",
            " 3   T-TPT      float64\n",
            " 4   P-MON-CKP  float64\n",
            " 5   T-JUS-CKP  float64\n",
            " 6   class      int64  \n",
            "dtypes: float64(5), int64(2)\n",
            "memory usage: 588.7 MB\n"
          ]
        }
      ],
      "source": [
        "df_env.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_estados(df_env):\n",
        "    # Contagem de valores para cada classe\n",
        "    class_counts = df_env['class'].value_counts().sort_index()\n",
        "    # soma os valores de 1 a 9 e 101 a 109\n",
        "    rare_class_counts_A = class_counts[(class_counts.index > 0) & (class_counts.index < 10)].sum()\n",
        "    rare_class_counts_B = class_counts[(class_counts.index > 10)].sum()\n",
        "    rare_class_counts_C = class_counts[(class_counts.index == 0)].sum()\n",
        "\n",
        "    # Total de amostras\n",
        "    total = rare_class_counts_A + rare_class_counts_B + rare_class_counts_C\n",
        "\n",
        "    print(f'Normal: {rare_class_counts_C} - {round(rare_class_counts_C/total*100, 2)}%')\n",
        "    print(f'Transiente de anomalia: {rare_class_counts_B} - {round(rare_class_counts_B/total*100, 2)}%')\n",
        "    print(f'Estável de anomalia: {rare_class_counts_A} - {round(rare_class_counts_A/total*100, 2)}%')\n",
        "    print(f'Total: {rare_class_counts_A + rare_class_counts_B + rare_class_counts_C}')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar('Normal', rare_class_counts_C, label='Normal')\n",
        "    ax.bar('Estável de anomalia', rare_class_counts_A, label='Estável de anomalia')\n",
        "    ax.bar('Transiente de anomalia', rare_class_counts_B, label='Transiente de anomalia')\n",
        "\n",
        "    ax.set_ylabel('Quantidade')\n",
        "    ax.set_title('Quantidade de amostras por classe')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_env.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "plot_estados(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('df_raw.csv')\n",
        "    \n",
        "# Lista de poços para treinamento do modelo\n",
        "wells_to_include = ['WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006', 'WELL-00015', 'WELL-00016']\n",
        "\n",
        "# Selecionando colunas específicas\n",
        "columns_to_select = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class']\n",
        "\n",
        "# Colunas para normalizar, exceto 'class'\n",
        "columns_to_normalize = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
        "\n",
        "df = df[(df['label'] == 0) | (df['label'] == 1)]\n",
        "\n",
        "# Filtrando o DataFrame para incluir apenas os poços desejados e colunas específicas, e removendo linhas com NaNs\n",
        "df_env = df[df['well'].isin(wells_to_include)][columns_to_select].dropna()\n",
        "\n",
        "# Aplicando Z-score Standardization\n",
        "df_env[columns_to_normalize] = (df_env[columns_to_normalize] - df_env[columns_to_normalize].mean()) / df_env[columns_to_normalize].std()\n",
        "\n",
        "# Converte a coluna 'class' para valor int\n",
        "df_env['class'] = df_env['class'].astype(int)\n",
        "\n",
        "\n",
        "df_test = df_env.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "plot_estados(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUQ2Bj9o2PgJ"
      },
      "source": [
        "# Classe 3W - Ambiente TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65g1xnRD2bf1"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-agents\n",
        "#!pip install dm-reverb[tensorflow]\n",
        "#!pip install dm-reverb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxhSSexx2Nzi"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "class env3W(py_environment.PyEnvironment):\n",
        "    '''\n",
        "    Essa classe é um Modelo-Livre do Ambiente em python para o problema de detecção de falhas em poços de petróleo.\n",
        "    O dataframe é fornecido como entrada para o ambiente com mais de 10 milhões de registros.\n",
        "\n",
        "     1. O ambiente é um repositório di github https://github.com/petrobras/3W\n",
        "     2. O ambiente é composto por dados de seis poços de petróleo, com cinco variáveis (observações) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rótulo indentificador de falha [class]\n",
        "     3. O ambiente é um ambiente de simulação, onde o agente pode escolher entre duas ações: 0 - Não Detectado ou 1 - Detectado para cada observação\n",
        "     4. A recompensa é calculada com base na ação escolhida e no rótulo de falha [class]:\n",
        "\n",
        "        Estados:\n",
        "        - rótulo de falha: 0 - Estado Normal\n",
        "        - rótulo de falha: 1 a 8 - Estável de Anomalia (Falha)\n",
        "        - rótulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
        "\n",
        "        Ações/Recompensas:\n",
        "        - Se o rótulo de falha for 0, a recompensa é 0,01 se a ação for 0, caso contrário, a recompensa é -1\n",
        "        - Se o rótulo de falha estiver entre 1 e 8, a recompensa é -1 se a ação for 0, caso contrário, a recompensa é 1\n",
        "        - Se o rótulo de falha estiver entre 101 e 108, a recompensa é -0,1 se a ação for 0, caso contrário, a recompensa é 0,1\n",
        "\n",
        "    '''\n",
        "    def __init__(self, dataframe):\n",
        "        super(env3W, self).__init__()\n",
        "        self._dataframe = dataframe\n",
        "        self._index = 0\n",
        "        # Ação: 0 - Não Deectado ou 1 - Detectado\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "        self.columns_needed = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
        "        num_features = len(self.columns_needed)\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(num_features,), dtype=np.float32, name='observation')\n",
        "        row = self._dataframe.iloc[self._index][self.columns_needed]\n",
        "        self._state = row.values\n",
        "        self._episode_ended = False\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "\n",
        "    def _update_state(self):\n",
        "        # Avaliar o index se é +1 ou não\n",
        "        row = self._dataframe.iloc[self._index][self.columns_needed]\n",
        "        self._state = row.values\n",
        "\n",
        "\n",
        "    def _reset(self):\n",
        "        self._index = 0\n",
        "        self._update_state()\n",
        "        self._episode_ended = False\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "\n",
        "    def _step(self, action):\n",
        "\n",
        "        if self._episode_ended:\n",
        "          return self._reset()\n",
        "\n",
        "        # Verifica se a ação é válida\n",
        "        if not 0 <= action <= 1:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=-1)\n",
        "        \n",
        "        reward = self._calculate_reward(action)\n",
        "\n",
        "        # Quando chegar na última linha, marque o episódio como terminado e pare.\n",
        "        if self._episode_ended or self._index >= len(self._dataframe) - 1:\n",
        "\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "        else:\n",
        "            self._index += 1\n",
        "            self._update_state()\n",
        "            return ts.transition(np.array(self._state, dtype=np.float32), reward=reward, discount=1.0)\n",
        "\n",
        "\n",
        "    def _calculate_reward(self, action):\n",
        "        class_value = self._dataframe.iloc[self._index]['class']\n",
        "        if class_value == 0:\n",
        "            return 0.01 if action == 0 else -1\n",
        "        elif class_value in range(1, 9):\n",
        "            return -1 if action == 0 else 1\n",
        "        elif class_value in range(101, 109):\n",
        "            return -0.1 if action == 0 else 0.1\n",
        "        else:\n",
        "            # Define uma recompensa padrão para qualquer outro valor de class não especificado\n",
        "            return 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38SzGfpZwUOB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BotUqckwUOB"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#import os\n",
        "#sys.path.append(os.path.join('..'))\n",
        "#from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
        "\n",
        "import reverb\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import random_tf_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGKJZD-PwUOC"
      },
      "source": [
        "# Hiperparâmentros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDHT9J9bwUOC"
      },
      "outputs": [],
      "source": [
        "initial_collect_steps = 1000  # Número de passos de coleta inicial\n",
        "collect_steps_per_iteration = 1  # Passos de coleta por iteração de treinamento\n",
        "replay_buffer_max_length = 100000  # Tamanho máximo do replay buffer\n",
        "\n",
        "log_interval = 1000  # Log do progresso a cada 1000 iterações de treinamento\n",
        "eval_interval = 5000  # Avaliação do desempenho do agente a cada 5000 iterações\n",
        "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
        "num_iterations = 10000  # Exemplo de número de iterações de treinamento\n",
        "learning_rate= 1e-3 # Taxa de aprendizado\n",
        "\n",
        "num_eval_episodes = 20  # @param {type:\"integer\"}\n",
        "num_parallel_envs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lePzndNbwUOC"
      },
      "source": [
        "# Ambiente\n",
        "\n",
        "No Aprendizado por Reforço (RL), um ambiente representa a tarefa ou problema a ser resolvido. Ambientes padrão podem ser criados em TF-Agents usando conjuntos tf_agents.environments.\n",
        "\n",
        "1. O ambiente é um repositório di github https://github.com/petrobras/3W\n",
        "2. O ambiente é composto por dados de seis poços de petróleo, com cinco variáveis (observações) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rótulo indentificador de falha [class]\n",
        "3. O ambiente é um ambiente de simulação, onde o agente pode escolher entre duas ações: 0 - Não Detectado ou 1 - Detectado para cada observação\n",
        "4. A recompensa é calculada com base na ação escolhida e no rótulo de falha [class]:\n",
        "\n",
        "Estados:\n",
        "- rótulo de falha: 0 - Estado Normal\n",
        "- rótulo de falha: 1 a 8 - Estável de Anomalia (Falha)\n",
        "- rótulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
        "\n",
        "Ações/Recompensas:\n",
        "- Se o rótulo de falha for 0, a recompensa é 1 se a ação for 0, caso contrário, a recompensa é -100\n",
        "- Se o rótulo de falha estiver entre 1 e 8, a recompensa é -100 se a ação for 0, caso contrário, a recompensa é 100\n",
        "- Se o rótulo de falha estiver entre 101 e 108, a recompensa é -10 se a ação for 0, caso contrário, a recompensa é 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.system import multiprocessing as tf_multiprocessing\n",
        "\n",
        "# Chame isto apenas uma vez no início do notebook\n",
        "tf_multiprocessing.enable_interactive_mode()\n",
        "\n",
        "# desabilitar o enable_interactive_mode\n",
        "\n",
        "#tf_multiprocessing.disable_interactive_mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7SsPggpwUOC"
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import tf_py_environment, ParallelPyEnvironment\n",
        "\n",
        "# Amostra aleatória de 1% dos dados\n",
        "df_test = df_env.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "# Criação de um ambiente de teste\n",
        "env = env3W(df_test)\n",
        "\n",
        "# Criação de num_parallel_envs ambientes de teste\n",
        "parallel_env = ParallelPyEnvironment([lambda: env3W(df_test) for _ in range(num_parallel_envs)])\n",
        "tf_env = tf_py_environment.TFPyEnvironment(parallel_env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "J-vWKZMEzqLA",
        "outputId": "1cfaa2e2-8fb6-4615-8be6-9d35a4b87934"
      },
      "outputs": [],
      "source": [
        "plot_estados(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoF8D_9AwUOC"
      },
      "source": [
        "Para validar o ambiente, é usado uma política aleatória para gerar ações e faremos a iteração em mais de 5 episódios para garantir que as coisas estejam funcionando como pretendido. Um erro é gerado se recebermos um time_step que não segue as especificações do ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY3lbajTwUOC"
      },
      "outputs": [],
      "source": [
        "# depois de 1h30 - cancelando verificação, no mo meu pc 54min com 10%, 36Mmin paralelo\n",
        "utils.validate_py_environment(env, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.validate_py_environment(parallel_env, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reward_list = []\n",
        "\n",
        "# Cria um step_list de 0 a 99 ao passo de 1\n",
        "step_list = np.arange(0, len(df_test), 1)\n",
        "\n",
        "# Para garantir que o ambiente esteja no estado inicial antes de começar a simulação\n",
        "time_step = env.reset()\n",
        "cumulative_reward = 0\n",
        "index = 0   \n",
        "for step in step_list:\n",
        "    # Supondo que 'detect' seja uma ação válida para o ambiente 'env'\n",
        "    if df_test['class'][index] == 0:\n",
        "        detect = np.array(0, dtype=np.int32)\n",
        "    elif df_test['class'][index] in range(1, 9):\n",
        "        detect = np.array(1, dtype=np.int32)\n",
        "    elif df_test['class'][index] in range(101, 109):\n",
        "        detect = np.array(1, dtype=np.int32)\n",
        "\n",
        "    index += 1\n",
        "    #detect = np.array(np.random.choice([0, 1]), dtype=np.int32)\n",
        "    time_step = env.step(detect)\n",
        "    cumulative_reward += time_step.reward\n",
        "    reward_list.append(cumulative_reward)\n",
        "\n",
        "# Plotar o gráfico step_list x reward_list\n",
        "plt.plot(step_list, reward_list)\n",
        "plt.xlabel('Passos')\n",
        "plt.ylabel('Recompensa Cumulativa')\n",
        "plt.title('Política Ótima : Passos vs Recompensa')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG3pf3QnwUOC",
        "outputId": "6edfa652-9d34-473a-de2e-124f11379eee"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "print('Current Time Step for Parallel Env:')\n",
        "parallel_env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5-XbFJawUOC",
        "outputId": "ccf791ab-56e8-49f8-fdd0-321313cb8b1d"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "#print(env.time_step_spec().observation)\n",
        "print(parallel_env.time_step_spec().observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWaL4ak5wUOC",
        "outputId": "19ec322b-9b78-44cd-8574-1efc26790ec5"
      },
      "outputs": [],
      "source": [
        "print('Reward Spec:')\n",
        "#print(env.time_step_spec().reward)\n",
        "print(parallel_env.time_step_spec().reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMogQ3o2wUOD",
        "outputId": "f49574fe-113a-4b8b-e405-3876c58db9f6"
      },
      "outputs": [],
      "source": [
        "print('Action Spec:')\n",
        "#print(env.action_spec())\n",
        "print(parallel_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY-H67iFwUOD"
      },
      "source": [
        "No ambiente env3W:\n",
        "\n",
        "* Observação é um array de cinco floats:\n",
        "\n",
        "    * Três sensores de pressão: 'P-PDG', 'P-TPT', 'P-MON-CKP'\n",
        "    * Dois sensores de temperatura: 'T-TPT', T-JUS-CKP'\n",
        "\n",
        "* A recompensa é um escalar inteiro.\n",
        "\n",
        "* A ação é um escalar inteiro com duas possibilidades:\n",
        "\n",
        "    * 0 — \"Não Detectado\"\n",
        "    * 1 — \"Detectado\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC34nmWEwUOD",
        "outputId": "cf69cd25-a71d-42a0-c445-ea54abd2b6fc"
      },
      "outputs": [],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFI-2T6iwUOD"
      },
      "source": [
        "Normalmente dois ambientes são instanciados: um para treinamento e outro para avaliação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_py_env = parallel_env\n",
        "eval_py_env = parallel_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5RP2etTwUOD"
      },
      "outputs": [],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)  # Avaliação pode ser em um env similar ou diferente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSIdpwmxwUOD"
      },
      "source": [
        "# Agente\n",
        "\n",
        "O algoritmo utilizado para resolver um problema de RL é representado por um Agente. TF-Agents fornece implementações padrão de uma variedade de Agentes, incluindo:\n",
        "\n",
        "* DQN (Usado para teste)\n",
        "* REINFORCE\n",
        "* DDPG\n",
        "* TD3\n",
        "* PPO\n",
        "* SAC\n",
        "\n",
        "O agente DQN pode ser utilizado em qualquer ambiente que possua um espaço de ação discreto.\n",
        "\n",
        "No coração de um Agente DQN está uma QNetwork, um modelo de rede neural que pode aprender a prever QValues (retornos esperados) para todas as ações, dada uma observação do ambiente.\n",
        "\n",
        "Usaremos tf_agents.networks. para criar uma QNetwork. A rede será composta por uma sequência de camadas tf.keras.layers.Dense, onde a camada final terá 1 saída para cada ação possível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXzk8CC-wUOD"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec()) # parallel_env\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Defina uma função auxiliar para criar camadas densas configuradas com o direito\n",
        "# ativação e inicializador do kernel.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "\n",
        "# QNetwork consiste em uma sequência de camadas densas seguidas por uma camada densa\n",
        "# com unidades `num_actions` para gerar um q_value por ação disponível como sua saída.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PKBr48wwUOD"
      },
      "source": [
        "Agora use tf_agents.agents.dqn.dqn_agent para instanciar um DqnAgent. Além de time_step_spec, action_spec e QNetwork, o construtor do agente também requer um otimizador (neste caso, AdamOptimizer), uma função de perda e um contador de passos inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPMv_SxEwUOD"
      },
      "outputs": [],
      "source": [
        "\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAYXp6d-wUOE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#train_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NO7kq2LwUOE"
      },
      "source": [
        "# Política\n",
        "\n",
        "Uma política define a forma como um agente atua em um ambiente. Normalmente, o objetivo da aprendizagem por reforço é treinar o modelo subjacente até que a política produza o resultado desejado.\n",
        "\n",
        "* O resultado desejado é a identificar uma falha com base nas observações\n",
        "* A política retorna uma ação (Não Detectado ou Detectado) para cada observação time_step.\n",
        "\n",
        "Os agentes contêm duas políticas:\n",
        "\n",
        "* agent.policy — A política principal usada para avaliação e implantação.\n",
        "* agent.collect_policy — Uma segunda política usada para coleta de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPjXJ4ZjwUOF"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "print(f'Policy evaluation: {eval_policy}')\n",
        "collect_policy = agent.collect_policy\n",
        "print(f'Policy collection: {collect_policy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmOeS48OwUOF"
      },
      "source": [
        "As políticas podem ser criadas independentemente dos agentes. Por exemplo, use tf_agents.policies.random_tf_policy para criar uma política que selecionará aleatoriamente uma ação para cada time_step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERGG2fNowUOF"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IYQt_z6wUOF"
      },
      "source": [
        "Para obter uma ação de uma política, chame o método policy.action(time_step). O time_step contém a observação do ambiente. Este método retorna um PolicyStep, que é uma tupla nomeada com três componentes:\n",
        "\n",
        "* ação - a ação a ser executada (neste caso, 0 ou 1)\n",
        "* estado – usado para políticas com estado (isto é, baseadas em ANN)\n",
        "* info — dados auxiliares, como log de probabilidades de ações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(env3W(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_step = example_environment.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2-PWtgpwUOF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_step = tf_env.reset()\n",
        "\n",
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_step.is_last()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = np.array(0, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_step.is_first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = np.array(0, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "next_time_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZgPIYxQwUOG"
      },
      "source": [
        "# Métricas e Avaliação\n",
        "\n",
        "A métrica mais comum usada para avaliar uma política é o retorno médio. O retorno é a soma das recompensas obtidas durante a execução de uma política em um ambiente durante um episódio. Vários episódios são executados, criando um retorno médio.\n",
        "\n",
        "A função a seguir calcula o retorno médio de uma política, dada a política, o ambiente e um número de episódios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEPGu2AYwUOG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for i in range(num_episodes):\n",
        "\n",
        "    logging.info(f'Episode: {i}')\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      logging.info(f'Action: {action_step.action}')\n",
        "      time_step = environment.step(action_step.action)\n",
        "      logging.info(f'Time Step: {time_step}')\n",
        "      logging.info(f'Reward: {time_step.reward}')\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  logging.info(f'Average Return: {avg_return}'.format(_))\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
        "\n",
        "eval_env_one = tf_py_environment.TFPyEnvironment(env3W(df_test))\n",
        "compute_avg_return(eval_env_one, random_policy, num_episodes = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def parallel_compute_avg_return_test(environment, policy, num_parallel_envs, num_eval_episodes):\n",
        "    total_return = 0.0\n",
        "    num_episodes_per_env = np.ceil(num_eval_episodes / num_parallel_envs).astype(int)\n",
        "    \n",
        "    time_step = environment.reset()\n",
        "    episode_return = tf.zeros(num_parallel_envs)\n",
        "    episode_counts = np.zeros(num_parallel_envs, dtype=int)\n",
        "    total_episodes = 0\n",
        "\n",
        "    logging.info(f'Número de Episódios por Ambiente: {num_episodes_per_env}')\n",
        "    while total_episodes < num_eval_episodes:\n",
        "        policy_step = policy.action(time_step)\n",
        "        logging.info(f'policy_step: {policy_step.action}')\n",
        "        time_step = environment.step(policy_step.action)\n",
        "        logging.info(f'reward: {time_step.reward}')\n",
        "        episode_return += time_step.reward\n",
        "\n",
        "        #logging.info(f'episode_return: {episode_return}')\n",
        "        for i in range(num_parallel_envs):\n",
        "            if time_step.is_last()[i]:\n",
        "                total_return += episode_return[i]\n",
        "                # Atualiza episode_return usando operações do TensorFlow\n",
        "                episode_return = tf.tensor_scatter_nd_update(\n",
        "                    episode_return, \n",
        "                    indices=[[i]], \n",
        "                    updates=[0.0]\n",
        "                )\n",
        "                episode_counts[i] += 1\n",
        "                total_episodes += 1\n",
        "                if total_episodes >= num_eval_episodes:\n",
        "                    break\n",
        "\n",
        "    avg_return = total_return / num_eval_episodes\n",
        "    logging.info(f'Recompensa Média: {avg_return}')\n",
        "    return avg_return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_17HcVG5wUOG"
      },
      "source": [
        "A execução desse cálculo em random_policy mostra um desempenho de linha de base no ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 99min value -387301.46008750144\n",
        "import numpy as np\n",
        "from tf_agents.environments import tf_py_environment, ParallelPyEnvironment\n",
        "from tf_agents.policies import random_tf_policy\n",
        "    \n",
        "    \n",
        "average_return = parallel_compute_avg_return_test(tf_env, random_policy, num_parallel_envs = 1, num_eval_episodes=1)\n",
        "print(f'Average return: {average_return}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgYQbTYHwUOG"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "Para acompanhar os dados coletados do ambiente, usaremos o Reverb, um sistema de replay eficiente, extensível e fácil de usar da Deepmind. Ele armazena dados de experiência quando coletamos trajetórias e são consumidos durante o treinamento.\n",
        "\n",
        "Este buffer de reprodução é construído usando especificações que descrevem os tensores que devem ser armazenados, que podem ser obtidos do agente usando agent.collect_data_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ4fdV_hwUOG"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvqrfwpIwUOG"
      },
      "source": [
        "Para a maioria dos agentes, collect_data_spec é uma tupla nomeada chamada Trajetória, contendo especificações para observações, ações, recompensas e outros itens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9WR-ksOwUOG"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx3YiSGQwUOH"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec._fields\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRnuk21nwUOH"
      },
      "source": [
        "# Coleta de dados\n",
        "\n",
        "Agora execute a política aleatória no ambiente por algumas etapas, registrando os dados no buffer de reprodução.\n",
        "\n",
        "Aqui estamos usando 'PyDriver' para executar o ciclo de coleta de experiência. Você pode aprender mais sobre o driver TF Agents em nosso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTyxgN8CwUOH"
      },
      "outputs": [],
      "source": [
        "# Um ambiente de treinamento\n",
        "\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import random_tf_policy, py_tf_eager_policy\n",
        "from tf_agents.metrics import py_metrics, tf_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory as traj\n",
        "from tf_agents.environments import ParallelPyEnvironment\n",
        "\n",
        "# Suposição: 'env3W' e 'df_test' estão definidos corretamente\n",
        "# Suposição: 'num_parallel_envs' está definido corretamente\n",
        "\n",
        "# Criação de num_parallel_envs ambientes de teste em paralelo\n",
        "parallel_env = ParallelPyEnvironment([lambda: env3W(df_test) for _ in range(num_parallel_envs)])\n",
        "tf_env = tf_py_environment.TFPyEnvironment(parallel_env)\n",
        "\n",
        "# Suposição: 'agent' já está definido e inicializado corretamente\n",
        "\n",
        "# Inicializa o Replay Buffer com batch_size correto\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=num_parallel_envs,  # Importante para ambientes paralelos\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "# Inicializa o Driver para coleta de experiências\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=initial_collect_steps)\n",
        "\n",
        "# Executa o driver para coletar experiências\n",
        "time_step = tf_env.reset()  # Use tf_env para resetar, não parallel_env diretamente\n",
        "final_time_step, _ = collect_driver.run(time_step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2pf1IIOwUOH"
      },
      "source": [
        "O buffer de repetição agora é uma coleção de trajetórias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom5AV9-wUOH"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVF9gTCXwUOH"
      },
      "source": [
        "O agente precisa de acesso ao buffer de reprodução. Isso é fornecido pela criação de um pipeline tf.data.Dataset iterável que alimentará os dados para o agente.\n",
        "\n",
        "Cada linha do buffer de reprodução armazena apenas uma única etapa de observação. Mas como o Agente DQN precisa da observação atual e da próxima para calcular a perda, o pipeline do conjunto de dados irá amostrar duas linhas adjacentes para cada item no lote (num_steps=2).\n",
        "\n",
        "Este conjunto de dados também é otimizado executando chamadas paralelas e pré-busca de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKil7H9wUOH"
      },
      "outputs": [],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwC---j4wUOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data\n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZbDgm7xwUOI"
      },
      "source": [
        "# Treinando o agente\n",
        "\n",
        "Duas coisas devem acontecer durante o ciclo de treinamento:\n",
        "\n",
        "* coletar dados do ambiente\n",
        "* usar esses dados para treinar a(s) rede(s) neural(is) do agente\n",
        "\n",
        "Este exemplo também avalia periodicamente a política e imprime a pontuação atual.\n",
        "\n",
        "O seguinte levará cerca de 5 minutos para ser executado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_agents.environments import tf_py_environment, ParallelPyEnvironment\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import random_tf_policy, py_tf_eager_policy\n",
        "from tf_agents.metrics import py_metrics, tf_metrics\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory as traj\n",
        "from tf_agents.environments import ParallelPyEnvironment\n",
        "# Suponha que todas as importações e definições necessárias estejam completas\n",
        "\n",
        "logging.info('Iniciando o ambiente de treinamento')\n",
        "def create_env():\n",
        "    return env3W(df_test)  # Suponha que env3W e df_test estejam definidos corretamente\n",
        "\n",
        "\n",
        "num_parallel_envs = 5\n",
        "parallel_env = ParallelPyEnvironment([lambda: create_env() for _ in range(num_parallel_envs)])\n",
        "tf_env = tf_py_environment.TFPyEnvironment(parallel_env)\n",
        "\n",
        "logging.info(f'Criando {num_parallel_envs} ambientes env3W em paralelo')\n",
        "\n",
        "# (Optional) Otimizações com TF function\n",
        "agent.train = common.function(agent.train)\n",
        "logging.info('Otimizando agent.train com TF function')\n",
        "\n",
        "# Reseta o contador de passos de treinamento\n",
        "agent.train_step_counter.assign(0)\n",
        "logging.info('Resetando o contador de passos de treinamento')\n",
        "\n",
        "\n",
        "logging.info('Iniciando a avaliação da política do agente antes do treinamento')\n",
        "# Avalia a política do agente antes do treinamento\n",
        "average_return = parallel_compute_avg_return_test(tf_env, agent.policy, num_parallel_envs, num_eval_episodes)\n",
        "returns = [average_return]\n",
        "logging.info(f'Recompensa Média: {average_return}')\n",
        "\n",
        "\n",
        "logging.info('Iniciando o treinamento do agente')\n",
        "# Reseta o ambiente\n",
        "time_step = tf_env.reset()\n",
        "\n",
        "# Inicializa o Replay Buffer com batch_size correto\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=num_parallel_envs,  # Importante para ambientes paralelos\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "# Inicializa o Driver para coleta de experiências\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=initial_collect_steps)\n",
        "\n",
        "logging.info('Iniciando a coleta de experiências')\n",
        "for i in range(num_iterations):\n",
        "    \n",
        "    # Coleta alguns passos e salva no replay buffer\n",
        "    time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "    # Amostra um batch de dados do buffer e atualiza a rede do agente\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    # Loga o progresso a cada log_interval passos\n",
        "    if step % log_interval == 0:\n",
        "        logging.debug(f'Logando o progresso a cada {log_interval} passos')\n",
        "        logging.info(f'step = {step}: loss = {train_loss}')\n",
        "\n",
        "    # Avalia a política do agente a cada eval_interval passos\n",
        "    if step % eval_interval == 0:\n",
        "        logging.debug(f'Avaliando a política do agente a cada {eval_interval} passos')\n",
        "        average_return = parallel_compute_avg_return_test(tf_env, agent.policy, num_parallel_envs, num_eval_episodes)        \n",
        "        logging.info(f'step = {step}: Average Return = {average_return}')\n",
        "        returns.append(average_return)\n",
        "\n",
        "    logging.info(f' [{i}] : step = {step}: loss = {train_loss}')\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "logging.info(f'Iterações: {iterations}')\n",
        "logging.info(f'Recompensas: {returns}')\n",
        "logging.info('Treinamento concluído')\n",
        "\n",
        "# Plota os retornos\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Um ambiente apenas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zICrWW_swUOI"
      },
      "outputs": [],
      "source": [
        "# 883 min - canceling\n",
        "'''try:\n",
        "  %%time\n",
        "except:\n",
        "  pass'''\n",
        "\n",
        "eval_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "logging.info('Iniciando a avaliação da política do agente antes do treinamento')\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = env.reset()\n",
        "\n",
        "logging.info(f'Iniciando a coleta de experiências')\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "logging.info(f'Iniciando o treinamento do agente')\n",
        "for i in range(num_iterations):\n",
        "\n",
        "  logging.info(f'Collecting step: {i}'.format(_))\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    logging.info(f'step % log_interval')\n",
        "    \n",
        "    logging.info(f'step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    logging.info(f'step % eval_interval')\n",
        "\n",
        "    logging.info(f'Iniciando a avaliação da política do agente')\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    \n",
        "    logging.info(f'step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaszOVDDwUOI"
      },
      "source": [
        "# Visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ssTtQXwUOI"
      },
      "outputs": [],
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
