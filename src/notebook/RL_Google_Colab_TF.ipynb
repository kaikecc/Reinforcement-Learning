{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaikecc/Reinforcement-Learning/blob/main/src/notebook/RL_Google_Colab_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBFlNsimwUN5"
      },
      "source": [
        "# 1. Importação e Configuração do 3W Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE09MzpdwUN6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "from pathlib import Path\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "from collections import defaultdict\n",
        "from natsort import natsorted\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIqiWDgnz9oW",
        "outputId": "554f75eb-ff80-40a1-cb1f-00d84b1e0c9e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L-x2zez_Ns2"
      },
      "source": [
        "# Teste GPU TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwCMGQIv79PM",
        "outputId": "1d6e3c24-2e6b-438b-fbef-628a573c4110"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTkPtwElH7wa",
        "outputId": "47add1af-7065-4ad3-da23-da265cea02ae"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQb8SQWdL67C",
        "outputId": "b79584ea-89f1-4a3e-f67a-b570faee897a"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjsIiU3E8V_G",
        "outputId": "4219f251-6d4f-49fe-f0e4-77cd0846e08e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coleta dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcq63mnrwUN8"
      },
      "outputs": [],
      "source": [
        "#data_path = Path('/content/drive/MyDrive/Colab Notebooks/dataset')\n",
        "data_path = Path('/home/dataset')\n",
        "\n",
        "events_names = {0: 'Normal',\n",
        "                1: 'Abrupt Increase of BSW',\n",
        "                2: 'Spurious Closure of DHSV',\n",
        "                3: 'Severe Slugging',\n",
        "                4: 'Flow Instability',\n",
        "                5: 'Rapid Productivity Loss',\n",
        "                6: 'Quick Restriction in PCK',\n",
        "                7: 'Scaling in PCK',\n",
        "                8: 'Hydrate in Production Line'\n",
        "               }\n",
        "columns = ['P-PDG',\n",
        "           'P-TPT',\n",
        "           'T-TPT',\n",
        "           'P-MON-CKP',\n",
        "           'T-JUS-CKP',\n",
        "           'P-JUS-CKGL',\n",
        "           'T-JUS-CKGL',\n",
        "           'QGL',\n",
        "           'class']\n",
        "rare_threshold = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm9HObEYwUN8"
      },
      "outputs": [],
      "source": [
        "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
        "    for class_path in data_path.iterdir():\n",
        "        if class_path.is_dir():\n",
        "            try:\n",
        "                class_code = int(class_path.stem)\n",
        "            except ValueError:\n",
        "                # Se não for possível converter para int, pule este diretório\n",
        "                continue\n",
        "\n",
        "            for instance_path in class_path.iterdir():\n",
        "                if (instance_path.suffix == '.csv'):\n",
        "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
        "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
        "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
        "                       (not instance_path.stem.startswith('DRAWN'))):\n",
        "                        yield class_code, instance_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlji4u1_wUN-"
      },
      "outputs": [],
      "source": [
        "real_instances = list(class_and_file_generator(data_path, real=True, simulated=False, drawn=False))\n",
        "simulated_instances = list(class_and_file_generator(data_path, real=False, simulated=True, drawn=False))\n",
        "drawn_instances = list(class_and_file_generator(data_path, real=False, simulated=False, drawn=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVdf9_MQwUN-",
        "outputId": "bdcee006-3640-43f9-c3fd-c20c240dd3cc"
      },
      "outputs": [],
      "source": [
        "print(f'Number of real instances: {len(real_instances)}')\n",
        "print(f'Number of simulated instances: {len(simulated_instances)}')\n",
        "print(f'Number of drawn instances: {len(drawn_instances)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "--yMNWGHwUN_",
        "outputId": "146961ac-e8f5-42cf-c685-4f8bc4fe3cc3"
      },
      "outputs": [],
      "source": [
        "def load_instance(instances):\n",
        "    class_code, instance_path = instances\n",
        "    try:\n",
        "        well, instance_id = instance_path.stem.split('_')\n",
        "        df = pd.read_csv(instance_path, index_col='timestamp', parse_dates=['timestamp'])\n",
        "        assert (df.columns == columns).all(), \"invalid columns in the file {}: {}\".format(str(instance_path), str(df.columns.tolist()))\n",
        "        df['label'] = class_code\n",
        "        df['well'] = well\n",
        "        df['id'] = instance_id\n",
        "        df = df[['label', 'well', 'id'] + columns]\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        raise Exception('error reading file {}: {}'.format(instance_path, e))\n",
        "\n",
        "\n",
        "df = load_instance(real_instances[0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "wH6I_KHwwUN_",
        "outputId": "b3cdc2c9-0d0c-49dc-8d8b-54ce58d919c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Supondo que real_instances, simulated_instances, drawn_instances e events_names já estejam definidos\n",
        "instances_class = [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'REAL'} for c, p in real_instances] + \\\n",
        "                  [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'SIMULATED'} for c, p in simulated_instances] + \\\n",
        "                  [{'TYPE OF EVENT': str(c) + ' - ' + events_names[c], 'SOURCE': 'DRAWN'} for c, p in drawn_instances]\n",
        "df_class = pd.DataFrame(instances_class)\n",
        "\n",
        "# Correção aqui: pivot(index, columns, values) com argumentos nomeados\n",
        "df_class_count = df_class.groupby(['TYPE OF EVENT', 'SOURCE']).size().reset_index(name='count')\n",
        "df_class_count = df_class_count.pivot(index='TYPE OF EVENT', columns='SOURCE', values='count').fillna(0).astype(int)\n",
        "\n",
        "# Transposição não é necessária após a correção, mas ajustaremos a orientação conforme sua intenção original\n",
        "#df_class_count = df_class_count.T\n",
        "\n",
        "# Ordenação natural dos índices, se necessário\n",
        "df_class_count = df_class_count.loc[natsorted(df_class_count.index)]\n",
        "\n",
        "# Reordenar colunas se 'REAL', 'SIMULATED', 'DRAWN' estiverem presentes, adicionando a coluna 'TOTAL'\n",
        "df_class_count['TOTAL'] = df_class_count.sum(axis=1)\n",
        "df_class_count.loc['TOTAL'] = df_class_count.sum(axis=0)\n",
        "\n",
        "# Exibindo o DataFrame final\n",
        "df_class_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55UkwUSmwUOA"
      },
      "source": [
        "# Criação de um único dataframe com todos os poços reais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6btO7FZ2wUOA",
        "outputId": "697fd538-6b99-490a-a9b2-3c82757c7f3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Carregar todos os DataFrames dos arquivos de instâncias reais\n",
        "df_all_instances_real = [pd.DataFrame(load_instance(instance)) for instance in real_instances]\n",
        "\n",
        "# Concatenar todos os DataFrames na lista para formar um único DataFrame\n",
        "df = pd.concat(df_all_instances_real, ignore_index=True)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqshw7WrwUOA"
      },
      "source": [
        "# Criação de um dataframe para o desenvolvimento de um Ambiente Python (TF)\n",
        "\n",
        "Critério de seleção dos poços\n",
        "\n",
        "1. Com mais de três tipos de eventos: 'WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006'\n",
        "2. Dois poços para equilibrar a falta de eventos: 'WELL-00015', 'WELL-00016'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q5JPWAswUOA",
        "outputId": "8e63e0d5-5e35-4229-ef6e-ac94aa0bc28c"
      },
      "outputs": [],
      "source": [
        "# Lista de poços para treinamento do modelo\n",
        "wells_to_include = ['WELL-00001', 'WELL-00002', 'WELL-00004', 'WELL-00006', 'WELL-00015', 'WELL-00016']\n",
        "\n",
        "# Selecionando colunas específicas\n",
        "columns_to_select = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'class']\n",
        "\n",
        "# Filtrando o DataFrame para incluir apenas os poços desejados e colunas específicas, e removendo linhas com NaNs\n",
        "df_env = df[df['well'].isin(wells_to_include)][columns_to_select].dropna()\n",
        "\n",
        "# Mostrando informações do DataFrame filtrado\n",
        "df_env.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPoiKYs-wUOB",
        "outputId": "75b33178-e0f5-44de-8431-abf10cf758ca"
      },
      "outputs": [],
      "source": [
        "nan_count = df_env.isna().sum()\n",
        "\n",
        "print(nan_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTA2HFRnwUOB"
      },
      "source": [
        "# Pré-processamento dos dados do dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c1TwtL1YwUOB",
        "outputId": "00c942b8-8a04-4650-8b8a-9ec39bf581c3"
      },
      "outputs": [],
      "source": [
        "# Colunas para normalizar, exceto 'class'\n",
        "columns_to_normalize = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']\n",
        "\n",
        "# Aplicando Z-score Standardization\n",
        "df_env[columns_to_normalize] = (df_env[columns_to_normalize] - df_env[columns_to_normalize].mean()) / df_env[columns_to_normalize].std()\n",
        "\n",
        "# Converte a coluna 'class' para valor int\n",
        "df_env['class'] = df_env['class'].astype(int)\n",
        "\n",
        "# Verifique as primeiras linhas para confirmar a normalização\n",
        "df_env.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contagem de valores para cada classe\n",
        "class_counts = df_env['class'].value_counts().sort_index()\n",
        "# soma os valores de 1 a 9 e 101 a 109\n",
        "rare_class_counts_A = class_counts[(class_counts.index > 0) & (class_counts.index < 10)].sum()\n",
        "rare_class_counts_B = class_counts[(class_counts.index > 10)].sum()\n",
        "rare_class_counts_C = class_counts[(class_counts.index == 0)].sum()\n",
        "\n",
        "print(f'Normal: {rare_class_counts_C}')\n",
        "print(f'Transiente de anomalia: {rare_class_counts_B}')\n",
        "print(f'Estável de anomalia: {rare_class_counts_A}')\n",
        "print(f'Total: {rare_class_counts_A + rare_class_counts_B + rare_class_counts_C}')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar('Normal', rare_class_counts_C, label='Normal')\n",
        "ax.bar('Estável de anomalia', rare_class_counts_A, label='Estável de anomalia')\n",
        "ax.bar('Transiente de anomalia', rare_class_counts_B, label='Transiente de anomalia')\n",
        "\n",
        "ax.set_ylabel('Quantidade')\n",
        "ax.set_title('Quantidade de amostras por classe')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUQ2Bj9o2PgJ"
      },
      "source": [
        "# Classe 3W - Ambiente TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65g1xnRD2bf1"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-agents\n",
        "#!pip install dm-reverb[tensorflow]\n",
        "#!pip install dm-reverb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxhSSexx2Nzi"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "class env3W(py_environment.PyEnvironment):\n",
        "    '''\n",
        "    Essa classe é um Modelo-Livre do Ambiente em python para o problema de detecção de falhas em poços de petróleo.\n",
        "    O dataframe é fornecido como entrada para o ambiente com mais de 10 milhões de registros.\n",
        "\n",
        "     1. O ambiente é um repositório di github https://github.com/petrobras/3W\n",
        "     2. O ambiente é composto por dados de seis poços de petróleo, com cinco variáveis (observações) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rótulo indentificador de falha [class]\n",
        "     3. O ambiente é um ambiente de simulação, onde o agente pode escolher entre duas ações: 0 - Não Detectado ou 1 - Detectado para cada observação\n",
        "     4. A recompensa é calculada com base na ação escolhida e no rótulo de falha [class]:\n",
        "\n",
        "        Estados:\n",
        "        - rótulo de falha: 0 - Estado Normal\n",
        "        - rótulo de falha: 1 a 8 - Estável de Anomalia (Falha)\n",
        "        - rótulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
        "\n",
        "        Ações/Recompensas:\n",
        "        - Se o rótulo de falha for 0, a recompensa é 1 se a ação for 0, caso contrário, a recompensa é -100\n",
        "        - Se o rótulo de falha estiver entre 1 e 8, a recompensa é -100 se a ação for 0, caso contrário, a recompensa é 100\n",
        "        - Se o rótulo de falha estiver entre 101 e 108, a recompensa é -10 se a ação for 0, caso contrário, a recompensa é 10\n",
        "    \n",
        "    '''\n",
        "    def __init__(self, dataframe):\n",
        "        super(env3W, self).__init__()\n",
        "        self._dataframe = dataframe\n",
        "        self._index = 0\n",
        "        # Ação: 0 - Não Deectado ou 1 - Detectado\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=1, name='action') \n",
        "        self.columns_needed = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP']               \n",
        "        num_features = len(self.columns_needed)\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(num_features,), dtype=np.float32, name='observation')\n",
        "        row = self._dataframe.iloc[self._index][self.columns_needed]\n",
        "        self._state = row.values\n",
        "        self._episode_ended = False\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "    \n",
        "   \n",
        "    def _update_state(self):\n",
        "        # Avaliar o index se é +1 ou não    \n",
        "        row = self._dataframe.iloc[self._index][self.columns_needed]\n",
        "        self._state = row.values\n",
        "\n",
        "   \n",
        "    def _reset(self):\n",
        "        self._index = 0\n",
        "        self._update_state()\n",
        "        self._episode_ended = False\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "    \n",
        "    \n",
        "    def _step(self, action):\n",
        "\n",
        "        # Verifica se a ação é válida\n",
        "        if not 0 <= action <= 1:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=0)             \n",
        "        \n",
        "        self._update_state()\n",
        "        reward = self._calculate_reward(action)\n",
        "\n",
        "        if self._episode_ended or self._index >= len(self._dataframe) - 1:\n",
        "            # Quando chegar na última linha, marque o episódio como terminado e pare.\n",
        "            self._episode_ended = True            \n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "        else:\n",
        "            self._index += 1\n",
        "            return ts.transition(np.array(self._state, dtype=np.float32), reward=reward, discount=1.0)\n",
        "               \n",
        "          \n",
        "    def _calculate_reward(self, action):\n",
        "        class_value = self._dataframe.iloc[self._index]['class']\n",
        "        if class_value == 0:\n",
        "            return 0.01 if action == 0 else -1\n",
        "        elif class_value in range(1, 9):\n",
        "            return -1 if action == 0 else 1\n",
        "        elif class_value in range(101, 109):\n",
        "            return -0.1 if action == 0 else 0.1\n",
        "        else:\n",
        "            # Define uma recompensa padrão para qualquer outro valor de class não especificado\n",
        "            return 0\n",
        "       \n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38SzGfpZwUOB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BotUqckwUOB"
      },
      "outputs": [],
      "source": [
        "#import sys\n",
        "#import os\n",
        "#sys.path.append(os.path.join('..'))\n",
        "#from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente\n",
        "\n",
        "import reverb\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import random_tf_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGKJZD-PwUOC"
      },
      "source": [
        "# Hiperparâmentros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDHT9J9bwUOC"
      },
      "outputs": [],
      "source": [
        "initial_collect_steps = 1000  # Número de passos de coleta inicial\n",
        "collect_steps_per_iteration = 1  # Passos de coleta por iteração de treinamento\n",
        "replay_buffer_max_length = 100000  # Tamanho máximo do replay buffer\n",
        "\n",
        "log_interval = 1000  # Log do progresso a cada 1000 iterações de treinamento\n",
        "eval_interval = 5000  # Avaliação do desempenho do agente a cada 5000 iterações\n",
        "batch_size = 64  # Exemplo de tamanho de batch para treinamento\n",
        "num_iterations = 10000  # Exemplo de número de iterações de treinamento\n",
        "learning_rate= 1e-3 # Taxa de aprendizado\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lePzndNbwUOC"
      },
      "source": [
        "# Ambiente\n",
        "\n",
        "No Aprendizado por Reforço (RL), um ambiente representa a tarefa ou problema a ser resolvido. Ambientes padrão podem ser criados em TF-Agents usando conjuntos tf_agents.environments.\n",
        "\n",
        "1. O ambiente é um repositório di github https://github.com/petrobras/3W\n",
        "2. O ambiente é composto por dados de seis poços de petróleo, com cinco variáveis (observações) de entrada ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP'] e um rótulo indentificador de falha [class]\n",
        "3. O ambiente é um ambiente de simulação, onde o agente pode escolher entre duas ações: 0 - Não Detectado ou 1 - Detectado para cada observação\n",
        "4. A recompensa é calculada com base na ação escolhida e no rótulo de falha [class]:\n",
        "\n",
        "Estados:\n",
        "- rótulo de falha: 0 - Estado Normal\n",
        "- rótulo de falha: 1 a 8 - Estável de Anomalia (Falha)\n",
        "- rótulo de falha: 101 a 108 - Transiente de Anomalia (Falha)\n",
        "\n",
        "Ações/Recompensas:\n",
        "- Se o rótulo de falha for 0, a recompensa é 1 se a ação for 0, caso contrário, a recompensa é -100\n",
        "- Se o rótulo de falha estiver entre 1 e 8, a recompensa é -100 se a ação for 0, caso contrário, a recompensa é 100\n",
        "- Se o rótulo de falha estiver entre 101 e 108, a recompensa é -10 se a ação for 0, caso contrário, a recompensa é 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7SsPggpwUOC"
      },
      "outputs": [],
      "source": [
        "#amostra aleatória de 10% dos dados\n",
        "df_test = df_env.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Criação do ambiente de teste\n",
        "env = env3W(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "J-vWKZMEzqLA",
        "outputId": "1cfaa2e2-8fb6-4615-8be6-9d35a4b87934"
      },
      "outputs": [],
      "source": [
        "# Contagem de valores para cada classe\n",
        "class_counts = df_test['class'].value_counts().sort_index()\n",
        "# soma os valores de 1 a 9 e 101 a 109\n",
        "rare_class_counts_A = class_counts[(class_counts.index > 0) & (class_counts.index < 10)].sum()\n",
        "rare_class_counts_B = class_counts[(class_counts.index > 10)].sum()\n",
        "rare_class_counts_C = class_counts[(class_counts.index == 0)].sum()\n",
        "\n",
        "total = rare_class_counts_A + rare_class_counts_B + rare_class_counts_C\n",
        "\n",
        "print(f'Normal: {rare_class_counts_C} - {round(rare_class_counts_C/total*100, 2)}%')\n",
        "print(f'Transiente de anomalia: {rare_class_counts_B} - {round(rare_class_counts_B/total*100, 2)}%')\n",
        "print(f'Estável de anomalia: {rare_class_counts_A} - {round(rare_class_counts_A/total*100, 2)}%')\n",
        "print(f'Total: {rare_class_counts_A + rare_class_counts_B + rare_class_counts_C}')\n",
        "\n",
        "# plot em um gráfico de barra em ordem decrescente rare_class_counts_C, rare_class_counts_B, rare_class_counts_A\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar('Normal', rare_class_counts_C, label='Normal')\n",
        "ax.bar('Estável de anomalia', rare_class_counts_A, label='Estável de anomalia')\n",
        "ax.bar('Transiente de anomalia', rare_class_counts_B, label='Transiente de anomalia')\n",
        "\n",
        "ax.set_ylabel('Quantidade')\n",
        "ax.set_title('Quantidade de amostras por classe')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoF8D_9AwUOC"
      },
      "source": [
        "Para validar o ambiente, é usado uma política aleatória para gerar ações e faremos a iteração em mais de 5 episódios para garantir que as coisas estejam funcionando como pretendido. Um erro é gerado se recebermos um time_step que não segue as especificações do ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY3lbajTwUOC"
      },
      "outputs": [],
      "source": [
        "# depois de 1h30 - cancelando verificação, no mo meu pc 54min com 10%\n",
        "utils.validate_py_environment(env, episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para mostrar mais casas decimais, podemos definir a opção de visualização do pandas.\n",
        "pd.set_option('display.precision', 8)\n",
        "\n",
        "# Agora, ao exibir o DataFrame, ele deve mostrar mais casas decimais.\n",
        "df_test.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "detect = np.array(0, dtype=np.int32) # Não Detectado\n",
        "\n",
        "\n",
        "time_step = env.reset()\n",
        "\n",
        "cumulative_reward = 0\n",
        "\n",
        "\n",
        "for _ in range(4):\n",
        "  time_step = env.step(detect)\n",
        "  print(time_step)\n",
        "  cumulative_reward += time_step.reward\n",
        "\n",
        "time_step = env.step(np.array(1, dtype=np.int32)) # Detectado\n",
        "print(time_step)\n",
        "cumulative_reward += time_step.reward\n",
        "print('Final Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "reward_list = []\n",
        "\n",
        "# crie um step_list de 0 à 100 ao passo de 1\n",
        "step_list = np.arange(0, 100, 1)\n",
        "\n",
        "for _ in range(4):\n",
        " \n",
        "    detect = np.array(0, dtype=np.int32) # Não Detectado\n",
        "    time_step = env.step(detect)\n",
        "    reward_list.append(time_step.reward)\n",
        "\n",
        "  # plotar o gráfico step_list x reward_list\n",
        "    \n",
        "plt.plot(step_list, reward_list)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Step vs Reward')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG3pf3QnwUOC",
        "outputId": "6edfa652-9d34-473a-de2e-124f11379eee"
      },
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5-XbFJawUOC",
        "outputId": "ccf791ab-56e8-49f8-fdd0-321313cb8b1d"
      },
      "outputs": [],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWaL4ak5wUOC",
        "outputId": "19ec322b-9b78-44cd-8574-1efc26790ec5"
      },
      "outputs": [],
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMogQ3o2wUOD",
        "outputId": "f49574fe-113a-4b8b-e405-3876c58db9f6"
      },
      "outputs": [],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY-H67iFwUOD"
      },
      "source": [
        "No ambiente env3W:\n",
        "\n",
        "* Observação é um array de cinco floats:\n",
        "\n",
        "    * Três sensores de pressão: 'P-PDG', 'P-TPT', 'P-MON-CKP'\n",
        "    * Dois sensores de temperatura: 'T-TPT', T-JUS-CKP'\n",
        "\n",
        "* A recompensa é um escalar inteiro.\n",
        "\n",
        "* A ação é um escalar inteiro com duas possibilidades:\n",
        "\n",
        "    * 0 — \"Não Detectado\"\n",
        "    * 1 — \"Detectado\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC34nmWEwUOD",
        "outputId": "cf69cd25-a71d-42a0-c445-ea54abd2b6fc"
      },
      "outputs": [],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFI-2T6iwUOD"
      },
      "source": [
        "Normalmente dois ambientes são instanciados: um para treinamento e outro para avaliação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5RP2etTwUOD"
      },
      "outputs": [],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(env)  # Avaliação pode ser em um env similar ou diferente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSIdpwmxwUOD"
      },
      "source": [
        "# Agente\n",
        "\n",
        "O algoritmo utilizado para resolver um problema de RL é representado por um Agente. TF-Agents fornece implementações padrão de uma variedade de Agentes, incluindo:\n",
        "\n",
        "* DQN (Usado para teste)\n",
        "* REINFORCE\n",
        "* DDPG\n",
        "* TD3\n",
        "* PPO\n",
        "* SAC\n",
        "\n",
        "O agente DQN pode ser utilizado em qualquer ambiente que possua um espaço de ação discreto.\n",
        "\n",
        "No coração de um Agente DQN está uma QNetwork, um modelo de rede neural que pode aprender a prever QValues (retornos esperados) para todas as ações, dada uma observação do ambiente.\n",
        "\n",
        "Usaremos tf_agents.networks. para criar uma QNetwork. A rede será composta por uma sequência de camadas tf.keras.layers.Dense, onde a camada final terá 1 saída para cada ação possível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXzk8CC-wUOD"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (100, 50)  # Exemplo de tamanho das camadas totalmente conectadas\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Defina uma função auxiliar para criar camadas densas configuradas com o direito\n",
        "# ativação e inicializador do kernel.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "\n",
        "# QNetwork consiste em uma sequência de camadas densas seguidas por uma camada densa\n",
        "# com unidades `num_actions` para gerar um q_value por ação disponível como sua saída.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PKBr48wwUOD"
      },
      "source": [
        "Agora use tf_agents.agents.dqn.dqn_agent para instanciar um DqnAgent. Além de time_step_spec, action_spec e QNetwork, o construtor do agente também requer um otimizador (neste caso, AdamOptimizer), uma função de perda e um contador de passos inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPMv_SxEwUOD"
      },
      "outputs": [],
      "source": [
        "\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAYXp6d-wUOE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NO7kq2LwUOE"
      },
      "source": [
        "# Política\n",
        "\n",
        "Uma política define a forma como um agente atua em um ambiente. Normalmente, o objetivo da aprendizagem por reforço é treinar o modelo subjacente até que a política produza o resultado desejado.\n",
        "\n",
        "* O resultado desejado é a identificar uma falha com base nas observações\n",
        "* A política retorna uma ação (Não Detectado ou Detectado) para cada observação time_step.\n",
        "\n",
        "Os agentes contêm duas políticas:\n",
        "\n",
        "* agent.policy — A política principal usada para avaliação e implantação.\n",
        "* agent.collect_policy — Uma segunda política usada para coleta de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPjXJ4ZjwUOF"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmOeS48OwUOF"
      },
      "source": [
        "As políticas podem ser criadas independentemente dos agentes. Por exemplo, use tf_agents.policies.random_tf_policy para criar uma política que selecionará aleatoriamente uma ação para cada time_step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERGG2fNowUOF"
      },
      "outputs": [],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IYQt_z6wUOF"
      },
      "source": [
        "Para obter uma ação de uma política, chame o método policy.action(time_step). O time_step contém a observação do ambiente. Este método retorna um PolicyStep, que é uma tupla nomeada com três componentes:\n",
        "\n",
        "* ação - a ação a ser executada (neste caso, 0 ou 1)\n",
        "* estado – usado para políticas com estado (isto é, baseadas em ANN)\n",
        "* info — dados auxiliares, como log de probabilidades de ações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2-PWtgpwUOF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "time_step = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4wriL68wUOG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Exemplo de criação de uma política aleatória\n",
        "# Supondo que env seja seu ambiente TF-Agents\n",
        "random_policy = random_tf_policy.RandomTFPolicy(action_spec=env.action_spec(),\n",
        "                                                 time_step_spec=env.time_step_spec())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZgPIYxQwUOG"
      },
      "source": [
        "# Métricas e Avaliação\n",
        "\n",
        "A métrica mais comum usada para avaliar uma política é o retorno médio. O retorno é a soma das recompensas obtidas durante a execução de uma política em um ambiente durante um episódio. Vários episódios são executados, criando um retorno médio.\n",
        "\n",
        "A função a seguir calcula o retorno médio de uma política, dada a política, o ambiente e um número de episódios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEPGu2AYwUOG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_17HcVG5wUOG"
      },
      "source": [
        "A execução desse cálculo em random_policy mostra um desempenho de linha de base no ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Fdpf7Hj3wUOG",
        "outputId": "5435013e-82c6-49f4-f133-62cf2815a3f1"
      },
      "outputs": [],
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgYQbTYHwUOG"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "Para acompanhar os dados coletados do ambiente, usaremos o Reverb, um sistema de replay eficiente, extensível e fácil de usar da Deepmind. Ele armazena dados de experiência quando coletamos trajetórias e são consumidos durante o treinamento.\n",
        "\n",
        "Este buffer de reprodução é construído usando especificações que descrevem os tensores que devem ser armazenados, que podem ser obtidos do agente usando agent.collect_data_spec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ4fdV_hwUOG"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvqrfwpIwUOG"
      },
      "source": [
        "Para a maioria dos agentes, collect_data_spec é uma tupla nomeada chamada Trajetória, contendo especificações para observações, ações, recompensas e outros itens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9WR-ksOwUOG"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx3YiSGQwUOH"
      },
      "outputs": [],
      "source": [
        "agent.collect_data_spec._fields\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRnuk21nwUOH"
      },
      "source": [
        "# Coleção de dados\n",
        "\n",
        "Agora execute a política aleatória no ambiente por algumas etapas, registrando os dados no buffer de reprodução.\n",
        "\n",
        "Aqui estamos usando 'PyDriver' para executar o ciclo de coleta de experiência. Você pode aprender mais sobre o driver TF Agents em nosso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTyxgN8CwUOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@test {\"skip\": true}\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2pf1IIOwUOH"
      },
      "source": [
        "O buffer de repetição agora é uma coleção de trajetórias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom5AV9-wUOH"
      },
      "outputs": [],
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVF9gTCXwUOH"
      },
      "source": [
        "O agente precisa de acesso ao buffer de reprodução. Isso é fornecido pela criação de um pipeline tf.data.Dataset iterável que alimentará os dados para o agente.\n",
        "\n",
        "Cada linha do buffer de reprodução armazena apenas uma única etapa de observação. Mas como o Agente DQN precisa da observação atual e da próxima para calcular a perda, o pipeline do conjunto de dados irá amostrar duas linhas adjacentes para cada item no lote (num_steps=2).\n",
        "\n",
        "Este conjunto de dados também é otimizado executando chamadas paralelas e pré-busca de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKil7H9wUOH"
      },
      "outputs": [],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwC---j4wUOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZbDgm7xwUOI"
      },
      "source": [
        "# Treinando o agente\n",
        "\n",
        "Duas coisas devem acontecer durante o ciclo de treinamento:\n",
        "\n",
        "* coletar dados do ambiente\n",
        "* usar esses dados para treinar a(s) rede(s) neural(is) do agente\n",
        "\n",
        "Este exemplo também avalia periodicamente a política e imprime a pontuação atual.\n",
        "\n",
        "O seguinte levará cerca de 5 minutos para ser executado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zICrWW_swUOI"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "'''try:\n",
        "  %%time\n",
        "except:\n",
        "  pass'''\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaszOVDDwUOI"
      },
      "source": [
        "# Visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ssTtQXwUOI"
      },
      "outputs": [],
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
