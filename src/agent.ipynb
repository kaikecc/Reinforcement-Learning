{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_agents\\typing\\types.py:114: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from classes._env3W import env3W  # Ajuste para o caminho correto da sua classe de ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instalar Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-agents tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir a Rede Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import q_network\n",
    "from tf_agents.utils import common\n",
    "\n",
    "df = pd.read_csv('df_env.csv')\n",
    "environment = env3W(df)\n",
    "fc_layer_params = (100, 50)  # Exemplo de parâmetros das camadas totalmente conectadas\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    environment.observation_spec(),\n",
    "    environment.action_spec(),\n",
    "    fc_layer_params=(100, 50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurar o Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "# Exemplo de criação da instância do ambiente\n",
    "# Supondo que 'dataframe' seja o seu DataFrame pandas\n",
    "\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    environment.time_step_spec(),\n",
    "    environment.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinar o Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    \n",
    "    # Supõe que 'observation' é a observação atual do timestep e precisa ser ajustada\n",
    "    # para incluir a dimensão do batch.\n",
    "    observation = np.expand_dims(time_step.observation, axis=0)\n",
    "    \n",
    "    # Cria um novo timestep com a observação ajustada.\n",
    "    adjusted_time_step = ts.TimeStep(\n",
    "        step_type=np.expand_dims(time_step.step_type, axis=0),\n",
    "        reward=np.expand_dims(time_step.reward, axis=0),\n",
    "        discount=np.expand_dims(time_step.discount, axis=0),\n",
    "        observation=observation)\n",
    "    \n",
    "    action_step = policy.action(adjusted_time_step)\n",
    "    \n",
    "    # Executa a ação no ambiente para obter o próximo timestep\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    \n",
    "    # Cria uma trajetória da transição e adiciona ao replay buffer.\n",
    "    traj = trajectory.from_transition(adjusted_time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Especificação de observação do ambiente: BoundedArraySpec(shape=(9,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)\n"
     ]
    }
   ],
   "source": [
    "print(\"Especificação de observação do ambiente:\", environment.observation_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'QNetwork' (type QNetwork).\n\nInput 0 of layer \"dense_8\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (50,)\n\nCall arguments received by layer 'QNetwork' (type QNetwork):\n  • observation=tf.Tensor(shape=(9,), dtype=float32)\n  • step_type=array(0)\n  • network_state=()\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Coletar experiências iniciais\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mcollect_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Configuração do dataset para treinamento\u001b[39;00m\n\u001b[0;32m     41\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# Definir conforme necessário para o treinamento\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36mcollect_step\u001b[1;34m(environment, policy, buffer)\u001b[0m\n\u001b[0;32m     11\u001b[0m time_step \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mcurrent_time_step()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Não é necessário adicionar manualmente a dimensão do batch se o ambiente já o faz.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m next_time_step \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action_step\u001b[38;5;241m.\u001b[39maction)\n\u001b[0;32m     15\u001b[0m traj \u001b[38;5;241m=\u001b[39m trajectory\u001b[38;5;241m.\u001b[39mfrom_transition(time_step, action_step, next_time_step)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\tf_policy.py:333\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[0;32m    332\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[1;32m--> 333\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[0;32m    336\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\utils\\common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m check_tf1_allowed()\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    191\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    192\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\epsilon_greedy_policy.py:122\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy._action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step, policy_state, seed):  \u001b[38;5;66;03m# pytype: disable=signature-mismatch  # overriding-parameter-count-checks\u001b[39;00m\n\u001b[0;32m    121\u001b[0m   seed_stream \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mSeedStream(seed\u001b[38;5;241m=\u001b[39mseed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_greedy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m   greedy_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m   random_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_policy\u001b[38;5;241m.\u001b[39maction(time_step, (), seed_stream())\n\u001b[0;32m    125\u001b[0m   outer_shape \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mget_outer_shape(time_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_spec)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\tf_policy.py:333\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[0;32m    332\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[1;32m--> 333\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[0;32m    336\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\utils\\common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m check_tf1_allowed()\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    191\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    192\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\tf_policy.py:589\u001b[0m, in \u001b[0;36mTFPolicy._action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of `action`.\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;124;03m    `info`: Optional side information such as action log probabilities.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    588\u001b[0m seed_stream \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mSeedStream(seed\u001b[38;5;241m=\u001b[39mseed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_agents_tf_policy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 589\u001b[0m distribution_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=wrong-arg-types\u001b[39;00m\n\u001b[0;32m    590\u001b[0m actions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m d: reparameterized_sampling\u001b[38;5;241m.\u001b[39msample(d, seed\u001b[38;5;241m=\u001b[39mseed_stream()),\n\u001b[0;32m    592\u001b[0m     distribution_step\u001b[38;5;241m.\u001b[39maction,\n\u001b[0;32m    593\u001b[0m )\n\u001b[0;32m    594\u001b[0m info \u001b[38;5;241m=\u001b[39m distribution_step\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\greedy_policy.py:82\u001b[0m, in \u001b[0;36mGreedyPolicy._distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour network\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms distribution does not implement mode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking it incompatible with a greedy policy.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m     80\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tfp\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mDeterministic(loc\u001b[38;5;241m=\u001b[39mgreedy_action)\n\u001b[1;32m---> 82\u001b[0m distribution_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy_step\u001b[38;5;241m.\u001b[39mPolicyStep(\n\u001b[0;32m     86\u001b[0m     tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(dist_fn, distribution_step\u001b[38;5;241m.\u001b[39maction),\n\u001b[0;32m     87\u001b[0m     distribution_step\u001b[38;5;241m.\u001b[39mstate,\n\u001b[0;32m     88\u001b[0m     distribution_step\u001b[38;5;241m.\u001b[39minfo,\n\u001b[0;32m     89\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\tf_policy.py:422\u001b[0m, in \u001b[0;36mTFPolicy.distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[0;32m    421\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[1;32m--> 422\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_log_probability:\n\u001b[0;32m    424\u001b[0m   \u001b[38;5;66;03m# This here is set only for compatibility with info_spec in constructor.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m   info \u001b[38;5;241m=\u001b[39m policy_step\u001b[38;5;241m.\u001b[39mset_log_probability(\n\u001b[0;32m    426\u001b[0m       step\u001b[38;5;241m.\u001b[39minfo,\n\u001b[0;32m    427\u001b[0m       tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m       ),\n\u001b[0;32m    431\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\policies\\q_policy.py:162\u001b[0m, in \u001b[0;36mQPolicy._distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m   network_observation, mask \u001b[38;5;241m=\u001b[39m observation_and_action_constraint_splitter(\n\u001b[0;32m    159\u001b[0m       network_observation\n\u001b[0;32m    160\u001b[0m   )\n\u001b[1;32m--> 162\u001b[0m q_values, policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_observation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m logits \u001b[38;5;241m=\u001b[39m q_values\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_and_action_constraint_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;66;03m# Overwrite the logits for invalid actions to logits.dtype.min.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\networks\\network.py:440\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(network_state)\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m network_state \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, ())\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m call_argspec\u001b[38;5;241m.\u001b[39mkeywords\n\u001b[0;32m    437\u001b[0m ):\n\u001b[0;32m    438\u001b[0m   normalized_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 440\u001b[0m outputs, new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnormalized_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=attribute-error  # typed-keras\u001b[39;00m\n\u001b[0;32m    442\u001b[0m nest_utils\u001b[38;5;241m.\u001b[39massert_matching_dtypes_and_inner_shapes(\n\u001b[0;32m    443\u001b[0m     new_state,\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_spec,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     specs_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`state_spec`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, new_state\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tf_agents\\networks\\q_network.py:157\u001b[0m, in \u001b[0;36mQNetwork.call\u001b[1;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the given observation through the network.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m  A tuple `(logits, network_state)`.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m state, network_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder(\n\u001b[0;32m    152\u001b[0m     observation,\n\u001b[0;32m    153\u001b[0m     step_type\u001b[38;5;241m=\u001b[39mstep_type,\n\u001b[0;32m    154\u001b[0m     network_state\u001b[38;5;241m=\u001b[39mnetwork_state,\n\u001b[0;32m    155\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    156\u001b[0m )\n\u001b[1;32m--> 157\u001b[0m q_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q_value_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_value, network_state\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'QNetwork' (type QNetwork).\n\nInput 0 of layer \"dense_8\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (50,)\n\nCall arguments received by layer 'QNetwork' (type QNetwork):\n  • observation=tf.Tensor(shape=(9,), dtype=float32)\n  • step_type=array(0)\n  • network_state=()\n  • training=False"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    # Não é necessário adicionar manualmente a dimensão do batch se o ambiente já o faz.\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
    "\n",
    "# Supondo que 'agent' já foi criado.\n",
    "# Inicialização do replay buffer\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=100000)\n",
    "\n",
    "\n",
    "\n",
    "environment.reset()\n",
    "\n",
    "# Coletar experiências iniciais\n",
    "for _ in range(1000):\n",
    "    collect_step(environment, agent.collect_policy, replay_buffer)\n",
    "\n",
    "\n",
    "\n",
    "# Configuração do dataset para treinamento\n",
    "batch_size = 64  # Definir conforme necessário para o treinamento\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "# Continuação do treinamento...\n",
    "\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Treinamento do agente\n",
    "num_iterations = 20000  # Definir o número de iterações de treinamento\n",
    "for i in range(num_iterations):\n",
    "    # Opcional: Coletar mais experiências durante o treinamento\n",
    "    if i % 100 == 0:\n",
    "        for _ in range(10):  # Número de passos adicionais a coletar a cada 100 iterações\n",
    "            collect_step(environment, agent.collect_policy, replay_buffer)\n",
    "    \n",
    "    # Treinar o agente com experiências do replay buffer\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    \n",
    "    # Opcional: Imprimir o progresso\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Iteração {i}, Perda: {train_loss}')\n",
    "\n",
    "# Adicione o código necessário para salvar o modelo ou avaliar o agente após o treinamento, se necessário.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
